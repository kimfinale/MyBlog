[
  {
    "objectID": "posts/sub-exponential-growth/index.html",
    "href": "posts/sub-exponential-growth/index.html",
    "title": "Sub-exponential growth",
    "section": "",
    "text": "대부분의 SIR 모형은 감염병 확산의 메커니즘을 아래와 같은 식으로 표현한다.\n\\[\\frac{\\mathrm{d}I}{\\mathrm{d}t} = \\beta S\\frac{I}{N}.\\]\n말로 설명하자면 다음과 같다. 감수성자가 단위 시간 당 평균적으로 \\(\\beta\\) 의 유효 접촉, 다시 말해 감염자와 만나면 감염이 일어나게 되는 접촉을 하게 된다. 무작위로 접촉을 하는 경우 감염자를 만날 확률은 \\(\\frac{I}{N}\\) 와 같다. \\(N\\)은 총 인구 수를 의미한다. 이러한 감염병 확산 메커니즘을 frequency-dependent 하다고 말한다.\n이와 같은 메커니즘의 결과 중 하나는 감염병의 확산 초기에 감염자의 수가 exponential growth (EG)를 보인다는 것이다. 하지만 Chowell et al. 이 지적한 것처럼 실제 감염병 유행 자료를 살펴 보면 감염자수의 증가 속도는 SIR 모형이 예측하는 것 보다는 느린 속도 sub-exponential growth (SEG) 가 흔히 나타난다. 기존의 연구들에서는 크게 두 가지의 메커니즘을 들어 SEG 를 설명한다. 첫째는 어떤 이유로든 감염병이 확산되고 있는 인구집단에서 inhomogeneous mixing이 일어나는 경우이다. 왜 inhomogeneous mixing이 SEG를 나타내는지 그리고 정량적으로 어떤 관계가 있는지는 다음에 살펴보자. 이번 포스트에서는 사람들이 골고루 섞이지 않음으로 인해 감염병의 발생이 특정 지역 및 집단에 국한 되어 전체적으로는 확산 속도가 느려진다는 정도로 이해해도 되겠다. 이렇게 inhomogeneous mixing 이 나타나는 경우는 집단 내에서 특히 접촉이 많은 소규모 집단이 있다거나 (네트워크 개념을 사용하자면 clustering) 혹은 공간적으로 더 위험한 지역과 덜 위험한 지역이 있다고 가정할 수도 있겠다. Inhomogeneous mixing이외 에도 감염병 확산에 대응하여 사람들이 위험 행동을 줄여나가면 확산 속도가 점차 감소하여 SEG가 나타날 수 있다.\nSEG를 표현하는 간단한 방법 중 하나는 아래 식에서 처럼 \\(\\alpha\\)와 같은 지수를 사용하는 것이다 .\n\\[\\frac{\\mathrm{d}I}{\\mathrm{d}t} = \\beta S\\frac{I^\\alpha}{N}.\\]\n이번 포스팅에서는 SEG을 SEIR 모형을 이용하여 구현하고 모수 추정 후 EG 모형과 비교해보고자 한다.\n우선 아래와 같이 SEIR 모형을 구현한다.\n\nseir_ode &lt;- function(t, y, params) {\n  # state variables \n  S &lt;- y[\"S\"]; \n  E &lt;- y[\"E\"]; \n  I &lt;- y[\"I\"]; \n  R &lt;- y[\"R\"];\n  CI &lt;- y[\"CI\"]\n  \n  epsilon &lt;- params[[\"epsilon\"]] # 1/epsilon = latent period\n  gamma &lt;- params[[\"gamma\"]] # 1/gamma = duration of infectiousness\n  beta &lt;- params[[\"beta\"]] # R0 = beta/gamma\n  alpha &lt;- params[[\"alpha\"]]\n  N &lt;- S + E + I + R # total population size\n  muSE &lt;- beta * S * (I^(alpha)) / N # rate from S to E\n  muEI &lt;- epsilon * E # rate from E to I, i.e., 1/epsilon = latent period\n  muIR &lt;- gamma * I # rate from I to R\n  \n  dS &lt;- - muSE # rate of change for S\n  dE &lt;- muSE - muEI # rate of change for E\n  dI &lt;- muEI - muIR # rate of change for I\n  dR &lt;- muIR # rate of change for R\n  dCI &lt;- muSE # rate of change for R\n  \n  return(list(c(dS, dE, dI, dR, dCI))) # return as a list to use deSolve package\n}\n\n초기 조건을 정의하자. 이 값들은 스크립트에서 global 변수로 사용할 것이다.\n\n# initial conditions as global variables\nI0 &lt;- 10 # initially infected people\ny0 &lt;- c(S=10000 - I0, E=0, I=I0, R=0, CI=0) # initial values for state variables\ntend &lt;- 100 # simulation end time 50 days\ntimes &lt;- seq(0, tend, by=1) # daily output for 150 days\n\n기본 모수를 정의하는 함수와 단위 시간 당 발생자 수를 조사하는 함수를 정의하자. 이 것들은 나중에 모수 추정과정에서 사용될 것이다.\n\n# baseline parameters\npinit &lt;- function(beta=0.3, alpha=1) {\n  params &lt;- list() # parameter input for the SIR model\n  params$epsilon &lt;- 0.5\n  params$gamma &lt;- 0.2\n  params$beta &lt;- beta\n  params$alpha &lt;- alpha\n  \n  return(params)\n}\n\nlibrary(dplyr)\nlibrary(deSolve)\n# incidence over a given time interval, delta_t\nincidence &lt;- function(p, delta_t=7) {\n  parm = pinit()\n  parm$beta &lt;- p[[\"beta\"]]\n  if(length(p) &gt; 1) {\n     parm$alpha &lt;- p[[\"alpha\"]]\n  }\n  ode(y=y0, times=times, func=seir_ode, parms=parm) %&gt;%\n  as.data.frame() -&gt; out\n  di = c(0, diff(out$CI))\n\n  return(di[seq(1, length(di), by=delta_t)])\n}\n\n아래와 같이 모수 추정에 사용할 거짓 자료를 만들어 보자. 전에 사용하 듯이 관찰값은 모형 예측값을 모수로 가지는 푸아송 변수로 가정하자. 기본 값으로 한 주간에 감염자수를 자료로 사용한다. 그림에서 원들은 거짓 관찰값을 점선은 모형 예측값을 나타낸다. 검정색과 빨강색은 각각 EG와 SEG를 나타낸다.\n\n# create fake data\nset.seed(42) # for reproducibility\n\ninc1 = incidence(p=pinit()) # baseline parameters, i.e., alpha=1\ndat1 &lt;- rpois(length(inc1), lambda=inc1)\nplot(dat1, xlab=\"Week\", ylab=\"Number of cases\", main=\"Exponential growth\")\nlines(inc1, col=2)\n\n\n\ninc2 = incidence(p=pinit(beta=0.5, alpha=0.8)) \ndat2 &lt;- rpois(length(inc2), lambda=inc2)\nplot(dat2, xlab=\"Week\", ylab=\"Number of cases\", main=\"Sub-exponential growth\")\nlines(inc2, col=2)\n\n\n\n\n거짓 자료를 이용하여 모수 추정을 하여 보자. 모수 추정 함수를 정의 하기 전에 아래와 같이 간단한 함수들을 정의하자. \\(\\mathrm{expit}\\)은 0과 1사이의 값만 정의되는 모수 (i.e., \\(\\alpha\\)) 를 사용하기 위해서 정의 하였고 이를 다시 원래값으로 되 돌리는 데 사용할 \\(\\mathrm{logit}\\)을 정의하였다. \\(\\mathrm{AIC}\\) (Akaike information criterion)는 모형의 우수함 (quality)의 상대 비교를 위해 사용한다. AIC 는 사용된 모형이 실제 자료를 만들어낸 모형과 다름으로 인해 잃어버리게 되는 정보의 양을 나타내는 상대적인 값이다. \\(\\mathrm{AIC}\\) 값이 작을 수록 정보를 덜 잃어버렸다는 뜻으로 더 우수한 모형을 나타낸다고 할 수 있다. \\(\\mathrm{AIC_c}\\) 는 모수 추정 시 사용된 자료의 수가 적은 경우에 더 적합한 방법이다.\n\\[ \\mathrm{logit}(p) := \\mathrm{ln} (\\frac{p}{1-p})\\] \\[\\mathrm{expit}(x) :=  \\frac{1}{1+\\mathrm{exp}(-x)}\\] \\[\\mathrm{AIC} := 2k - 2\\mathrm{ln}(\\hat{L})\\] \\[ \\mathrm{AIC_c} := \\mathrm{AIC} + \\frac{2 k^2+2 k}{n-k-1}\\]\n\nexpit &lt;- function(x) {\n  1/(1+exp(-x))  \n}\nlogit &lt;- function(x) {\n  log(x/(1-x))  \n}\naic = function(k, L){\n  2*k - 2*log(L)\n}\naicc = function(k, L, n){\n  2*k - 2*log(L) + (2*k^2+2*k)/(n-k-1)\n}\n\n모형 에측값과 관찰값과의 유사성을 측정하기 위해 likelihood 함수를 사용하자. 함수를 최소화 하는 optim의 기본 기능을 사용할 것이기 때문에 negative log likelihood를 정의한다. SEG 모형의 경우 \\(\\beta, \\alpha\\) 두 개의 함수를 추정하자.\n\nnegloglik &lt;- function (p, y) {\n  if (length(p) == 1) x &lt;- incidence(p=pinit(beta=exp(p[1])))\n  if (length(p) == 2) x &lt;- incidence(p=pinit(beta=exp(p[1]), alpha=expit(p[2])))\n  nll &lt;-  - sum(dpois(y, lambda=x, log=T), na.rm=T)\n  return(nll)\n}\n\n\nfit1 = optim(par=log(0.3), fn=negloglik, y=dat1, method=\"Brent\", lower=log(1e-6), upper=log(0.9))\nplot(dat1, xlab=\"Week\", ylab=\"Number of cases\")\nlines(incidence(p=pinit(beta=exp(fit1$par))), col=2, lty=2)\nlegend(1, 100, legend=c(\"Data\", \"Fit\"),\n       col=c(\"black\", \"red\"), lty=c(NA,2), pch=c(1,NA), cex=0.8)\n\n\n\n\n이번에는 이번 포스팅의 주제인 SEG 모형으로 만든 거짓자료를 EG 그리고 SEG 모형 두 가지로 최적화하여 보자.\n\nfit2 = optim(par=log(0.3), fn=negloglik, y=dat2, method=\"Brent\", lower=log(1e-6), upper=log(0.9))\nfit3 = optim(par=c(log(0.3), logit(0.2)), fn=negloglik, y=dat2, method=\"Nelder-Mead\")\n\n# check log likelihood\n-fit2$value\n\n[1] -54.8522\n\n-fit3$value\n\n[1] -33.3477\n\nplot(dat2, xlab=\"Week\", ylab=\"Number of cases\")\nlines(incidence(p=pinit(beta=exp(fit2$par[1]))), col=2, lty=2)\nlines(incidence(p=pinit(beta=exp(fit3$par[1]), alpha=expit(fit3$par[2]))), col=3, lty=2)\nlegend(1, 17, legend=c(\"Data\", \"Exponental fit\", \"Sub-exponential fit\"),\n       col=c(\"black\", \"red\", \"green\"), lty=c(NA,2.2), pch=c(1,NA,NA), cex=0.8)\n\n\n\n\n플롯을 살펴보았을때 SEG 모형을 이용한 피팅이 더 잘 맞아들어가는 것 같은데 \\(\\mathrm{AIC}\\)를 이용해서 모형을 비교해보자. \\(\\mathrm{AIC}\\) 와 \\(\\mathrm{AIC_c}\\) 둘 다 SEG 모형이 자료를 더 잘 설명하는 모형임을 보여준다.\n\n# Akaike information criterion to compare models\naic(k=1, L=exp(-fit1$value)) # 83.75242\n\n[1] 94.48891\n\naic(k=2, L=exp(-fit2$value)) # 69.54993\n\n[1] 113.7044\n\naicc(k=1, L=exp(-fit1$value), n=length(dat2)) #84.06011\n\n[1] 94.7966\n\naicc(k=2, L=exp(-fit2$value), n=length(dat2)) #70.54993\n\n[1] 114.7044"
  },
  {
    "objectID": "posts/reproduction-number/index.html",
    "href": "posts/reproduction-number/index.html",
    "title": "감염재생산지수 계산하기",
    "section": "",
    "text": "코로나19에 효과적으로 대응하고자 방역 당국과 연구자들이 코로나19의 전파 양상을 분석한 결과가 뉴스에 종종 보도 되었는데 그 내용 중에 빠지지 않는 것이 감염재생산지수이다. 영어로는 reproduction number (\\(\\mathcal{R}\\)) 로 불리는 데 한 명의 감염자로부터 야기된 후속 감염자의 수를 말한다. \\(\\mathcal{R}\\)이 1을 넘으면 감염자의 규모가 시간이 지남에 따라 커질 것이고 1보다 작으면 규모가 감소할 것이다. 누가 누구를 감염시켰는지 모두 알고 있다면 감염자들의 수를 세서 \\(\\mathcal{R}\\) 구할 수 있을 것이다. 하지만 한국 코로나 19 상황처럼 확진자가 많아서 모든 환자의 감염 경로를 알지 못하고 일별 확진자 자료를 가지고 있다면 어떻게 \\(\\mathcal{R}\\)을 계산할까? 이 글에서는 이에 관해 살펴보고자 한다.\n\n\n\\(\\mathcal{R}\\)의 정의\n위에서 언급한 것처럼 \\(\\mathcal{R}\\)은 한 명의 감염자에서 야기되는 후속 감염자의 수를 의미한다. 감염을 야기한 사람을 먼저 왔다는 의미로 ‘선행 감염자’ (infector) 그리고 새로이 감염된 사람들을 후에 감염되었다는 의미로 ‘후속 감염자’ (infectee) 라 칭하겠다. 그렇다면 아래와 같은 식을 쓸 수 있을 것 같다. \\[\\mathcal{R} = \\frac{새끼의 수}{어미의 수} =\\frac{후속 감염자 수}{선행 감염자 수} = \\frac{\\textrm{number of infectee}}{\\textrm{number of infector}}\\]\n\n\n\\(\\mathcal{R}\\) 계산 방법\n일별 확진자 자료를 이용하여 (\\(\\mathcal{R}\\))을 구하는 방법을 알아보기 전에 감염 경로를 모두 아는 경우를 살펴보자. 예를 들어 아래 그림과 같이 감염병이 전파되고 있다고 생각해보자. 그림에서 점들은 사람을 나타내고 화살표는 감염이 일어난 방향을 나타낸다. 그리고 0, 1, 2는 세대를 나타내는데 0세대는 외부에서 유입된 최초 감염자를 나타낸다. 측 최초 감염자가 3명을 감염시켰고 후속 감염자들도 각각 3명을 감염시켰다.\n\n2세대 이후의 상황은 모른다 가정하고 2세대까지만 계산에 넣으면 다음과 같이 계산할 수 있을 것이다. \\(\\mathcal{R}=12/4=3\\). 감염이 계속 일어나 총 \\(n\\)명의 인구 집단이 모두 감염되었다면 \\(\\mathcal{R}\\)은 얼마일까? 선행 감염자의 수는 최초의 유입된 감염자를 포함해서 \\(n+1\\) 그리고 후속 감염자의 수는 \\(n\\)이 될 것이다. 즉 \\(\\mathcal{R} = \\frac{n}{n+1}\\). 그리고 \\(n\\)이 큰 경우라면 \\(\\mathcal{R}\\)은 1로 수렴할 것이다.\n본론으로 들어가서 감염 경로는 모른채 일별 확진자수만을 가지고 \\(\\mathcal{R}\\)을 어떻게 계산할까? 아래 그림을 살펴보자. 이 그림은 중국에서 처음 발견된 확진자 수를 나타내는 유행 곡선 (epidemic curve) 이다. 붉은막대는 발열자를 나타내는데 논의의 편의를 위해서 감염자 수라 가정해보자. 녹색 네모로 표시한 2월 17일에 감염된 사람들은 녹색 화살표로 나타낸 것처럼 2월 17 일 이전에 감염된 사람들에 의하여 감염되었을 것이다. 정확히 누구에게 혹은 몇 일에 감염된 사람으로부터 감염되었는지는 알 수 없지만 말이다. 그리고 한 가지 더 알 수 있는 것은 화살표의 두께로 표현한 것처럼 선행 감염자가 언제 감염되었는지에 따라 2월 17일에 후속 감염을 일으킬 수 있는 확률이 다를 수 있다는 사실이다. 달리 표현하면 감염 후 시간이 지남에 따라 후속 감염을 일으킬 수 있는 확률이 변하게 된다는 것을 의미한다.\n\n감염 후 시간에 따라 후속 감염을 일으킬 수 있는 확률이 변할 수 있다는 것은 코로나19에 걸리게 되면 나타나는 일련의 인체 내에서의 변화 및 사람의 생활 습성등을 고려하면 어느 정도 이해할 수 있다. 바이러스에 감염되어 후속 감염자를 만들어 내기 위해서는 바이러스가 인체 내에서 증식해야 하므로 시간이 필요하다. 소위 잠재기 (latent period)가 필요하다. 이후 바이러스가 계속 증식하고 증가하고 감염 확률이 증가할 것이다. 이후 잠복기 (incubation period)를 거쳐 증상이 나타나고 회복기에 접어들면 감염 확률이 줄어들 것이다. 이런한 일련의 인체 반응에 더해 사람의 행동도 감염 확률에 영향을 미칠 것이다. 즉 몸에 바이러스가 아무리 많아도 아파서 타인을 만나지 않는다면 전파는 일어나지 않을 것이다.\n감염 후 시간에 따라 후속 감염을 일으킬 확률은 세대기 (generation interval, generation time, or transmission interval)의 분포를 이용하면 표현이 가능하다. 세대기는 한 감염자가 후속 감염을 일으킬 때 까지 걸리는 시간이다. 코로나19의 세대기는 대체로 아래와 같은 분포를 가진다고 가정해 보자. 즉 감염됨 사람이 후속 감염을 일으키려면 감염 후 하루가 지나야 하고 6일 째가 되면 후속 감염을 일으키지 않는다고 가정해보자.\n\n이걸 역으로 생각해보면 오늘 감염된 사람이 발견된 경우 이 사람을 감염시킨 선행 감염자는 2일-5일 전에 감염되었을 것이다. 이러한 세대기의 분포를 이용하면 \\(\\mathcal{R}\\) 계산식에서 문제가 되었던 부분 즉 분모에 해당하는 선행 감염자 수를 계산해 볼 수 있다. 일별 감염자가 100명씩 열흘간 발생했다고 가정해보자. 감염자 수가 일정하게 유지되고 있으니 계산할 것도 없이 \\(\\mathcal{R}\\)은 1일 것이다. 그래도 위의 논리를 이용하여 계산 하여 보자. 오늘 감염된 사람 100명이 후속 감염자가 되고 2일-5일 전에 감염된 사람이 선행 감염자가 된다. 주의할 점은 2일-5일 사이에 감염된 사람 중 위의 확률에 따라 일부만이 선행 감염자가 된다. \\[\\mathcal{R} = \\frac{후속 감염자 수}{선행 감염자 수} = \\frac{100}{100 \\times 0.25 + 100 \\times 0.35 + 100 \\times 0.25 + 100 \\times 0.15 } = 1\\]"
  },
  {
    "objectID": "posts/pop-monte-carlo/index.html",
    "href": "posts/pop-monte-carlo/index.html",
    "title": "Population Monte Carlo 파퓰레이션 몬테카를로",
    "section": "",
    "text": "최근에 파티클필터링 (particle filtering; PF) 방법을 이용하여 \\(\\mathcal{R}_t\\) 추정하는 과정에 대한 논문을 썼다. 그런데, 항상 의문이었던 것은 PF를 조금만 변형하면 감염병 모형의 감염속도 \\(\\beta=\\mathcal{R}_0 \\gamma\\) 와 같은 time-invariant 파라미터를 추정할 수도 있지 않을까 하는 것이었다. Population Monte Carlo (PMC)가 바로 그 방법이었다.\n이번 포스트에서는 SIR 모형의 모수 \\(\\beta\\)를 PMC 방법으로 추정하여 보았다. 추정하는 PMC 알고리즘을 아래에 구현하였다. 전에 구현했던 particle filtering 와 유사하다. 즉 중요도 샘플링 (importance sampling)을 연속으로 구현하는 데 연속으로 샘플링 하기 위해 Markov Chain Monte Carlo 에서 사용하듯이 proposal 을 이용하여 다음 단계의 샘플을 만들고 중요도 샘플링을 이용하여 추정을 하는 것이다.\n\nlibrary(truncnorm) # draw or evaluate according to a truncated normal dist  \npmc &lt;- function (params = NULL,\n                 x0 = NULL, # initial values\n                 y = NULL, # observation\n                 npart = 1000, # number of particles \n                 niter = 10, # iterations\n                 tend = 100, # to control the number of daily y to be fitted\n                 dt = 0.1, # dt for the ODE integration\n                 prior_mean = 0.5,\n                 prior_sd = 2,\n                 prior_lb = 0,\n                 prior_ub = 2) {\n  \n  # makes it easy to use truncated normal distribution\n  nstate &lt;- length(x0) # number of state variables (i.e., S, I, R, CI)\n  \n  # initial betas are sampled according to the prior distribution\n  beta0 &lt;- rtruncnorm(npart, a=prior_lb, b=prior_ub, mean=prior_mean, sd=prior_sd)\n  beta &lt;- matrix(NA, ncol=npart, nrow=niter) # to store the samples for beta\n  beta[1,] &lt;- beta0 # the initial values for the first row\n  # proposal for the next iteration, which is then resampled according to the  weight\n  sd = sd(beta[1,]) # scale for the proposal is adapted according to the current sample \n  beta[2,] = rtruncnorm(npart, a=prior_lb, b=prior_ub, mean=beta[1,], sd=sd)\n  \n  lik &lt;- matrix(NA, ncol = npart, nrow = niter) # likelihood \n  proposal_prob &lt;- matrix(NA, ncol = npart, nrow = niter)\n  wt &lt;- matrix(NA, ncol = npart, nrow = niter) # weight \n  W &lt;- matrix(NA, ncol = npart, nrow = niter) # normalized weights\n  A &lt;- matrix(NA, ncol = npart, nrow = niter) # Resample according to the normalized weight\n  # initial value  \n  proposal_prob[1,] &lt;- 1\n  wt[1,] &lt;- 1 / npart  # initial weights\n  W[1,] &lt;- wt[1,]\n \n  for (i in 2:niter) {\n    # cat(\"i =\", i, \"\\n\")\n    # tend increases by 1 accounts for the initial values\n    X &lt;- array(0, dim = c(npart, tend+1, nstate),\n               dimnames = list(NULL, NULL, names(x0)))\n    for (nm in names(x0)) {# starting values for each particle\n      X[, 1, nm] &lt;- x0[[nm]]\n    }\n    # run process model (i.e., SIR model) \n    x_1_tend &lt;- \n      process_model(params = params,\n                   x = X,\n                   dt = dt,\n                   beta = beta[i,])\n    # calculate weights (likelihood)\n    lik[i,] &lt;- assign_weights(x = x_1_tend, y = y[1:tend])\n    # normalize particle weights\n    proposal_prob[i,] = dtruncnorm(beta[i,], beta[i-1,], a=prior_lb, b=prior_ub, sd=sd)\n    prior_prob = dtruncnorm(beta[i,], a=prior_lb, b=prior_ub, mean=prior_mean, sd=prior_sd)\n    wt[i,] &lt;- lik[i,] * prior_prob / proposal_prob[i,]\n    \n    W[i,] &lt;- wt[i,] / sum(wt[i,])\n    # resample particles by sampling parent particles according to normalized weights\n    A[i,] &lt;- sample(1:npart, prob=W[i,], replace=T)\n    beta[i,] &lt;- beta[i, A[i,]] # resampled beta according to the normalized weight\n    # sd for the proposal can be adapted in various other ways, but we use the sd of the current sample\n    sd = sd(beta[i,]) \n    # generate proposals for the next iteration\n    if (i &lt; niter) {\n      beta[i+1,] &lt;- rtruncnorm(npart, a=prior_lb, b=prior_ub, mean=beta[i,], sd=sd)\n    } \n  } # end iteration\n  return (list(theta=beta, lik=lik, W=W, A=A))\n}\n\n감염병 확산 과정을 나타내는 SIR 모형을 구현해보자. 파티클수에 따라 벡터형태로 SIR 모형을 구현하였다.\n\nprocess_model &lt;- function (params = NULL,\n                           x = NULL,\n                           dt = 0.1,\n                           beta = NULL) {\n  \n  S &lt;- x[, 1, \"S\"] # a vector of initial S across the particles\n  I &lt;- x[, 1, \"I\"] # a vector of initial I across the particles\n  R &lt;- x[, 1, \"R\"] # a vector of initial S across the particles\n  \n  len &lt;- length(x[1,,\"S\"]) # length of model predictions (same as the data points) + 1 accounting for the initial values\n         \n  N &lt;- S + I + R\n  gamma &lt;- params[[\"gamma\"]]\n  \n  for (j in 2:len) {\n    daily_infected &lt;- 0 # to track the daily infection\n    for (i in seq(dt, 1, dt)) { # steps per day\n      FOI &lt;- beta * I * S/N\n      S_to_I &lt;- FOI * dt\n      I_to_R &lt;- I * gamma * dt\n  \n      S &lt;- S - S_to_I\n      I &lt;- I + S_to_I - I_to_R\n      R &lt;- R + I_to_R\n      \n      daily_infected &lt;- daily_infected + S_to_I\n    }\n    \n    x[, j, \"S\"] &lt;- S\n    x[, j, \"I\"] &lt;- I\n    x[, j, \"R\"] &lt;- R\n    x[, j, \"Inc\"] &lt;- daily_infected\n  }\n  return(x[, 2:len, \"Inc\"])\n}\n\n모수 추정에 사용할 거짓 일별 감염자수를 만들어보자. 위에서 구현한 process_model에서 예측되는 일별 감염자 수를 평균으로 하는 푸아송 분포를 이용하여 만들었다.\n\nparm = list(gamma=0.3) #\nx0 = c(S=9990, I=10, R=0, Inc=0)#\ntend = 50 # the number of observations\n# tend + 1 to account for the initial values\nX &lt;- array(0, dim = c(1, tend+1, 4), \n               dimnames = list(NULL, NULL, names(x0)))\nfor (nm in names(x0)) {# starting values for each particle\n  X[, 1, nm] &lt;- x0[[nm]]\n}\ntruebeta &lt;- 0.6 # true beta\npred &lt;- process_model(params=parm, x=X, beta=truebeta)\ny &lt;- rpois(tend, lambda=round(pred))  # \n\npmc 함수에 사용된 또 다른 함수 assign_weights를 아래에 구현하였다.\n\nassign_weights &lt;- function (x, y) {\n  di &lt;- dim(x)\n  npart &lt;- di[1] # number of particles\n  nobs &lt;- di[2] # number of observations\n  loglik &lt;- rep(NA, npart)\n  for (i in 1:npart) {\n    mean_case &lt;- x[i,] # for the ith particle\n    expected_case &lt;- pmax(0, mean_case)\n    obs_case &lt;- round(y)\n    loglik[i] &lt;- sum(dpois(obs_case, lambda=expected_case, log=T), na.rm=T)\n  }\n  return (exp(loglik)) # convert to normal probability\n}\n\nPMC를 이용하여 모수 추정을 해보고 결과를 그림으로 나타내보자.\n\nset.seed(44)\n# gamma and x0 are set to the same as the model used to generate the data\nparm = list(gamma=0.3)\nx0 = c(S=9990, I=10, R=0, Inc=0)# initial condition\nniter = 50\n# out = pmc(params = parm, x0 = x0, y=y, npart=10000, niter=niter, \n#    tend = length(y), dt=0.1, prior_mean=0.5, prior_sd=0.1, prior_lb=0,\n#    prior_ub=2)\n# saveRDS(out, \"out_20230811.rds\")\nout &lt;- readRDS(\"out_20230811.rds\")\nhist(out$theta[niter,], xlab=expression(beta), main=\"\")\nabline(v=truebeta, col=2, lwd=2)"
  },
  {
    "objectID": "posts/modeling-philosophy/index.html",
    "href": "posts/modeling-philosophy/index.html",
    "title": "모델링이 뭔지는 알고 하나",
    "section": "",
    "text": "감염병 전파를 이해하는 데에는 수리 모형 (mathematical model)이 아주 중요한 역할을 한다. 그런데 어떻게 현실과는 다른 모형을 이용하여 현실에 대해 배울 수 있을까? 스탠포드 철학 백과사전에 이러한 내용을 담고 있는 부분이 있어 여기에 정리를 해본다. 아직 만족한 만한 해답을 찾지는 못했다.\n철학자들은 모형을 만들고 (building), 조작하고 (manipulation), 적용하고 (application), 평가함 (evaluation)으로써 현실에 대해 추론 (reasoning) 을 할 수 있다고 기술하고 있는 것 같다. 이런 과정을 “surrogative reasoning” 혹은 model-based reasoning 이라는 용어로 표현하기도 했다. 추론 과정은 모형 자체를 이해하는 과정과 그 모형에 대한 이해를 현실에 적용하는 과정 두 가지로 나누어 볼 수 있다. 모형을 만드는 과정은 모형의 여러 부분들이 어떻게 서로 맞아 들어가는 지 알게 되는 과정이다. 모형을 조작하는 과정은 모형을 모수(parameter)를 변화시켜가며 시물레이션을 통해 그 결과를 확인하는 과정이 될 것이다. 모형을 만들고 조작함으로써 알게된 지식을 어떻게 그 대상인 (target system)인 현실의 언어로 번역할 수 있을까? 모형이 현실의 일부를 나타내도록 (represent) 만들어 졌다면 즉 다시 말해 우리가 알고자 하는 현실 (예를 들면 백신 접종으로 인한 감염자수의 감소 정도)에 상응하는 부분이 모형에 충실하게 구현되어 있다면 모형을 통해서 얻은 지식이 현실에 적용될 수 있을 것 같다. 적고 보니 너무나 뻔한 이야기인 것 같다. 뭘 읽은 거지 ㅎ?"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "101-1804\nkimfinale@gmail.com\n\n\n174 Solsaem-ro, Gangbuk-gu\nwww.jonghoonk.com\n\n\nSeoul, Korea\n+82-10-9482-2517\n\n\n\n\n\n\n2021-2023 (expected)\n\nMicroMaster, Statistics and Data Science; MITx (online learning initiative of the Massachusetts Institute of Technology)\n\n2006-2010\n\nPhD, Theoretical Epidemiology (Disease Transmission Modeling); University of Michigan, Ann Arbor\nThesis title: Dynamic partnerships and HIV transmissions by stage\n\n2007-2010\n\nMSc, Biophysics and molecular biology; Gwangju Institute of Science and Technology\n\n2007-2010\n\nBSc, Biochemistry; Chungnam National University\n\n\n\n\n\n2008-9\nRackham Fellowship ($20,000) University of Michigan, Ann Arbor 2004 Student Award ($3,000)\nBlue Cross Blue Shield Michigan Foundation 2003-2004 Study Abroad Scholarship ($60,000) Korea Science and Engineering Foundation\n\n\n\nYour Most Recent Work Experience:\nShort text containing the type of work done, results obtained, lessons learned and other remarks. Can also include lists and links:\n\nFirst item\nItem with link. Links will work both in the html and pdf versions.\n\nThat Other Job You Had\nAlso with a short description.\n\n\n\nResearch Scientist at the International Vaccine Institute (2018-present)\n\n\n\nI am a theoretical epidemiologist specializing in the modeling of infectious disease transmission. My research involves utilizing mathematical and statistical models as well as data science techniques to examine the dynamics of disease transmission and evaluate the effectiveness of intervention programs like vaccination. Over the course of my career, I have investigated a wide range of pathogens including HIV, poliovirus, cholera, typhoid fever, non-typhoidal Salmonella disease, and COVID-19. To continue expanding my expertise in the field, I actively pursue new knowledge in machine learning and data science. Additionally, I have shared my knowledge through numerous lectures and participated in advisory panels aimed at controlling the spread of COVID-19 in Korea.\n\n\n\n\nMy Cool Side Project\n\nFor items which don’t have a clear time ordering, a definition list can be used to have named items.\n\nThese items can also contain lists, but you need to mind the indentation levels in the markdown source.\nSecond item.\n\n\nOpen Source\n\nList open source contributions here, perhaps placing emphasis on the project names, for example the Linux Kernel, where you implemented multithreading over a long weekend, or node.js (with link) which was actually totally your idea…\n\nProgramming Languages\n\nfirst-lang: Here, we have an itemization, where we only want to add descriptions to the first few items, but still want to mention some others together at the end. A format that works well here is a description list where the first few items have their first word emphasized, and the last item contains the final few emphasized terms. Notice the reasonably nice page break in the pdf version, which wouldn’t happen if we generated the pdf via html.\n\n\nsecond-lang: Description of your experience with second-lang, perhaps again including a [link] ref, this time placing the url reference elsewhere in the document to reduce clutter (see source file).\n\n\nobscure-but-impressive-lang: We both know this one’s pushing it.\n\n\nBasic knowledge of C, x86 assembly, forth, Common Lisp\n\n\n\n\n\n\nHuman Languages:\n\nEnglish (native speaker)\n???\nThis is what a nested list looks like.\n\nRandom tidbit\nOther sort of impressive-sounding thing you did"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "2021-2023 (expected)\n\nMicroMaster, Statistics and Data Science; MITx (online learning initiative of the Massachusetts Institute of Technology)\n\n2006-2010\n\nPhD, Theoretical Epidemiology (Disease Transmission Modeling); University of Michigan, Ann Arbor\nThesis title: Dynamic partnerships and HIV transmissions by stage\n\n2007-2010\n\nMSc, Biophysics and molecular biology; Gwangju Institute of Science and Technology\n\n2007-2010\n\nBSc, Biochemistry; Chungnam National University"
  },
  {
    "objectID": "cv.html#honors-and-awards",
    "href": "cv.html#honors-and-awards",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "2008-9\nRackham Fellowship ($20,000) University of Michigan, Ann Arbor 2004 Student Award ($3,000)\nBlue Cross Blue Shield Michigan Foundation 2003-2004 Study Abroad Scholarship ($60,000) Korea Science and Engineering Foundation"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "Your Most Recent Work Experience:\nShort text containing the type of work done, results obtained, lessons learned and other remarks. Can also include lists and links:\n\nFirst item\nItem with link. Links will work both in the html and pdf versions.\n\nThat Other Job You Had\nAlso with a short description."
  },
  {
    "objectID": "cv.html#current-position",
    "href": "cv.html#current-position",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "Research Scientist at the International Vaccine Institute (2018-present)"
  },
  {
    "objectID": "cv.html#research-interests",
    "href": "cv.html#research-interests",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "I am a theoretical epidemiologist specializing in the modeling of infectious disease transmission. My research involves utilizing mathematical and statistical models as well as data science techniques to examine the dynamics of disease transmission and evaluate the effectiveness of intervention programs like vaccination. Over the course of my career, I have investigated a wide range of pathogens including HIV, poliovirus, cholera, typhoid fever, non-typhoidal Salmonella disease, and COVID-19. To continue expanding my expertise in the field, I actively pursue new knowledge in machine learning and data science. Additionally, I have shared my knowledge through numerous lectures and participated in advisory panels aimed at controlling the spread of COVID-19 in Korea."
  },
  {
    "objectID": "cv.html#technical-experience",
    "href": "cv.html#technical-experience",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "My Cool Side Project\n\nFor items which don’t have a clear time ordering, a definition list can be used to have named items.\n\nThese items can also contain lists, but you need to mind the indentation levels in the markdown source.\nSecond item.\n\n\nOpen Source\n\nList open source contributions here, perhaps placing emphasis on the project names, for example the Linux Kernel, where you implemented multithreading over a long weekend, or node.js (with link) which was actually totally your idea…\n\nProgramming Languages\n\nfirst-lang: Here, we have an itemization, where we only want to add descriptions to the first few items, but still want to mention some others together at the end. A format that works well here is a description list where the first few items have their first word emphasized, and the last item contains the final few emphasized terms. Notice the reasonably nice page break in the pdf version, which wouldn’t happen if we generated the pdf via html.\n\n\nsecond-lang: Description of your experience with second-lang, perhaps again including a [link] ref, this time placing the url reference elsewhere in the document to reduce clutter (see source file).\n\n\nobscure-but-impressive-lang: We both know this one’s pushing it.\n\n\nBasic knowledge of C, x86 assembly, forth, Common Lisp"
  },
  {
    "objectID": "cv.html#extra-section-call-it-whatever-you-want",
    "href": "cv.html#extra-section-call-it-whatever-you-want",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "Human Languages:\n\nEnglish (native speaker)\n???\nThis is what a nested list looks like.\n\nRandom tidbit\nOther sort of impressive-sounding thing you did"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Jong-Hoon Kim, and I live in Seoul, South Korea. I am a theoretical epidemiologist who uses mathematical, statistical, and machine-learning models to study infectious disease epidemiology. My work involves developing theories about infectious disease transmission, making predictions, and assessing the effectiveness of intervention strategies to improve public health. Currently, in 2023, I work at the International Vaccine Institute, which is located in Seoul, South Korea. My current research primarily focuses on typhoid fever, non-typhoidal Salmonella, cholera, and COVID-19.\nPlease email me if you are interested in collaborating with me"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jong-Hoon’s Blog",
    "section": "",
    "text": "odin package\n\n\n\n\n\n\n\nODE\n\n\nR\n\n\nodin\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nPubMed search, ChatGPT summary, and sending an email in R\n\n\n\n\n\n\n\nChatGPT\n\n\nR\n\n\nxml\n\n\nhttr\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nShapefile의 polygon 면적 구하기\n\n\n\n\n\n\n\nmap\n\n\nR\n\n\nggplot2\n\n\nsf\n\n\nRColorBrewer\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nggplot2로 지도 그리기\n\n\n\n\n\n\n\nmap\n\n\nR\n\n\nggplot2\n\n\nsf\n\n\nRColorBrewer\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nWriting a paper: Start with an outline\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nImportant figures from the book, How to avoid a climate diaster? by Bill Gates\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nRegression toward the mean\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nSurvivor bias\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nMaximum Likelihood and Profile Likelihood for the SEIR model\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasic reproduction number using SymPy\n\n\n\n\n\n\n\nMonte Carlo\n\n\nR\n\n\nparameter estimation\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nPopulation Monte Carlo 파퓰레이션 몬테카를로\n\n\n\n\n\n\n\nMonte Carlo\n\n\nR\n\n\nparameter estimation\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nSimple mathematical models with very complicated dynamics\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nODE models in Stan\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nSub-exponential growth\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\n모델링이 뭔지는 알고 하나\n\n\n\n\n\n\n\nmodeling\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\n감염재생산지수 계산하기\n\n\n\n\n\n\n\nmodeling\n\n\nreproduction number\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ode-in-stan/index.html",
    "href": "posts/ode-in-stan/index.html",
    "title": "Sub-exponential growth",
    "section": "",
    "text": "Stan은 통계 모형 뿐 아니라 ODE 모형을 시물레이션하고 모수를 추정하는 데에도 유용하다. 이 포스팅에서는 일별 감염자 자료가 주어졌을 경우 Stan을 이용하여 SIR 모형의 두 개의 모수 (\\(\\beta, \\gamma\\))를 추정하는 과정을 기술하겠다. 먼저 deSolve 패키지 양식을 따라 SIR 모형을 아래와 같이 구현하고 모형에서 예측되는 일별 감염자 자료 (dayinc) 를 평균으로 하는 거짓 관찰값을 만든다 (yobs).\n\nsir &lt;- function(t, state, parameters) {\n  with(as.list(c(state, parameters)),{\n    # rate of change\n    N &lt;- S + I + R\n    dS &lt;- - beta*S*I/N \n    dI &lt;- + beta*S*I/N - gamma*I\n    dR &lt;- + gamma*I\n    dCI &lt;- + beta*S*I/N \n    \n    # return the rate of change\n    list(c(dS, dI, dR, dCI))\n  }) # end with(as.list ...\n}\n\ny0 &lt;- c(S=999, I=1, R=0, CI=0)\nparms &lt;- c(beta=0.6, gamma=0.4)\ntimes &lt;- seq(0, 40, by = 1)\n\nlibrary(dplyr)\ndeSolve::ode(y=y0, times=times, func=sir, parms=parms) %&gt;% \n  as.data.frame() -&gt; out\n\ndayinc &lt;- diff(out$CI)\nset.seed(42)\nyobs &lt;- rpois(length(dayinc), lambda=dayinc)\n\ndf &lt;- data.frame(time=1:length(dayinc), \n                 model=dayinc,\n                 obs=yobs)\nlibrary(ggplot2)\n# the ggplot theme was adopted from the following website: https://mpopov.com/tutorials/ode-stan-r/\n\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(df)+ \n  geom_line(aes(time, model, color=\"Model\"), linetype=\"dashed\")+\n  geom_point(aes(time, model, color=\"Model\"))+\n  geom_line(aes(time, obs, color=\"Observation\"), linetype=\"dashed\")+\n  geom_point(aes(time, obs, color=\"Observation\"))+\n  labs(x=\"Time (day)\", y=\"Daily incidence\", title=\"Incidence from the SIR model\")+\n  scale_color_manual(\"\", values=c(\"Model\"=\"black\",\"Observation\"=\"firebrick\"))+\n  theme(legend.position=\"bottom\")\n\n\n\n\n아래와 같이 Stan 모형을 만든다. Posterior predictive check 을 하기 위해 generated quantities 블록에 ypred 변수를 넣었다.\n\nstan_code &lt;- \"functions {\n  vector sir(real t,        // time\n             vector y,      // state\n             vector theta  // parameters\n             ) {      \n    vector[4] dydt;\n        \n    real S = y[1];\n    real I = y[2];\n    real R = y[3];\n    real N = S + I + R;\n    \n    real beta = theta[1];\n    real gamma = theta[2];\n    \n    dydt[1] = - beta * S * I / N;\n    dydt[2] = beta * S * I / N - gamma * I;\n    dydt[3] = gamma * I;\n    dydt[4] = beta * S * I / N;\n    \n    return dydt;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; T;\n  real t0;\n  array[T] real ts; \n  vector[4] y0;\n  int y_obs[T];\n}\n\nparameters {\n  vector&lt;lower=0&gt;[2] theta; // [beta, gamma]\n}\n\nmodel {\n  array[T] vector[4] mu = ode_rk45(sir, y0, t0, ts, theta);\n  real dayinc[T]; // daily incidence\n  dayinc[1] = mu[1, 4] + 1e-12;\n  for (t in 2:T){\n    dayinc[t] = mu[t, 4] - mu[t-1, 4] + 1e-12; \n  }\n  theta ~ exponential(1); // both parameters are on the positive real line\n  y_obs ~ poisson(dayinc); // likelihood\n}\n\ngenerated quantities {\n  array[T] vector[4] mu = ode_rk45(sir, y0, t0, ts, theta);\n  real dayinc[T];\n  dayinc[1] = mu[1, 4] + 1e-12;\n  for (t in 2:T){\n    dayinc[t] = mu[t,4] - mu[t-1,4] + 1e-12;\n  }\n  int ypred[T]; // posterior predictive \n  for (t in 1:T) {\n    ypred[t] = poisson_rng(dayinc[t]);\n  }\n}\n\"\n\n아래와 같이 Stan 모형을 이용해서 샘플링을 한다.\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\n# this is for the stan model in a separate file\n# mod &lt;- stan_model(file=paste0(getwd(),\"/stan/sir_stan.stan\"),\n#                   verbose=TRUE)\nmod &lt;- stan_model(model_code=stan_code, verbose=TRUE)\nT &lt;- 40 # end time unit for the ODE model, also the number of data points\ndata &lt;- list(T=T, t0=0.0, ts=1:T, y0=c(999,1,0,0), y_obs=yobs)\nsmp &lt;- sampling(object=mod, data=data, seed=42, chains=4, iter=2000)\n# saveRDS(smp, \"outputs/stan_smp_20230801.rds\")\n\n모수의 posterior 분포를 살펴보자.\n\n# smp &lt;- readRDS(\"outputs/stan_smp_20230801.rds\")\nsmp &lt;- readRDS(\"stan_smp_20230801.rds\") # file is under the content/post/the_relevant_post_name/index_files/figure_html\ndf &lt;- as.data.frame(smp)\npr &lt;- c(0.5,0.025,0.975)\nd &lt;- as.data.frame(t(apply(df[,grepl(\"^theta.*\", names(df))],\n                           2, quantile, probs=pr)))\nd$name &lt;- c(\"beta\", \"gamma\")\nd$true &lt;- c(0.6, 0.4)\nggplot(d)+ \n  geom_errorbar(aes(x=name, ymin=`2.5%`, ymax=`97.5%`), width=0.0)+\n  geom_point(aes(x=name, y=`50%`, color=\"Estimates\"), size=2)+\n  geom_point(aes(x=name, y=true, col=\"True value\"), size=3)+\n  scale_color_manual(values=c(\"Estimates\"=\"black\",\"True value\"=\"firebrick\"))+\n  labs(x=\"\", y=\"\", title=\"Median estimates with 95% CrI\")+\n  theme(legend.position=\"bottom\", legend.title=element_blank())+\n  scale_x_discrete(breaks=c(\"beta\",\"gamma\"),\n                   labels=c(expression(beta),expression(gamma)))+\n  coord_flip()\n\n\n\n\n마지막으로 posterior predictive check을 통해서 모수 추정을 위해 사용했던 자료와 비교해 보자.\n\n# pr &lt;- c(0.5,0.025,0.975)\nd &lt;- as.data.frame(t(df[,grepl(\"^ypred.*\", names(df))]))\nd$time &lt;- 1:40\ndlong &lt;- tidyr::pivot_longer(d, cols=-time)\ndayincdf &lt;- data.frame(inc=dayinc, time=1:40)\nyobsdf &lt;- data.frame(obs=yobs, time=1:40)\n\nggplot(dlong)+ \n  geom_line(aes(time, value, group=name, color=\"Posterior predictive\"))+\n  geom_line(data=dayincdf, aes(time, inc, color=\"Model\"))+\n  geom_point(data=yobsdf, aes(time, obs, color=\"Observation\"))+\n  geom_line(data=yobsdf, aes(time, obs, color=\"Observation\"), linetype=\"dashed\")+\n  labs(x=\"Time (day)\", y=\"Daily incidence\", title=\"Posterior predictive check\")+\n  scale_color_manual(\"\", values=c(\"Model\"=\"black\",\"Posterior predictive\"=\"grey\",\"Observation\"=\"firebrick\"))+\n  theme(legend.position=\"bottom\")"
  },
  {
    "objectID": "posts/R0-sympy/index.html",
    "href": "posts/R0-sympy/index.html",
    "title": "Population Monte Carlo 파퓰레이션 몬테카를로",
    "section": "",
    "text": "감염병의 전파를 이해하는 데 있어 가장 기본적인 개념이 재감염지수, 특히 기초재감염지수 (\\(\\mathcal{R}_0\\)) 이다. 재감염지수는 한 명의 감염자로부터 생산되는 평균 후속 감염자의 수를 일컫는데 기초재감염지수는 코로나19의 경우 처럼 인구 집단에 면역력을 가진 사람이 없어 모든 사람이 감염될 수 있는 상태하 에서의 재감염지수를 말한다. 기초 재감염 지수는 다음과 같은 수식으로 표현할 수 있다.\n\\[ \\mathcal{R}_0 = \\beta c D \\]\n\\(\\beta\\) 는 한 명의 감염자가 타인을 접촉할 때 상대방을 감염시킬 수 있는 확률, \\(c\\) 는 단위 시간 당 접촉이 일어나는 횟수, \\(D\\) 는 감염 상태가 지속되는 시간을 나타낸다. \\(\\beta\\) 만으로 \\(\\beta c\\) 를 대신해 사용하는 경우도 흔하다. 그 경우 \\(\\beta\\) 는 단위 시간 당 후속 감염자의 수로 표현할 수 있을 것 같다. 미분방정식에 기반한 감염병 모형의 경우는 \\(\\mathcal{R}_0\\)를 어떻게 계산할까? 아래와 같이 SIR 모형을 정의해 보자.\n\\[\\begin{align}\n\\mathrm{d}S/\\mathrm{d}t &= -\\beta I S/N \\\\\n\\mathrm{d}I/\\mathrm{d}t &= \\beta I S/N - \\gamma I\\\\\n\\mathrm{d}R/\\mathrm{d}t &= \\gamma I\n\\end{align}\\]\n위의 정의에서 사용되었던 개념을 적용한다면 \\(\\mathcal{R}_0 = \\beta/\\gamma\\) 라고 할 수 있다. 이는 \\(\\mathcal{R}_0\\)가 감염병이 집단 내에서 유행을 일으킬 수 있는 역치조건임을 이용해도 동일한 결론에 이를 수 있다. (i.e., \\(\\mathrm{d}I/\\mathrm{d}t&gt;0\\))\n위와는 달리 Diekmann et al. 에 의해서 도입된 next generation 방법으로 좀 더 다양한 상황 하에서 \\(\\mathcal{R}_0\\)를 구할 수 있다. 이 방법에서는 \\(\\mathcal{R}_0\\)가 next generation operator의 spectral radius 가 된다. 위 논문 보다는 van den Driessche et al. 가 좀 더 이해하기 쉬운 것 같아 이 방법을 기준으로 살펴보겠다. 그리고 그 계산을 python의 SymPy 라이브러리를 이용해서 구현을 해보겠다. 먼저 간단한 우선 ( SEIR ) 모형의 경우부터 살펴보자.\nNext generation operator \\(G\\) 은 전파를 통해서 생산되는 새로운 감염이 발생하는 속도를 나타내는 행렬 \\(F\\)와 감염이 다른 상태로 변화되는 속도 (V)로 구성되며 다음과 같은 관계를 갖는다. \\(G=FV^{-1}\\). 그리고 \\(R_0\\)는 \\(G\\)의 spectral radius가 된다. 아래 파이썬 구현에서는 \\(\\beta, \\gamma\\) 를 b, g로 나타내었다.\n\nfrom sympy import *\nb, k, g, = symbols('p k g')\nF = Matrix([[0, b],[0, 0]])\nV = Matrix([[k, 0], [-k, g]])\nM = F*(V**-1)\neigval = M.eigenvals(simplify=true)\ninit_printing(use_unicode=True)\neigval\n\n⎧      p   ⎫\n⎨0: 1, ─: 1⎬\n⎩      g   ⎭\n\n#⎧      b   ⎫\n#⎨0: 1, ─: 1⎬\n#⎩      g   ⎭\nlst = list(eigval.keys())\nlst[1]\n\np\n─\ng\n\n#b\n#─\n#g\n\n위에서 언급한 SEIR 모형의 경우는 너무 간단하니 그 보다 조금 더 복잡한 그 예로 다음 연구를 살펴보자. Pitzer et al.의 연구인데 사용된 감염병 모형은 사람 간 직접 전파와 물을 통한 간접 전파 두 가지의 전파 메케니즘을 구현 하였고 (\\(\\lambda_p\\) 와 \\(\\lambda_w\\)) 최초 감염 \\(S_1 \\rightarrow I_1\\) 과 중복 감염\\(R \\rightarrow S_2 \\rightarrow I_2\\) 을 다르게 취급하였다. 부록 (supplementary material)을 보면 사용된 미분식을 볼 수 있다.\n\\[\\begin{align}\n\\mathrm{d}S_1/\\mathrm{d}t &= B + \\epsilon S_2 - (\\lambda_p+\\lambda_w-\\mu)S_1\\\\\n\\mathrm{d}I_1/\\mathrm{d}t &= (\\lambda_p+\\lambda_w)S_1 - (\\delta+\\mu) I_1 \\\\\n\\mathrm{d}R/\\mathrm{d}t &= \\delta(1-\\theta-\\alpha)(I_1+I_2) - (\\omega +\\mu)R \\\\\n\\mathrm{d}C/\\mathrm{d}t &= \\delta\\theta(I_1+I_2) - \\mu C \\\\\n\\mathrm{d}S_2/\\mathrm{d}t &= \\omega R -\\epsilon S_2 - (\\lambda_p+\\lambda_w-\\mu) S_2\\\\\n\\mathrm{d}I_2/\\mathrm{d}t &= (\\lambda_p+\\lambda_w) S_2 - (\\delta+\\mu) I_2 \\\\\n\\mathrm{d}W/\\mathrm{d}t &= \\gamma(I_1+rI_2+rC) - \\xi W\n\\end{align}\\]\n또한 아래와 같이 기초재감염지수도 계산 결과를 보여준다. SymPy를 통해 동일한 결과를 얻을 수 있는지 확인해보자.\n\\[\\begin{align}\nR_0 = \\frac{1}{\\mu+\\delta} \\left(\\beta_p +\\frac{\\gamma \\beta_w}{\\xi}\\right) \\left(1 +\\frac{\\delta\\theta r}{\\mu}\\right)\n\\end{align}\\]\n\np, r, w, N, d, m, t, m, g, x = symbols('p r w N d m t m g x')\nF = Matrix([[p, r*p, r*p, w*N], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\nV = Matrix([[d+m, 0, 0, 0], [0, d+m, 0, 0], [-d*t, -d*t, m, 0], [-g, -r*g, -r*g, x]])\nM = F*(V**-1)\neigval = M.eigenvals(simplify=true)\ninit_printing(use_unicode=True)\neigval \n\n⎧      (N⋅g⋅w + p⋅x)⋅(d⋅r⋅t + m)   ⎫\n⎨0: 3, ─────────────────────────: 1⎬\n⎩             m⋅x⋅(d + m)          ⎭\n\nlst = list(eigval.keys())\nR0_eig = lst[1]\nR0 = (1/(d+m))*(p+N*g*w/x)*(1+(d*t*r)/m) # R0 from the Pitzer (2014)\nsimplify(R0-R0_eig) # 0 for the same expression (symbolic assessment)\n\n0\n\n#0\nR0.equals(R0_eig) # True for the same expression (numerical assessment)\n\nTrue\n\n#True\n\n부록에 보면 기초 재감염지수에 이르는 상세한 과정이 나와있는데 \\(V_{3,3}\\) 에 오류가 있음을 알아냈다. \\(\\delta +\\mu\\) 가 \\(\\mu\\) 로 바뀌어야 한다. 이유는 위 미분식에서 \\(C\\) 식을 보면 알 수 있는데 이는 만성 감염자를 나타내고 따라서 회복을 의미하는 \\(\\delta\\) 가 없어야 한다. 이는 기록하는 과정에서의 오류인 듯 하고 결과로 얻어진 \\(R_0\\) 는 우리가 계산한 결과와 동일하다. 다만, 계산 결과를 식 (18) 에서와 같이 의미있는 구획으로 나누어서 표현하려면 SymPy 결과를 직접 수정하여야 한다."
  },
  {
    "objectID": "posts/simple-model-complicated-dynamics/index.html",
    "href": "posts/simple-model-complicated-dynamics/index.html",
    "title": "Simple mathematical models with very complicated dynamics",
    "section": "",
    "text": "Simple mathematical models with very complicated dynamics\nRobert M. May Nature Vol. 261 June 10, 1976\nThis article discusses a simple first order difference equations that can display very complicated dynamics.\n\\[X_{t+1} = F(X_t)\\]\nIn biological population, the nonlinear function \\(F(x)\\) often has the following properties. \\(F(0)=0\\); \\(F(x)\\) increases monotonically as \\(X\\) increases through the range of \\(0&lt;X&lt;A\\) (with \\(F(x)\\) attaining its maximum value at \\(X=A\\)); \\(F(X)\\) decreases monotonically as \\(X\\) increases beyond \\(X=A\\) \\(N_{t+1} = N_t(a-bN_t)\\)\n\\(X_{t+1} = a X_t (1-X_t)\\)\nX must remain on the interval \\(0&lt;X&lt;1\\); if \\(X\\) ever exceeds unity, subsequent iterations diverge towards \\(-\\infty\\). Furthermore, \\(F(X)\\) attains a maximum value of \\(a/4\\) at \\(X=1/2\\); the equation therefore possesses non-trivial dynamical behaviour only if \\(a&lt;4\\). On the other hand, all trajectories are attracted to \\(X=0\\) if \\(a&lt;1\\).\n\n# function to compute the value at the next time step\n# 0 &lt; x &lt; 1\n# a &lt; 1 for x to go to zero\n# a &gt; 4 leads to x &gt; 1 at one point, which then leads to - infinity\n# 1 &lt; a &lt; 4 for x to exhibit non-trivial dynamics\nx_next &lt;- function(a, x){\n  a*x*(1-x)\n}\n\nx0 = seq(0.01, 0.99, 0.01)\na = c(2.707, 3.414) # values were adopted from the paper by May Nature Vol. 261 June 10, 1976\nxnext = sapply(x0, function(x) x_next(a, x))\n\nplot(x0, xnext[1,], type='l', ylim=c(0,1), xlim=c(0,1),\n     xlab=expression(X[t]), ylab=expression(X[t+1]))\nlines(x0, xnext[2,])\nlines(0:1, 0:1) # line y = x\n\nxstar = 1 - 1/a # points where X(t+1) = X(t)\npoints(xstar[1], xstar[1])\npoints(xstar[2], xstar[2], col=2)\n# slope at the point x given a\ndx &lt;- function(a,x){\n  -2*a*x+a\n}\n\n# function to compute intercept at the given slope b and point x\nintcpt = function(b,x){\n  x - b*x\n}\n\nabline(a=intcpt(b=dx(a=a,x=xstar[1]),x=xstar[1]), b=dx(a=a,x=xstar[1]), lty=2)\nabline(a=intcpt(b=dx(a=a,x=xstar[2]),x=xstar[2]), b=dx(a=a,x=xstar[2]), lty=2, col=2)\n\n\n\n\n\nx_iter &lt;- function(a, x, iter, func){\n  xvec = rep(NA, iter)\n  xvec[1] = x\n  for(i in 2:iter){\n    xvec[i] = func(a, xvec[i-1])  \n  }\n  return(xvec)\n}\nplot(x_iter(2.9, 0.8, 100, x_next), type=\"l\")\n\n\n\n\n\\(X_{t+1} = X_t \\textrm{exp}[r(1-X_t)]\\)\n\nx_next_exp &lt;- function(r, x){\n  x*exp(r*(1-x))\n}\n\nplot(x_iter(2, 0.8, 100, x_next_exp), type=\"l\")\n\n\n\n\n\nx_next2 &lt;- function(r, x){\n x1 &lt;- a*x*(1-x)\n x2 &lt;- a*x1*(1-x1)\n return(x2)\n}\n\nxnext = sapply(x0, function(x) x_next2(a, x))\n\nplot(x0, xnext[1,], type='l', ylim=c(0,1), xlim=c(0,1),\n     xlab=expression(X[t]), ylab=expression(X[t+2]))\nlines(x0, xnext[2,])\nlines(0:1, 0:1) # line y = x"
  },
  {
    "objectID": "posts/regression-toward-mean/index.html",
    "href": "posts/regression-toward-mean/index.html",
    "title": "Regression toward the mean",
    "section": "",
    "text": "In his lecture Joseph Blitzstein talks about two basic statistical phenomena: Regression toward to the mean (RTTM) and survivor bias. The RTTM is today’s topic. Topic was mainly written by GPT 4.\nRegression toward the mean is a statistical phenomenon where extreme data points are likely to be followed by less extreme ones when measured again. In simpler terms, it means that if an extreme observation is observed, the next observation is likely to be closer to the mean or average.\n\nWhy does it happen?\nIt’s primarily a matter of probability. Extreme values are, by definition, rare. So, when you take a second measurement, it’s simply more probable that the new value will be closer to the mean than the previous extreme value was.\n\n\nExamples:\nSports Performance: Imagine a basketball player who has an outstanding game, scoring well above their average number of points. If they’ve played at such an exceptional level, it’s likely that in the next game they will score closer to their average (not necessarily because their skill has decreased, but simply due to the natural variability in performance).\nStudent Test Scores: If a student who typically scores around the average on exams gets an exceptionally high score on one test, they might score closer to their average on a subsequent test. Conversely, if they score exceptionally low on one test, they might score higher the next time.\nInvestment Returns: If a particular stock has an exceptionally good year with returns way above the market average, it might have more modest (or even below-average) returns the following year.\nHeight of Parents and their Children: This is a classic example by Sir Francis Galton. He found that very tall parents tend to have children who are shorter than them (but still above average), and very short parents tend to have children who are taller than them (but still below average). The children’s heights tend to “regress” towards the mean height.\nMedical Treatments: If patients are selected for a clinical trial because they have exceptionally high blood pressure, some of them will probably show a reduction in blood pressure over time even without any treatment. This isn’t because of any therapeutic effect, but simply because their initial measurements were unusually high and subsequent measurements tend to be closer to the mean.\n\n\nMisunderstandings:\nThis concept is often misunderstood. For instance, if a student performs poorly on a test and then improves on the next one, it might be tempting to attribute this improvement to a particular intervention (like tutoring). While the intervention might have had an effect, it’s also possible that some of the improvement was due to regression toward the mean.\nIn research and experimental designs, this phenomenon needs to be taken into account, especially when making causal inferences from observed changes.\nToward the end of his talk, he mentions about the quote by Daniel Kahneman that very clearly explain the concept of RTTM.\nI had the most satisfying Eureka experience of my career while attempting to teach flight instructors that praise is more effective than punishment for promoting skill-learning. …. [A flight instructor objected:] “On many occasions I have praised flight cadets for clean execution of some aerobatic maneuver, and in general when they try it again, they do worse. On the other hand, I have often screamed at cadets for bad execution, and in general they do better the next time. So please don’t tell us that reinforcement works and punishment does not, because the opposite is the case.” …\nThis was a joyous moment, in which I understood an important truth about the world: because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them.\nFull details of Kahneman’s quote can be found from the following article"
  },
  {
    "objectID": "posts/max-likelihood/index.html",
    "href": "posts/max-likelihood/index.html",
    "title": "Maximum Likelihood and Profile Likelihood for the SEIR model",
    "section": "",
    "text": "통계학은 많은 부분 확률모형의 모수를 추정하는 (inferential statistics) 과정이고 모수 추정방법으로 가장 많이 사용되는 방법이 maximum likelihood (ML)이다. 이번 포스트는 2014년 출간된 Cole et al.의 Maximum Likelihood, Profile Likelihood, and Penalized Likelihood: A Primer을 차용하여 maximum likelihood (ML) 와 profile likelihood에 대하여 기술하여 보고자 한다.\n\n최대 가능도 (ML)\n잠복기를 추정하기 위해 증상 발현일과 기존 감염자와의 접촉일을 묻는 설문조사를 했다고 하자. \\(n\\) 명을 인터뷰하고 \\(n\\) 개의 관찰값 \\(y_1, y_2, ..., y_n\\) 을 얻었다고 하자. 잠복기의 분포에 대한 확률모형 \\(f (y|\\boldsymbol{\\theta})\\) 은 주어진 모수\\(\\boldsymbol{\\theta}=(\\theta_1, \\theta_2, ..., \\theta_j)\\) 하에서 \\(Y=y\\)가 될 확률을 나타낸다.\n최대 가능도 방법은 미지의 모수 하에서 관찰값의 확률의 나타낸다. 확률 모형 \\(f(y|\\boldsymbol{\\theta})\\)이 주어진 모수 \\(\\boldsymbol{\\theta}\\) 하에서 \\(Y\\)의 확률을 나타내는 반면 최대 가능도 방법은 \\(Y\\)를 관찰값에 고정한 채 \\(\\boldsymbol{\\theta}\\)의 함수로 표현하게 된다. 따라서 확률모형과는 다르게 다음과 같은 식 \\(\\mathcal{L}(\\boldsymbol{\\theta};y_i)\\) 을 사용한다. 즉, 우리가 관심있어 하는 것은 확률모형 \\(f (y|\\boldsymbol{\\theta})\\)이 \\(Y\\) 가 아니고 \\(\\boldsymbol{\\theta}\\) 에 따라 어떻게 변하는가 하는 것이다. \\(\\mathcal{L}(\\boldsymbol{\\theta};y_i)\\) 를 \\(i\\) 번째 관찰값이 가능도에 영향을 미치는 정도라 하고 관찰값이 상호독립적이라고 가정하면 관찰값 전체의 가능도는 아래와 같이 표현할 수 있다.\n\\[\\mathcal{L}(\\boldsymbol{\\theta};\\boldsymbol{y}) = \\prod_{i=1}^{n} \\mathcal{L}(\\boldsymbol{\\theta};y_i) = \\prod_{i=1}^{n} f(y_i;\\boldsymbol{\\theta})\\]\n위 식에서 \\(\\boldsymbol{y}=(y_1, y_2, ..., y_n)\\)을 나타낸다.\n\\(\\mathcal{L}(\\boldsymbol{\\theta};\\boldsymbol{y})\\) 는 \\(\\boldsymbol{\\theta}\\)에 대한 확률을 알 수는 없기 때문에 확률모형이 아닌 가능도 (혹은 우도) 함수라고 한다. ML은 가능도 함수를 최대로 만들어 주는 \\(\\boldsymbol{\\theta}\\)로 모수에 대한 추정치를 정의한다.\n\\[\\hat{\\theta} = \\textrm{argmax}_{\\theta}\\{{\\mathrm{log} \\mathcal{L}(\\theta)}\\}\\]\n위 식에서 \\(\\mathrm{log}\\)를 사용한 이유는 가능도 값이 매우 작은 수가되는 경우가 많고 따라서 컴퓨터를 이용한 계산상의 안정성을 위해서 (i.e., arithmetic underflow 가 일어나지 않게 하기 위해) 실제로는 \\(\\mathrm{log} \\mathcal{L}(\\theta)\\)를 사용하기 때문이다. 추가적으로 많은 최적화 알고리듬의 경우 최소화가 기본값으로 설정되어 있어 최대 가능도법을 구현할 때는 \\(-\\mathrm{log} \\mathcal{L}(\\theta)\\)를 사용하는 경우가 많다.\n최대 가능도법을 이용하여 푸아송 분포의 모수를 추정하는 과정을 살펴보자.\n\n\n푸아송 분포 모수 추정\n위에서 언급했던 잠복기의 예를 살펴보자. 잠복기는 Weibull, Gamma, 혹은 Lognormal 등 두 개의 모수를 가지는 확률모형이 많이 사용되는 데 아래 예에서는 계산상의 편의를 위해서 하나의 모수를 가지는 푸아송 분포를 사용하였다.\n\nset.seed(1220)\nn &lt;- 50 # number of observations\nlamb &lt;- 23 # true parameter value\ny &lt;- rpois(n, lambda=lamb) # observations\nnll_theta &lt;- function(theta){\n  - sum(dpois(y, lambda=theta, log=T)) # negative log likelihood\n}\nres = optimize(f=nll_theta, interval=c(0,1e6))\nres$minimum #\\hat{\\theta} compare w/ lamb\n\n[1] 24.2\n\nexp(- res$minimum) # likelihood\n\n[1] 3.090828e-11\n\n\n다음 번 포스팅에는 ML로 추정된 모수의 신뢰구간을 구하는 방법을 샆펴보자."
  },
  {
    "objectID": "posts/survivor-bias/index.html",
    "href": "posts/survivor-bias/index.html",
    "title": "Survivor bias",
    "section": "",
    "text": "In his lecture titled “The Soul of statistics” Joseph Blitzstein talks about a survivor bias (or conditioning more broadly) Dr. Derek Muller also talks about various examples of Korean houses in Bukchon Hanok Village on his YouTube\nSurvivor bias is today’s topic and the following was written mostly by GPT 4.\nSurvivor Bias: What Remains Tells Only Half the Story\nImagine walking through a forest and noticing the tallest trees. You marvel at their height and strength, thinking that this is the natural order of things. But what about the saplings and smaller trees that didn’t survive? This is the essence of survivor bias.\nWhat is Survivor Bias?\nSurvivor bias, or survivorship bias, is a logical error of focusing on the people or things that made it past some selection process and overlooking those that did not, typically because of their lack of visibility. This can lead to false conclusions in numerous different ways.\nA Classic Example: WWII Airplanes\nDuring World War II, military officials examined planes returning from combat missions to determine where they were most frequently hit by enemy fire. The plan was simple: reinforce these areas to improve the aircraft’s survival rate. The bullet holes were predominantly in the wings, body, and tail. So, it might seem logical to reinforce these parts.\nHowever, a statistician named Abraham Wald pointed out a flaw in this reasoning. The planes they were inspecting had survived. The real question was: where were the bullet holes on the planes that didn’t return? Wald hypothesized that the missing airplanes had been hit in the engine, a critical area absent of damage in the returning planes. By only looking at the survivors, the military had almost made a grave error.\nWhy Does It Matter?\nSurvivor bias can skew our understanding and lead to incorrect conclusions in various fields:\n\nBusiness: When studying successful companies, we might conclude that their practices are best. But what about companies that followed the same practices and failed?\nMedicine: If we only focus on patients who return for follow-up after treatment, we might miss side effects or outcomes in those who didn’t return.\nCulture: Celebrating only the top artists or authors might make us think that a particular style or theme is the key to success, overlooking other potential talents.\n\nOvercoming Survivor Bias\nAwareness is the first step. Whenever you’re examining successes, ask yourself: “What am I not seeing?” Seek out the failures, the unseen, the unreturned. By considering the whole picture, not just the apparent survivors, you get a clearer, more accurate view of reality.\nIn conclusion, while it’s natural to focus on winners and success stories, it’s crucial to remember the unseen and unspoken failures. They often hold the most valuable lessons."
  },
  {
    "objectID": "posts/climate-disaster/index.html",
    "href": "posts/climate-disaster/index.html",
    "title": "Important figures from the book, How to avoid a climate diaster? by Bill Gates",
    "section": "",
    "text": "How to Avoid a Climate Disaster: The Solutions We Have and the Breakthroughs We Need by Bill Gates is a comprehensive and accessible guide on how to tackle the urgent issue of climate change. Gates begins by laying out the scope of the problem, explaining that the world needs to reduce its greenhouse gas emissions to zero to avoid a catastrophe. I’ve compiled important numbers from the book to understand the climate change issues.\n\n51 Billion Tonnes: This is the amount of greenhouse gases, measured in CO2 equivalent (\\(\\mathrm{CO_{2}e}\\)), that humanity adds to the atmosphere every year.\nZero: According to Bill Gates, we need to bring this number down to zero to avoid a climate disaster. And he thinks it is possible through our technological advances. Watch his TED talk, Innovating to zero!. He is a great speaker!\n\n\n\n\nPercentage\nItem\n\n\n\n\n27%\nHow we generate electricity\n\n\n31%\nHow we make things\n\n\n18%\nHow we grow our food\n\n\n16%\nHow we move around\n\n\n6%\nHow we keep warm or cool"
  },
  {
    "objectID": "posts/pubmed-chatgpt_summary/index.html",
    "href": "posts/pubmed-chatgpt_summary/index.html",
    "title": "PubMed search, ChatGPT summary, and sending an email in R",
    "section": "",
    "text": "최신 연구 동향을 잘 알기 위해서 ChatGPT의 요약기능을 사용해보자. PubMed에서 검색을 하고 ChatGPT를 이용하여 초록을 한 두 문장으로 요약하여 그 결과를 이메일로 보내주는 것이다. 이 모든 것을 R에서 쉽게 할 수 있다.\nchatgpt_api_token &lt;- readRDS(\"G:/My Drive/Personal/chatGPT_api_key.rds\")\nlibrary(rentrez)\n\nquery &lt;- \"typhoid\" # the search query\nsearch_results &lt;- entrez_search(db=\"pubmed\", term=query, datetype=\"pdat\", reldate=10) # any other useful parameters?\n# Get the IDs of the articles\nids &lt;- search_results$ids\n# Retrieve the details of the data in xml format\narticle_details &lt;- entrez_fetch(db=\"pubmed\", id=ids, rettype=\"xml\")\nlibrary(xml2)\n# Parse the XML data\ndoc &lt;- read_xml(article_details)\n# Extract the titles and abstracts\ntitles &lt;- xml_text(xml_find_all(doc, \"//ArticleTitle\"))\n# abstracts &lt;- xml_text(xml_find_all(doc, \"//AbstractText\"))\nabstracts &lt;- xml_text(xml_find_all(doc, \"//Abstract\"))\ndois &lt;- xml_text(xml_find_all(doc, \".//PubmedData/ArticleIdList/ArticleId[@IdType='doi']\")) # to get the doi's\nget_completion &lt;- function(prompt, model=\"gpt-3.5-turbo\", temperature=0){\n  response &lt;- httr::POST(\n    url = \"https://api.openai.com/v1/chat/completions\", \n    add_headers(Authorization = paste(\"Bearer\", chatgpt_api_token)),\n    content_type_json(),\n    encode = \"json\",\n    body = list(\n      model = model,\n      temperature = temperature, # this is the degree of randomness of the model's output\n      messages = list(list(\n        role = \"user\", \n        content = prompt\n     ))\n   )\n  )\n  return(content(response)$choices[[1]]$message$content)\n}\n\n\n\nabstract_summary &lt;- rep(NA,length(abstracts))\n# You may want to try various prompts to suit your needs\nfor (i in 1:length(abstracts)) {\n  prompt = paste0(\"Your task is to generate a short summary of a scientific article based on its title and abstract. Summarize the text delimited by triple backticks into one sentence. ``` Title: \", titles[i], \". Abstract: \", abstracts[i], \"```\")\n  abstract_summary[i] &lt;- get_completion(prompt=prompt)\n}\nlibrary(blastula)\n\ncreate_summary &lt;- function(titles, abstract_summary, ids, dois){\n  summary &lt;- sapply(1:length(abstract_summary), function(i) paste0(\"&lt;p&gt;\", \" &lt;b&gt; \", titles[i], \" &lt;/b&gt; \", abstract_summary[i], \" PMID=\", ids[i] , \" DOI=\", dois[i], \"&lt;/p&gt;\"))\n  return(summary)\n}\n\nemail &lt;- compose_email(\n  title = \"Test Email\",\n  body = md(create_summary(titles, abstract_summary, ids, dois)))\n\nemail %&gt;%\n  smtp_send( \n    from = \"kimfinale@gmail.com\",\n    to = \"jonghoon.kim@ivi.int\",\n    subject = \"Daily summary of PubMed search\",\n    # credentials = creds_key(id = \"gmail\"),\n    credentials = creds_file(\"gmail_cred\")\n  )\n\n# email %&gt;%\n#   smtp_send(\n#     from = \"jonghoon.kim@ivi.int\",\n#     to = \"jonghoon.kim@ivi.int\",\n#     subject = \"Testing the `smtp_send()` function\",\n#     credentials = creds_key(id = \"outlook\")\n#   )"
  },
  {
    "objectID": "posts/pubmed-chatgpt_summary/index.html#make-it-into-a-single-function",
    "href": "posts/pubmed-chatgpt_summary/index.html#make-it-into-a-single-function",
    "title": "PubMed search, ChatGPT summary, and sending an email in R",
    "section": "Make it into a single function",
    "text": "Make it into a single function\nMake the above functions into a single function and register it for Windows task scheduler such that it can happen every day.\n\npubmed_search_chatgpt_summary &lt;- \n  function(query=\"typhoid\", reldate=10, num_sentence=1,                                          model=\"gpt-3.5-turbo\", temperature=0){\n  library(rentrez)\n  library(httr)\n  library(xml2)\n  library(blastula)\n  # chatgpt_api_token &lt;- readRDS(\"chatGPT_api_key.rds\")\n  chatgpt_api_token &lt;- readRDS(\"G:/My Drive/Personal/chatGPT_api_key.rds\")\n\n  get_completion &lt;- function(prompt, model=\"gpt-3.5-turbo\",\n                             temperature=0, api_token=chatgpt_api_token){\n  response &lt;- POST(\n    url = \"https://api.openai.com/v1/chat/completions\", \n    add_headers(Authorization = paste(\"Bearer\", api_token)),\n    content_type_json(),\n    encode = \"json\",\n    body = list(\n      model = model,\n      temperature = temperature,\n      messages = list(list(\n        role = \"user\", \n        content = prompt))\n    )\n  )\n  return(content(response)$choices[[1]]$message$content)\n}\n  \n  res &lt;- entrez_search(db=\"pubmed\", term=query, datetype=\"pdat\", reldate=reldate)\n  \n  if (length(res$ids) &gt; 0){ # one or more hits\n    ids &lt;- res$ids\n    details &lt;- entrez_fetch(db=\"pubmed\", id=ids, rettype=\"xml\")\n    doc &lt;- read_xml(details)\n    titles &lt;- xml_text(xml_find_all(doc, \"//ArticleTitle\"))\n    abstracts &lt;- xml_text(xml_find_all(doc, \"//Abstract\"))\n    dois &lt;- xml_text(xml_find_all(doc, \"//PubmedData/ArticleIdList/ArticleId[@IdType='doi']\"))\n    \n    abstract_summary &lt;- rep(NA, length(abstracts))\n\n    for (i in 1:length(abstracts)) {\n      prompt &lt;- paste0(\"Your task is to generate a short summary of a scientific article based on its title and abstract. Summarize the text delimited by triple backticks into \", num_sentence , \" sentence. ``` Title: \", titles[i], \". Abstract: \", abstracts[i], \"```\")\n      abstract_summary[i] &lt;- get_completion(prompt=prompt, model=model, temperature=temperature)\n    }\n  summary &lt;- sapply(1:length(abstract_summary), function(i) paste0(\"&lt;p&gt;\", \" &lt;b&gt; \", titles[i], \" &lt;/b&gt; \", abstract_summary[i], \" PMID=\", ids[i] , \" DOI=\", dois[i], \"&lt;/p&gt;\"))\n\n  email &lt;- compose_email(\n    title = \"Weekly summary of PubMed search\",\n    body =  md(summary))\n  \n  smtp_send(\n    email = email,\n    to = \"jonghoon.kim@ivi.int\",\n    from = \"kimfinale@gmail.com\",\n    subject = \"Daily summary of PubMed search\",\n    # credentials = creds_key(id = \"gmail\")\n    credentials = creds_file(\"gmail_cred\")\n  )\n  }\n}\npubmed_search_chatgpt_summary()\n\n\nlibrary(taskscheduleR)\n# Schedule the script to run daily at a specific time\ntaskscheduler_create(taskname = \"PubMed ChatGPT Summary\",\n                     rscript = \"G:/My Drive/Personal/pubmed_search_chatgpt_summary.R\"),\nschedule = \"DAILY\", starttime = \"08:00\")"
  },
  {
    "objectID": "posts/how-to-write-a-paper/index.html",
    "href": "posts/how-to-write-a-paper/index.html",
    "title": "Writing a paper: Start with an outline",
    "section": "",
    "text": "어찌 보면 연구자의 주 업무는 글쓰기이다. 그 중에서도 논문 쓰기가 꽤나 중요하다. 논문으로 쓰여지지 못한 연구는 타인에게는 존재하지 않는 것이나 마찬가지다. Writing a paper by George M. Whitesides 에 논문 쓰기에 유용한 팁이 있어 여기에 기록으로 남긴다. 한 마디로 요약하면 outline (개요)을 이용하는 것이다. 개요를 연구과제의 초기에 작성하여 연구의 계획표로 활용하며 공저자 (주로 제 1저자와 책임저자) 간에 논문에 대한 의견 교환시 개요를 사용하는 것이다. 그리고, 표, 수식, 그림 등이 거의 최종 상태에 가까워지면 개요를 바탕으로 논문 쓰기를 시작한다. 이렇게 하면 불필요한 실험 및 쓰기 등을 줄일 수 있다. 이렇게 적고 보니 당연한 이야기인 것 같다.\n지금까지 연구 과정을 돌아보니 프로젝트 초기에 논문의 개요를 작성하는 과정을 대체로 하긴 했었고 최종 논문 쓰기에 효과적임을 느끼고는 있었다. 그런데 개요를 이용하여 논문의 구조를 고민하고 그림이나 표를 최종적으로 만들후에 눈문 텍스트를 작성하는 일은 하지 않았던 것 같다. 이제 돌이켜 보니 Whitesides 가 적었듯이 나중에 사용하지 않은 텍스트를 작성하는 데 시간을 많이 소비한 것이 아닌가 하는 생각이 든다. 그림이나 표가 최종이 되기 전까지는 개요 작성에 노력을 기울이고 논문 쓰기는 그 이후에 하는 것이 더 효과적일 것 같다. 대체로, 코딩을 하면서는 결국에 방법 (methods)란에 들어갈 내용이라는 생각이 들어서 대체로 방법 부분은 좀 개요를 작성하는 단계에서도 자세히 쓰고는 했었는데 써 두었던 것이 결국에는 방법을 바꾸게 되어 그대로는 사용할 수 없는 경우도 있었다. 개요, 그림, 그리고 표를 가지고 좀 더 고민하고 나중에 텍스트를 입히는 것이 효과적일 것 같다."
  },
  {
    "objectID": "posts/drawing-map/index.html",
    "href": "posts/drawing-map/index.html",
    "title": "ggplot2로 지도 그리기",
    "section": "",
    "text": "ggplot2를 이용하여 지도 그리기를 해보자. 지도는 shapefile에 담겨져 있다고 가정하자. 쉐입파일을 읽는 방법은 여러가지가 있을 수 있는데 sf 패키지의 read_sf 함수를 이용한 후 ggplot2의 geom_sf를 이용하여 그리는 것이 가장 쉬운 것 같다.\n\nlibrary(sf)\nkor &lt;- read_sf(dsn=\"C:/Users/jonghoon.kim/Documents/myblog/posts/drawing-map\", layer=\"gadm41_KOR_1\")\nset.seed(42)\n# normalized number of characters of the name of the admin unit (level 1)\nchar_len &lt;- sapply(kor$NAME_1, nchar)\nkor$prop_char &lt;- char_len / max(char_len)\n\nlibrary(ggplot2)\nplt &lt;- ggplot(kor)+\n  geom_sf(aes(fill=prop_char))+\n  scale_fill_viridis_c(name=\"Normalized\\ncharacter length\", limits=c(0,1)) +\n  # scale_fill_gradientn(name=\"Probability\", colors=pal, limits=c(0,1)) +\n  theme_minimal()+\n  theme(legend.position=\"right\")\n\nplt\n\n\n\n# use color brewer \nlibrary(RColorBrewer)\npal &lt;- brewer.pal(9,\"YlOrBr\")\nplt &lt;- plt + \n  scale_fill_gradientn(name=\"Normalized\\ncharacter length\", colors=pal, limits=c(0,1))\n  \nplt\n\n\n\n# Clear some background stuff\nplt &lt;- plt +\n  theme(panel.background = element_blank(), # bg of the panel\n        plot.background = element_blank(), # bg of the plot\n        legend.background = element_blank(), # get rid of legend bg\n        legend.box.background = element_blank(),\n        panel.spacing = unit(c(0,0,0,0), \"null\"),\n        plot.margin = unit(c(0,0,0,0), \"null\"),\n        axis.line = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        panel.border = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"bottom\",\n        plot.title = element_text(hjust=0.5, size=12))\nplt"
  },
  {
    "objectID": "posts/area-polygon/index.html",
    "href": "posts/area-polygon/index.html",
    "title": "Shapefile의 polygon 면적 구하기",
    "section": "",
    "text": "shapefile에 담겨져 있는 polygon의 면적을 구해보자 raster 패키지의 area 혹은 sf 패키지의 st_area 함수를 이용할 수 이 있다.\n\nlibrary(sf)\nkor &lt;- read_sf(dsn=\"C:/Users/jonghoon.kim/Documents/myblog/posts/area-polygon\", layer=\"gadm41_KOR_1\")\nset.seed(42)\n# 1e6 to get population density per 1 km^2\nkor$area_sqkm1 &lt;- as.numeric(st_area(kor))/1e6\n\n# raster package way\n# library(raster)\n# kor &lt;- shapefile(\"C:/Users/jonghoon.kim/Documents/myblog/posts/area-polygon/gadm41_KOR_1.shp\")\n# raster pkg works for the Spatial* object\n# kor$area_sqkm2 &lt;- raster::area(as(kor, 'Spatial'))/1e6\n\n# plot population density\nlibrary(ggplot2)\nlabels = expression(atop(\"Population density\", per~1~km^2))\nplt &lt;- ggplot(kor)+\n  geom_sf(aes(fill=area_sqkm1))+\n  scale_fill_viridis_c(name=labels) +\n  theme_minimal()+\n  theme(legend.position=\"right\")\nplt\n\n\n\n# use color brewer \nlibrary(RColorBrewer)\npal &lt;- brewer.pal(9,\"YlOrBr\")\nplt + \n  scale_fill_gradientn(name=labels, colors=pal)"
  },
  {
    "objectID": "posts/odin/index.html",
    "href": "posts/odin/index.html",
    "title": "odin package",
    "section": "",
    "text": "이번 Vaccine Impact Modeling Consoritum (VIMC) 연례회의에서 odin이라는 패키지에 대해 알게 되었다. deSolve의 업그레이드 버전이라고 보면 될까? R 코드를 C언어로 컴파일하기 때문에 최종 모형의 구동속도가 빠르다. 따라서 모형을 여러번 돌려야 하는 경우 (예를 들어 MCMC) 에 유리하다. pomp 보다도 훨씬 더 빠르다고 했는데 정확한 비교 수치는 잘 기억이 안남. 종종 C++로 모형을 만들었는데 odin 패키지를 사용하면 훨씬 쉬워질 것 같다. 좀 더 살펴보아야 할 텐데 일단 잊지 않기 위해 간단히 SIR 모형만 만들어 보았다.\n\nDeterministic SIR model\n\npath_sir_model &lt;- \"C:/Users/jonghoon.kim/Documents/myblog/posts/odin/sir.R\"\nwriteLines(readLines(path_sir_model))\n\n## Core equations for transitions between compartments:\nupdate(S) &lt;- S - beta * S * I / N\nupdate(I) &lt;- I + beta * S * I / N - gamma * I\nupdate(R) &lt;- R + gamma * I\n\n## Total population size (odin will recompute this at each timestep:\n## automatically)\nN &lt;- S + I + R\n\n## Initial states:\ninitial(S) &lt;- S_ini # will be user-defined\ninitial(I) &lt;- I_ini # will be user-defined\ninitial(R) &lt;- 0\n\n## User defined parameters - default in parentheses:\nS_ini &lt;- user(1000)\nI_ini &lt;- user(1)\nbeta &lt;- user(0.2)\ngamma &lt;- user(0.1)\n\n\nRun the model and plot the results\n\nlibrary(odin)\nsir_generator &lt;- odin::odin(path_sir_model)\n\n\nx &lt;- sir_generator$new()\n# see what the object is like\n# x\nsir_col &lt;- c(\"#8c8cd9\", \"#cc0044\", \"#999966\")\nx_res &lt;- x$run(0:200)\n\npar(mar = c(4.1, 5.1, 0.5, 0.5), las = 1)\nmatplot(x_res[, 1], x_res[, -1], xlab = \"Time\", ylab = \"Number of individuals\",\n        type = \"l\", col = sir_col, lty = 1)\nlegend(\"topright\", lwd = 1, col = sir_col, legend = c(\"S\", \"I\", \"R\"), bty = \"n\")\n\n\n\n\n\n\nStochastic SIR model\n\npath_sir_stoch_model &lt;- \"C:/Users/jonghoon.kim/Documents/myblog/posts/odin/sir_stoch.R\"\nwriteLines(readLines(path_sir_stoch_model))\n\n## Core equations for transitions between compartments:\nupdate(S) &lt;- S - n_SI\nupdate(I) &lt;- I + n_SI - n_IR\nupdate(R) &lt;- R + n_IR\n\n## Individual probabilities of transition:\np_SI &lt;- 1 - exp(-beta * I / N) # S to I\np_IR &lt;- 1 - exp(-gamma) # I to R\n\n## Draws from binomial distributions for numbers changing between\n## compartments:\nn_SI &lt;- rbinom(S, p_SI)\nn_IR &lt;- rbinom(I, p_IR)\n\n## Total population size\nN &lt;- S + I + R\n\n## Initial states:\ninitial(S) &lt;- S_ini\ninitial(I) &lt;- I_ini\ninitial(R) &lt;- 0\n\n## User defined parameters - default in parentheses:\nS_ini &lt;- user(1000)\nI_ini &lt;- user(1)\nbeta &lt;- user(0.2)\ngamma &lt;- user(0.1)\n\n\nRun the model and plot the results\n\nsir_generator &lt;- odin::odin(path_sir_stoch_model)\n\n\nset.seed(42)\nx &lt;- sir_generator$new()\nx_res &lt;- x$run(0:200)\npar(mar = c(4.1, 5.1, 0.5, 0.5), las = 1)\nmatplot(x_res[, 1], x_res[, -1], xlab = \"Time\", ylab = \"Number of individuals\",\n        type = \"l\", col = sir_col, lty = 1)\nlegend(\"topright\", lwd = 1, col = sir_col, legend = c(\"S\", \"I\", \"R\"), bty = \"n\")"
  }
]