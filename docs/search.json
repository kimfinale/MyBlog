[
  {
    "objectID": "posts/sub-exponential-growth/index.html",
    "href": "posts/sub-exponential-growth/index.html",
    "title": "Sub-exponential growth",
    "section": "",
    "text": "대부분의 SIR 모형은 감염병 확산의 메커니즘을 아래와 같은 식으로 표현한다.\n\\[\\frac{\\mathrm{d}I}{\\mathrm{d}t} = \\beta S\\frac{I}{N}.\\]\n말로 설명하자면 다음과 같다. 감수성자가 단위 시간 당 평균적으로 \\(\\beta\\) 의 유효 접촉, 다시 말해 감염자와 만나면 감염이 일어나게 되는 접촉을 하게 된다. 무작위로 접촉을 하는 경우 감염자를 만날 확률은 \\(\\frac{I}{N}\\) 와 같다. \\(N\\)은 총 인구 수를 의미한다. 이러한 감염병 확산 메커니즘을 frequency-dependent 하다고 말한다.\n이와 같은 메커니즘의 결과 중 하나는 감염병의 확산 초기에 감염자의 수가 exponential growth (EG)를 보인다는 것이다. 하지만 Chowell et al. 이 지적한 것처럼 실제 감염병 유행 자료를 살펴 보면 감염자수의 증가 속도는 SIR 모형이 예측하는 것 보다는 느린 속도 sub-exponential growth (SEG) 가 흔히 나타난다. 기존의 연구들에서는 크게 두 가지의 메커니즘을 들어 SEG 를 설명한다. 첫째는 어떤 이유로든 감염병이 확산되고 있는 인구집단에서 inhomogeneous mixing이 일어나는 경우이다. 왜 inhomogeneous mixing이 SEG를 나타내는지 그리고 정량적으로 어떤 관계가 있는지는 다음에 살펴보자. 이번 포스트에서는 사람들이 골고루 섞이지 않음으로 인해 감염병의 발생이 특정 지역 및 집단에 국한 되어 전체적으로는 확산 속도가 느려진다는 정도로 이해해도 되겠다. 이렇게 inhomogeneous mixing 이 나타나는 경우는 집단 내에서 특히 접촉이 많은 소규모 집단이 있다거나 (네트워크 개념을 사용하자면 clustering) 혹은 공간적으로 더 위험한 지역과 덜 위험한 지역이 있다고 가정할 수도 있겠다. Inhomogeneous mixing이외 에도 감염병 확산에 대응하여 사람들이 위험 행동을 줄여나가면 확산 속도가 점차 감소하여 SEG가 나타날 수 있다.\nSEG를 표현하는 간단한 방법 중 하나는 아래 식에서 처럼 \\(\\alpha\\)와 같은 지수를 사용하는 것이다 .\n\\[\\frac{\\mathrm{d}I}{\\mathrm{d}t} = \\beta S\\frac{I^\\alpha}{N}.\\]\n이번 포스팅에서는 SEG을 SEIR 모형을 이용하여 구현하고 모수 추정 후 EG 모형과 비교해보고자 한다.\n우선 아래와 같이 SEIR 모형을 구현한다.\n\nseir_ode &lt;- function(t, y, params) {\n  # state variables \n  S &lt;- y[\"S\"]; \n  E &lt;- y[\"E\"]; \n  I &lt;- y[\"I\"]; \n  R &lt;- y[\"R\"];\n  CI &lt;- y[\"CI\"]\n  \n  epsilon &lt;- params[[\"epsilon\"]] # 1/epsilon = latent period\n  gamma &lt;- params[[\"gamma\"]] # 1/gamma = duration of infectiousness\n  beta &lt;- params[[\"beta\"]] # R0 = beta/gamma\n  alpha &lt;- params[[\"alpha\"]]\n  N &lt;- S + E + I + R # total population size\n  muSE &lt;- beta * S * (I^(alpha)) / N # rate from S to E\n  muEI &lt;- epsilon * E # rate from E to I, i.e., 1/epsilon = latent period\n  muIR &lt;- gamma * I # rate from I to R\n  \n  dS &lt;- - muSE # rate of change for S\n  dE &lt;- muSE - muEI # rate of change for E\n  dI &lt;- muEI - muIR # rate of change for I\n  dR &lt;- muIR # rate of change for R\n  dCI &lt;- muSE # rate of change for R\n  \n  return(list(c(dS, dE, dI, dR, dCI))) # return as a list to use deSolve package\n}\n\n초기 조건을 정의하자. 이 값들은 스크립트에서 global 변수로 사용할 것이다.\n\n# initial conditions as global variables\nI0 &lt;- 10 # initially infected people\ny0 &lt;- c(S=10000 - I0, E=0, I=I0, R=0, CI=0) # initial values for state variables\ntend &lt;- 100 # simulation end time 50 days\ntimes &lt;- seq(0, tend, by=1) # daily output for 150 days\n\n기본 모수를 정의하는 함수와 단위 시간 당 발생자 수를 조사하는 함수를 정의하자. 이 것들은 나중에 모수 추정과정에서 사용될 것이다.\n\n# baseline parameters\npinit &lt;- function(beta=0.3, alpha=1) {\n  params &lt;- list() # parameter input for the SIR model\n  params$epsilon &lt;- 0.5\n  params$gamma &lt;- 0.2\n  params$beta &lt;- beta\n  params$alpha &lt;- alpha\n  \n  return(params)\n}\n\nlibrary(dplyr)\nlibrary(deSolve)\n# incidence over a given time interval, delta_t\nincidence &lt;- function(p, delta_t=7) {\n  parm = pinit()\n  parm$beta &lt;- p[[\"beta\"]]\n  if(length(p) &gt; 1) {\n     parm$alpha &lt;- p[[\"alpha\"]]\n  }\n  ode(y=y0, times=times, func=seir_ode, parms=parm) %&gt;%\n  as.data.frame() -&gt; out\n  di = c(0, diff(out$CI))\n\n  return(di[seq(1, length(di), by=delta_t)])\n}\n\n아래와 같이 모수 추정에 사용할 거짓 자료를 만들어 보자. 전에 사용하 듯이 관찰값은 모형 예측값을 모수로 가지는 푸아송 변수로 가정하자. 기본 값으로 한 주간에 감염자수를 자료로 사용한다. 그림에서 원들은 거짓 관찰값을 점선은 모형 예측값을 나타낸다. 검정색과 빨강색은 각각 EG와 SEG를 나타낸다.\n\n# create fake data\nset.seed(42) # for reproducibility\n\ninc1 = incidence(p=pinit()) # baseline parameters, i.e., alpha=1\ndat1 &lt;- rpois(length(inc1), lambda=inc1)\nplot(dat1, xlab=\"Week\", ylab=\"Number of cases\", main=\"Exponential growth\")\nlines(inc1, col=2)\n\n\n\ninc2 = incidence(p=pinit(beta=0.5, alpha=0.8)) \ndat2 &lt;- rpois(length(inc2), lambda=inc2)\nplot(dat2, xlab=\"Week\", ylab=\"Number of cases\", main=\"Sub-exponential growth\")\nlines(inc2, col=2)\n\n\n\n\n거짓 자료를 이용하여 모수 추정을 하여 보자. 모수 추정 함수를 정의 하기 전에 아래와 같이 간단한 함수들을 정의하자. \\(\\mathrm{expit}\\)은 0과 1사이의 값만 정의되는 모수 (i.e., \\(\\alpha\\)) 를 사용하기 위해서 정의 하였고 이를 다시 원래값으로 되 돌리는 데 사용할 \\(\\mathrm{logit}\\)을 정의하였다. \\(\\mathrm{AIC}\\) (Akaike information criterion)는 모형의 우수함 (quality)의 상대 비교를 위해 사용한다. AIC 는 사용된 모형이 실제 자료를 만들어낸 모형과 다름으로 인해 잃어버리게 되는 정보의 양을 나타내는 상대적인 값이다. \\(\\mathrm{AIC}\\) 값이 작을 수록 정보를 덜 잃어버렸다는 뜻으로 더 우수한 모형을 나타낸다고 할 수 있다. \\(\\mathrm{AIC_c}\\) 는 모수 추정 시 사용된 자료의 수가 적은 경우에 더 적합한 방법이다.\n\\[ \\mathrm{logit}(p) := \\mathrm{ln} (\\frac{p}{1-p})\\] \\[\\mathrm{expit}(x) :=  \\frac{1}{1+\\mathrm{exp}(-x)}\\] \\[\\mathrm{AIC} := 2k - 2\\mathrm{ln}(\\hat{L})\\] \\[ \\mathrm{AIC_c} := \\mathrm{AIC} + \\frac{2 k^2+2 k}{n-k-1}\\]\n\nexpit &lt;- function(x) {\n  1/(1+exp(-x))  \n}\nlogit &lt;- function(x) {\n  log(x/(1-x))  \n}\naic = function(k, L){\n  2*k - 2*log(L)\n}\naicc = function(k, L, n){\n  2*k - 2*log(L) + (2*k^2+2*k)/(n-k-1)\n}\n\n모형 에측값과 관찰값과의 유사성을 측정하기 위해 likelihood 함수를 사용하자. 함수를 최소화 하는 optim의 기본 기능을 사용할 것이기 때문에 negative log likelihood를 정의한다. SEG 모형의 경우 \\(\\beta, \\alpha\\) 두 개의 함수를 추정하자.\n\nnegloglik &lt;- function (p, y) {\n  if (length(p) == 1) x &lt;- incidence(p=pinit(beta=exp(p[1])))\n  if (length(p) == 2) x &lt;- incidence(p=pinit(beta=exp(p[1]), alpha=expit(p[2])))\n  nll &lt;-  - sum(dpois(y, lambda=x, log=T), na.rm=T)\n  return(nll)\n}\n\n\nfit1 = optim(par=log(0.3), fn=negloglik, y=dat1, method=\"Brent\", lower=log(1e-6), upper=log(0.9))\nplot(dat1, xlab=\"Week\", ylab=\"Number of cases\")\nlines(incidence(p=pinit(beta=exp(fit1$par))), col=2, lty=2)\nlegend(1, 100, legend=c(\"Data\", \"Fit\"),\n       col=c(\"black\", \"red\"), lty=c(NA,2), pch=c(1,NA), cex=0.8)\n\n\n\n\n이번에는 이번 포스팅의 주제인 SEG 모형으로 만든 거짓자료를 EG 그리고 SEG 모형 두 가지로 최적화하여 보자.\n\nfit2 = optim(par=log(0.3), fn=negloglik, y=dat2, method=\"Brent\", lower=log(1e-6), upper=log(0.9))\nfit3 = optim(par=c(log(0.3), logit(0.2)), fn=negloglik, y=dat2, method=\"Nelder-Mead\")\n\n# check log likelihood\n-fit2$value\n\n[1] -54.8522\n\n-fit3$value\n\n[1] -33.3477\n\nplot(dat2, xlab=\"Week\", ylab=\"Number of cases\")\nlines(incidence(p=pinit(beta=exp(fit2$par[1]))), col=2, lty=2)\nlines(incidence(p=pinit(beta=exp(fit3$par[1]), alpha=expit(fit3$par[2]))), col=3, lty=2)\nlegend(1, 17, legend=c(\"Data\", \"Exponental fit\", \"Sub-exponential fit\"),\n       col=c(\"black\", \"red\", \"green\"), lty=c(NA,2.2), pch=c(1,NA,NA), cex=0.8)\n\n\n\n\n플롯을 살펴보았을때 SEG 모형을 이용한 피팅이 더 잘 맞아들어가는 것 같은데 \\(\\mathrm{AIC}\\)를 이용해서 모형을 비교해보자. \\(\\mathrm{AIC}\\) 와 \\(\\mathrm{AIC_c}\\) 둘 다 SEG 모형이 자료를 더 잘 설명하는 모형임을 보여준다.\n\n# Akaike information criterion to compare models\naic(k=1, L=exp(-fit1$value)) # 83.75242\n\n[1] 94.48891\n\naic(k=2, L=exp(-fit2$value)) # 69.54993\n\n[1] 113.7044\n\naicc(k=1, L=exp(-fit1$value), n=length(dat2)) #84.06011\n\n[1] 94.7966\n\naicc(k=2, L=exp(-fit2$value), n=length(dat2)) #70.54993\n\n[1] 114.7044"
  },
  {
    "objectID": "posts/reproduction-number/index.html",
    "href": "posts/reproduction-number/index.html",
    "title": "감염재생산지수 계산하기",
    "section": "",
    "text": "코로나19에 효과적으로 대응하고자 방역 당국과 연구자들이 코로나19의 전파 양상을 분석한 결과가 뉴스에 종종 보도 되었는데 그 내용 중에 빠지지 않는 것이 감염재생산지수이다. 영어로는 reproduction number (\\(\\mathcal{R}\\)) 로 불리는 데 한 명의 감염자로부터 야기된 후속 감염자의 수를 말한다. \\(\\mathcal{R}\\)이 1을 넘으면 감염자의 규모가 시간이 지남에 따라 커질 것이고 1보다 작으면 규모가 감소할 것이다. 누가 누구를 감염시켰는지 모두 알고 있다면 감염자들의 수를 세서 \\(\\mathcal{R}\\) 구할 수 있을 것이다. 하지만 한국 코로나 19 상황처럼 확진자가 많아서 모든 환자의 감염 경로를 알지 못하고 일별 확진자 자료를 가지고 있다면 어떻게 \\(\\mathcal{R}\\)을 계산할까? 이 글에서는 이에 관해 살펴보고자 한다.\n\n\n\\(\\mathcal{R}\\)의 정의\n위에서 언급한 것처럼 \\(\\mathcal{R}\\)은 한 명의 감염자에서 야기되는 후속 감염자의 수를 의미한다. 감염을 야기한 사람을 먼저 왔다는 의미로 ‘선행 감염자’ (infector) 그리고 새로이 감염된 사람들을 후에 감염되었다는 의미로 ‘후속 감염자’ (infectee) 라 칭하겠다. 그렇다면 아래와 같은 식을 쓸 수 있을 것 같다. \\[\\mathcal{R} = \\frac{새끼의 수}{어미의 수} =\\frac{후속 감염자 수}{선행 감염자 수} = \\frac{\\textrm{number of infectee}}{\\textrm{number of infector}}\\]\n\n\n\\(\\mathcal{R}\\) 계산 방법\n일별 확진자 자료를 이용하여 (\\(\\mathcal{R}\\))을 구하는 방법을 알아보기 전에 감염 경로를 모두 아는 경우를 살펴보자. 예를 들어 아래 그림과 같이 감염병이 전파되고 있다고 생각해보자. 그림에서 점들은 사람을 나타내고 화살표는 감염이 일어난 방향을 나타낸다. 그리고 0, 1, 2는 세대를 나타내는데 0세대는 외부에서 유입된 최초 감염자를 나타낸다. 측 최초 감염자가 3명을 감염시켰고 후속 감염자들도 각각 3명을 감염시켰다.\n\n2세대 이후의 상황은 모른다 가정하고 2세대까지만 계산에 넣으면 다음과 같이 계산할 수 있을 것이다. \\(\\mathcal{R}=12/4=3\\). 감염이 계속 일어나 총 \\(n\\)명의 인구 집단이 모두 감염되었다면 \\(\\mathcal{R}\\)은 얼마일까? 선행 감염자의 수는 최초의 유입된 감염자를 포함해서 \\(n+1\\) 그리고 후속 감염자의 수는 \\(n\\)이 될 것이다. 즉 \\(\\mathcal{R} = \\frac{n}{n+1}\\). 그리고 \\(n\\)이 큰 경우라면 \\(\\mathcal{R}\\)은 1로 수렴할 것이다.\n본론으로 들어가서 감염 경로는 모른채 일별 확진자수만을 가지고 \\(\\mathcal{R}\\)을 어떻게 계산할까? 아래 그림을 살펴보자. 이 그림은 중국에서 처음 발견된 확진자 수를 나타내는 유행 곡선 (epidemic curve) 이다. 붉은막대는 발열자를 나타내는데 논의의 편의를 위해서 감염자 수라 가정해보자. 녹색 네모로 표시한 2월 17일에 감염된 사람들은 녹색 화살표로 나타낸 것처럼 2월 17 일 이전에 감염된 사람들에 의하여 감염되었을 것이다. 정확히 누구에게 혹은 몇 일에 감염된 사람으로부터 감염되었는지는 알 수 없지만 말이다. 그리고 한 가지 더 알 수 있는 것은 화살표의 두께로 표현한 것처럼 선행 감염자가 언제 감염되었는지에 따라 2월 17일에 후속 감염을 일으킬 수 있는 확률이 다를 수 있다는 사실이다. 달리 표현하면 감염 후 시간이 지남에 따라 후속 감염을 일으킬 수 있는 확률이 변하게 된다는 것을 의미한다.\n\n감염 후 시간에 따라 후속 감염을 일으킬 수 있는 확률이 변할 수 있다는 것은 코로나19에 걸리게 되면 나타나는 일련의 인체 내에서의 변화 및 사람의 생활 습성등을 고려하면 어느 정도 이해할 수 있다. 바이러스에 감염되어 후속 감염자를 만들어 내기 위해서는 바이러스가 인체 내에서 증식해야 하므로 시간이 필요하다. 소위 잠재기 (latent period)가 필요하다. 이후 바이러스가 계속 증식하고 증가하고 감염 확률이 증가할 것이다. 이후 잠복기 (incubation period)를 거쳐 증상이 나타나고 회복기에 접어들면 감염 확률이 줄어들 것이다. 이런한 일련의 인체 반응에 더해 사람의 행동도 감염 확률에 영향을 미칠 것이다. 즉 몸에 바이러스가 아무리 많아도 아파서 타인을 만나지 않는다면 전파는 일어나지 않을 것이다.\n감염 후 시간에 따라 후속 감염을 일으킬 확률은 세대기 (generation interval, generation time, or transmission interval)의 분포를 이용하면 표현이 가능하다. 세대기는 한 감염자가 후속 감염을 일으킬 때 까지 걸리는 시간이다. 코로나19의 세대기는 대체로 아래와 같은 분포를 가진다고 가정해 보자. 즉 감염됨 사람이 후속 감염을 일으키려면 감염 후 하루가 지나야 하고 6일 째가 되면 후속 감염을 일으키지 않는다고 가정해보자.\n\n이걸 역으로 생각해보면 오늘 감염된 사람이 발견된 경우 이 사람을 감염시킨 선행 감염자는 2일-5일 전에 감염되었을 것이다. 이러한 세대기의 분포를 이용하면 \\(\\mathcal{R}\\) 계산식에서 문제가 되었던 부분 즉 분모에 해당하는 선행 감염자 수를 계산해 볼 수 있다. 일별 감염자가 100명씩 열흘간 발생했다고 가정해보자. 감염자 수가 일정하게 유지되고 있으니 계산할 것도 없이 \\(\\mathcal{R}\\)은 1일 것이다. 그래도 위의 논리를 이용하여 계산 하여 보자. 오늘 감염된 사람 100명이 후속 감염자가 되고 2일-5일 전에 감염된 사람이 선행 감염자가 된다. 주의할 점은 2일-5일 사이에 감염된 사람 중 위의 확률에 따라 일부만이 선행 감염자가 된다. \\[\\mathcal{R} = \\frac{후속 감염자 수}{선행 감염자 수} = \\frac{100}{100 \\times 0.25 + 100 \\times 0.35 + 100 \\times 0.25 + 100 \\times 0.15 } = 1\\]"
  },
  {
    "objectID": "posts/pop-monte-carlo/index.html",
    "href": "posts/pop-monte-carlo/index.html",
    "title": "Population Monte Carlo 파퓰레이션 몬테카를로",
    "section": "",
    "text": "최근에 파티클필터링 (particle filtering; PF) 방법을 이용하여 \\(\\mathcal{R}_t\\) 추정하는 과정에 대한 논문을 썼다. 그런데, 항상 의문이었던 것은 PF를 조금만 변형하면 감염병 모형의 감염속도 \\(\\beta=\\mathcal{R}_0 \\gamma\\) 와 같은 time-invariant 파라미터를 추정할 수도 있지 않을까 하는 것이었다. Population Monte Carlo (PMC)가 바로 그 방법이었다.\n이번 포스트에서는 SIR 모형의 모수 \\(\\beta\\)를 PMC 방법으로 추정하여 보았다. 추정하는 PMC 알고리즘을 아래에 구현하였다. 전에 구현했던 particle filtering 와 유사하다. 즉 중요도 샘플링 (importance sampling)을 연속으로 구현하는 데 연속으로 샘플링 하기 위해 Markov Chain Monte Carlo 에서 사용하듯이 proposal 을 이용하여 다음 단계의 샘플을 만들고 중요도 샘플링을 이용하여 추정을 하는 것이다.\n\nlibrary(truncnorm) # draw or evaluate according to a truncated normal dist  \npmc &lt;- function (params = NULL,\n                 x0 = NULL, # initial values\n                 y = NULL, # observation\n                 npart = 1000, # number of particles \n                 niter = 10, # iterations\n                 tend = 100, # to control the number of daily y to be fitted\n                 dt = 0.1, # dt for the ODE integration\n                 prior_mean = 0.5,\n                 prior_sd = 2,\n                 prior_lb = 0,\n                 prior_ub = 2) {\n  \n  # makes it easy to use truncated normal distribution\n  nstate &lt;- length(x0) # number of state variables (i.e., S, I, R, CI)\n  \n  # initial betas are sampled according to the prior distribution\n  beta0 &lt;- rtruncnorm(npart, a=prior_lb, b=prior_ub, mean=prior_mean, sd=prior_sd)\n  beta &lt;- matrix(NA, ncol=npart, nrow=niter) # to store the samples for beta\n  beta[1,] &lt;- beta0 # the initial values for the first row\n  # proposal for the next iteration, which is then resampled according to the  weight\n  sd = sd(beta[1,]) # scale for the proposal is adapted according to the current sample \n  beta[2,] = rtruncnorm(npart, a=prior_lb, b=prior_ub, mean=beta[1,], sd=sd)\n  \n  lik &lt;- matrix(NA, ncol = npart, nrow = niter) # likelihood \n  proposal_prob &lt;- matrix(NA, ncol = npart, nrow = niter)\n  wt &lt;- matrix(NA, ncol = npart, nrow = niter) # weight \n  W &lt;- matrix(NA, ncol = npart, nrow = niter) # normalized weights\n  A &lt;- matrix(NA, ncol = npart, nrow = niter) # Resample according to the normalized weight\n  # initial value  \n  proposal_prob[1,] &lt;- 1\n  wt[1,] &lt;- 1 / npart  # initial weights\n  W[1,] &lt;- wt[1,]\n \n  for (i in 2:niter) {\n    # cat(\"i =\", i, \"\\n\")\n    # tend increases by 1 accounts for the initial values\n    X &lt;- array(0, dim = c(npart, tend+1, nstate),\n               dimnames = list(NULL, NULL, names(x0)))\n    for (nm in names(x0)) {# starting values for each particle\n      X[, 1, nm] &lt;- x0[[nm]]\n    }\n    # run process model (i.e., SIR model) \n    x_1_tend &lt;- \n      process_model(params = params,\n                   x = X,\n                   dt = dt,\n                   beta = beta[i,])\n    # calculate weights (likelihood)\n    lik[i,] &lt;- assign_weights(x = x_1_tend, y = y[1:tend])\n    # normalize particle weights\n    proposal_prob[i,] = dtruncnorm(beta[i,], beta[i-1,], a=prior_lb, b=prior_ub, sd=sd)\n    prior_prob = dtruncnorm(beta[i,], a=prior_lb, b=prior_ub, mean=prior_mean, sd=prior_sd)\n    wt[i,] &lt;- lik[i,] * prior_prob / proposal_prob[i,]\n    \n    W[i,] &lt;- wt[i,] / sum(wt[i,])\n    # resample particles by sampling parent particles according to normalized weights\n    A[i,] &lt;- sample(1:npart, prob=W[i,], replace=T)\n    beta[i,] &lt;- beta[i, A[i,]] # resampled beta according to the normalized weight\n    # sd for the proposal can be adapted in various other ways, but we use the sd of the current sample\n    sd = sd(beta[i,]) \n    # generate proposals for the next iteration\n    if (i &lt; niter) {\n      beta[i+1,] &lt;- rtruncnorm(npart, a=prior_lb, b=prior_ub, mean=beta[i,], sd=sd)\n    } \n  } # end iteration\n  return (list(theta=beta, lik=lik, W=W, A=A))\n}\n\n감염병 확산 과정을 나타내는 SIR 모형을 구현해보자. 파티클수에 따라 벡터형태로 SIR 모형을 구현하였다.\n\nprocess_model &lt;- function (params = NULL,\n                           x = NULL,\n                           dt = 0.1,\n                           beta = NULL) {\n  \n  S &lt;- x[, 1, \"S\"] # a vector of initial S across the particles\n  I &lt;- x[, 1, \"I\"] # a vector of initial I across the particles\n  R &lt;- x[, 1, \"R\"] # a vector of initial S across the particles\n  \n  len &lt;- length(x[1,,\"S\"]) # length of model predictions (same as the data points) + 1 accounting for the initial values\n         \n  N &lt;- S + I + R\n  gamma &lt;- params[[\"gamma\"]]\n  \n  for (j in 2:len) {\n    daily_infected &lt;- 0 # to track the daily infection\n    for (i in seq(dt, 1, dt)) { # steps per day\n      FOI &lt;- beta * I * S/N\n      S_to_I &lt;- FOI * dt\n      I_to_R &lt;- I * gamma * dt\n  \n      S &lt;- S - S_to_I\n      I &lt;- I + S_to_I - I_to_R\n      R &lt;- R + I_to_R\n      \n      daily_infected &lt;- daily_infected + S_to_I\n    }\n    \n    x[, j, \"S\"] &lt;- S\n    x[, j, \"I\"] &lt;- I\n    x[, j, \"R\"] &lt;- R\n    x[, j, \"Inc\"] &lt;- daily_infected\n  }\n  return(x[, 2:len, \"Inc\"])\n}\n\n모수 추정에 사용할 거짓 일별 감염자수를 만들어보자. 위에서 구현한 process_model에서 예측되는 일별 감염자 수를 평균으로 하는 푸아송 분포를 이용하여 만들었다.\n\nparm = list(gamma=0.3) #\nx0 = c(S=9990, I=10, R=0, Inc=0)#\ntend = 50 # the number of observations\n# tend + 1 to account for the initial values\nX &lt;- array(0, dim = c(1, tend+1, 4), \n               dimnames = list(NULL, NULL, names(x0)))\nfor (nm in names(x0)) {# starting values for each particle\n  X[, 1, nm] &lt;- x0[[nm]]\n}\ntruebeta &lt;- 0.6 # true beta\npred &lt;- process_model(params=parm, x=X, beta=truebeta)\ny &lt;- rpois(tend, lambda=round(pred))  # \n\npmc 함수에 사용된 또 다른 함수 assign_weights를 아래에 구현하였다.\n\nassign_weights &lt;- function (x, y) {\n  di &lt;- dim(x)\n  npart &lt;- di[1] # number of particles\n  nobs &lt;- di[2] # number of observations\n  loglik &lt;- rep(NA, npart)\n  for (i in 1:npart) {\n    mean_case &lt;- x[i,] # for the ith particle\n    expected_case &lt;- pmax(0, mean_case)\n    obs_case &lt;- round(y)\n    loglik[i] &lt;- sum(dpois(obs_case, lambda=expected_case, log=T), na.rm=T)\n  }\n  return (exp(loglik)) # convert to normal probability\n}\n\nPMC를 이용하여 모수 추정을 해보고 결과를 그림으로 나타내보자.\n\nset.seed(44)\n# gamma and x0 are set to the same as the model used to generate the data\nparm = list(gamma=0.3)\nx0 = c(S=9990, I=10, R=0, Inc=0)# initial condition\nniter = 50\n# out = pmc(params = parm, x0 = x0, y=y, npart=10000, niter=niter, \n#    tend = length(y), dt=0.1, prior_mean=0.5, prior_sd=0.1, prior_lb=0,\n#    prior_ub=2)\n# saveRDS(out, \"out_20230811.rds\")\nout &lt;- readRDS(\"out_20230811.rds\")\nhist(out$theta[niter,], xlab=expression(beta), main=\"\")\nabline(v=truebeta, col=2, lwd=2)"
  },
  {
    "objectID": "posts/modeling-philosophy/index.html",
    "href": "posts/modeling-philosophy/index.html",
    "title": "Modeling philosophy",
    "section": "",
    "text": "감염병 전파를 이해하는 데에는 수리 모형 (model)이 아주 중요한 역할을 한다. 그런데 현실과는 모형을 통해서 어떻게 현실에 대해 배울 수 있을까? 스탠포드 철학 백과사전에 이러한 내용을 담고 있는 부분이 있어 여기에 정리를 해본다. 아직 만족한 만한 해답을 찾지는 못했다.\n철학자들은 모형을 만들고 (building), 조작하고 (manipulation), 적용하고 (application), 평가함 (evaluation)으로써 현실에 대해 추론 (reasoning) 을 할 수 있다고 기술하고 있는 것 같다. 이런 과정을 “surrogative reasoning” 혹은 “model-based reasoning” 이라는 용어로 표현하기도 했다. 추론 과정은 모형 자체를 이해하는 과정과 그 모형에 대한 이해를 현실에 적용하는 과정 두 가지로 나누어 볼 수 있다. 모형을 만드는 과정은 모형의 여러 부분들이 어떻게 서로 맞아 들어가는 지 알게 되는 과정이다. 모형을 조작하는 과정은 모형을 모수(parameter)를 변화시켜가며 시물레이션을 통해 그 결과를 확인하는 과정이 될 것이다. 모형을 만들고 조작함으로써 알게된 지식을 어떻게 그 대상인 (target system)인 현실의 언어로 변역할 수 있을까? 모형이 현실의 일부를 나타내도록 (represent) 만들어 졌다면 즉 다시 말해 우리가 알고자 하는 현실 (예를 들면 백신 접종 시에 감염자수의 감소 정도)에 상응하는 부분이 모형에 구현되어 있다면 모형을 통해서 얻은 지식이 현실에 적용될 수 있을 것 같다. 그런데 모형에 구현된 부분이 현실을 잘 반영하는지를 어떻게 알 수 있을까?"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "101-1804\nkimfinale@gmail.com\n\n\n174 Solsaem-ro, Gangbuk-gu\nwww.jonghoonk.com\n\n\nSeoul, Korea\n+82-10-9482-2517\n\n\n\n\n\n\n2021-2023 (expected)\n\nMicroMaster, Statistics and Data Science; MITx (online learning initiative of the Massachusetts Institute of Technology)\n\n2006-2010\n\nPhD, Theoretical Epidemiology (Disease Transmission Modeling); University of Michigan, Ann Arbor\nThesis title: Dynamic partnerships and HIV transmissions by stage\n\n2007-2010\n\nMSc, Biophysics and molecular biology; Gwangju Institute of Science and Technology\n\n2007-2010\n\nBSc, Biochemistry; Chungnam National University\n\n\n\n\n\n2008-9\nRackham Fellowship ($20,000) University of Michigan, Ann Arbor 2004 Student Award ($3,000)\nBlue Cross Blue Shield Michigan Foundation 2003-2004 Study Abroad Scholarship ($60,000) Korea Science and Engineering Foundation\n\n\n\nYour Most Recent Work Experience:\nShort text containing the type of work done, results obtained, lessons learned and other remarks. Can also include lists and links:\n\nFirst item\nItem with link. Links will work both in the html and pdf versions.\n\nThat Other Job You Had\nAlso with a short description.\n\n\n\nResearch Scientist at the International Vaccine Institute (2018-present)\n\n\n\nI am a theoretical epidemiologist specializing in the modeling of infectious disease transmission. My research involves utilizing mathematical and statistical models as well as data science techniques to examine the dynamics of disease transmission and evaluate the effectiveness of intervention programs like vaccination. Over the course of my career, I have investigated a wide range of pathogens including HIV, poliovirus, cholera, typhoid fever, non-typhoidal Salmonella disease, and COVID-19. To continue expanding my expertise in the field, I actively pursue new knowledge in machine learning and data science. Additionally, I have shared my knowledge through numerous lectures and participated in advisory panels aimed at controlling the spread of COVID-19 in Korea.\n\n\n\n\nMy Cool Side Project\n\nFor items which don’t have a clear time ordering, a definition list can be used to have named items.\n\nThese items can also contain lists, but you need to mind the indentation levels in the markdown source.\nSecond item.\n\n\nOpen Source\n\nList open source contributions here, perhaps placing emphasis on the project names, for example the Linux Kernel, where you implemented multithreading over a long weekend, or node.js (with link) which was actually totally your idea…\n\nProgramming Languages\n\nfirst-lang: Here, we have an itemization, where we only want to add descriptions to the first few items, but still want to mention some others together at the end. A format that works well here is a description list where the first few items have their first word emphasized, and the last item contains the final few emphasized terms. Notice the reasonably nice page break in the pdf version, which wouldn’t happen if we generated the pdf via html.\n\n\nsecond-lang: Description of your experience with second-lang, perhaps again including a [link] ref, this time placing the url reference elsewhere in the document to reduce clutter (see source file).\n\n\nobscure-but-impressive-lang: We both know this one’s pushing it.\n\n\nBasic knowledge of C, x86 assembly, forth, Common Lisp\n\n\n\n\n\n\nHuman Languages:\n\nEnglish (native speaker)\n???\nThis is what a nested list looks like.\n\nRandom tidbit\nOther sort of impressive-sounding thing you did"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "2021-2023 (expected)\n\nMicroMaster, Statistics and Data Science; MITx (online learning initiative of the Massachusetts Institute of Technology)\n\n2006-2010\n\nPhD, Theoretical Epidemiology (Disease Transmission Modeling); University of Michigan, Ann Arbor\nThesis title: Dynamic partnerships and HIV transmissions by stage\n\n2007-2010\n\nMSc, Biophysics and molecular biology; Gwangju Institute of Science and Technology\n\n2007-2010\n\nBSc, Biochemistry; Chungnam National University"
  },
  {
    "objectID": "cv.html#honors-and-awards",
    "href": "cv.html#honors-and-awards",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "2008-9\nRackham Fellowship ($20,000) University of Michigan, Ann Arbor 2004 Student Award ($3,000)\nBlue Cross Blue Shield Michigan Foundation 2003-2004 Study Abroad Scholarship ($60,000) Korea Science and Engineering Foundation"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "Your Most Recent Work Experience:\nShort text containing the type of work done, results obtained, lessons learned and other remarks. Can also include lists and links:\n\nFirst item\nItem with link. Links will work both in the html and pdf versions.\n\nThat Other Job You Had\nAlso with a short description."
  },
  {
    "objectID": "cv.html#current-position",
    "href": "cv.html#current-position",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "Research Scientist at the International Vaccine Institute (2018-present)"
  },
  {
    "objectID": "cv.html#research-interests",
    "href": "cv.html#research-interests",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "I am a theoretical epidemiologist specializing in the modeling of infectious disease transmission. My research involves utilizing mathematical and statistical models as well as data science techniques to examine the dynamics of disease transmission and evaluate the effectiveness of intervention programs like vaccination. Over the course of my career, I have investigated a wide range of pathogens including HIV, poliovirus, cholera, typhoid fever, non-typhoidal Salmonella disease, and COVID-19. To continue expanding my expertise in the field, I actively pursue new knowledge in machine learning and data science. Additionally, I have shared my knowledge through numerous lectures and participated in advisory panels aimed at controlling the spread of COVID-19 in Korea."
  },
  {
    "objectID": "cv.html#technical-experience",
    "href": "cv.html#technical-experience",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "My Cool Side Project\n\nFor items which don’t have a clear time ordering, a definition list can be used to have named items.\n\nThese items can also contain lists, but you need to mind the indentation levels in the markdown source.\nSecond item.\n\n\nOpen Source\n\nList open source contributions here, perhaps placing emphasis on the project names, for example the Linux Kernel, where you implemented multithreading over a long weekend, or node.js (with link) which was actually totally your idea…\n\nProgramming Languages\n\nfirst-lang: Here, we have an itemization, where we only want to add descriptions to the first few items, but still want to mention some others together at the end. A format that works well here is a description list where the first few items have their first word emphasized, and the last item contains the final few emphasized terms. Notice the reasonably nice page break in the pdf version, which wouldn’t happen if we generated the pdf via html.\n\n\nsecond-lang: Description of your experience with second-lang, perhaps again including a [link] ref, this time placing the url reference elsewhere in the document to reduce clutter (see source file).\n\n\nobscure-but-impressive-lang: We both know this one’s pushing it.\n\n\nBasic knowledge of C, x86 assembly, forth, Common Lisp"
  },
  {
    "objectID": "cv.html#extra-section-call-it-whatever-you-want",
    "href": "cv.html#extra-section-call-it-whatever-you-want",
    "title": "Jong-Hoon Kim",
    "section": "",
    "text": "Human Languages:\n\nEnglish (native speaker)\n???\nThis is what a nested list looks like.\n\nRandom tidbit\nOther sort of impressive-sounding thing you did"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Jong-Hoon Kim, and I live in Seoul, South Korea. I am a theoretical epidemiologist who uses mathematical, statistical, and machine-learning models to study infectious disease epidemiology. My work involves developing theories about infectious disease transmission, making predictions, and assessing the effectiveness of intervention strategies to improve public health. Currently, in 2023, I work at the International Vaccine Institute, which is located in Seoul, South Korea. My current research primarily focuses on typhoid fever, non-typhoidal Salmonella, cholera, and COVID-19.\nPlease email me if you are interested in collaborating with me"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jong-Hoon Kim's Blog",
    "section": "",
    "text": "Modeling the waning of the vaccine-derived immunity in the ODE model\n\n\n\n\n\n\n\nvaccine-derived immunity\n\n\nwaning\n\n\ncholera\n\n\nODE\n\n\nexponential\n\n\nGamma\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2024\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nIncremental Cost-Effectiveness Ratio (ICER)\n\n\n\n\n\n\n\ncholera\n\n\nsub-Saharan Africa\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nImplementing incubation period of cholera in the ODE model\n\n\n\n\n\n\n\nincubation period\n\n\nODE\n\n\ncholera\n\n\nexponential\n\n\nErlang\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nMass-action assumption: density- vs. frequency-dependent transmission\n\n\n\n\n\n\n\nmass action\n\n\nfrequency-dependent\n\n\ndensity-dependent\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2024\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nLabelledArrays and NamedTupleTools make it easy to use the ODE model in Julia\n\n\n\n\n\n\n\njulia\n\n\nODE\n\n\nLabelledArrays\n\n\nNamedTupleTools\n\n\nSEIR\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2024\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nSIR model benchmarks: deSolve, odin, and diffeqr\n\n\n\n\n\n\n\nODE\n\n\nR\n\n\ndeSolve\n\n\nodin\n\n\ndiffeqr\n\n\nC++\n\n\nJulia\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\ndiffeqr: R interface to the Julia’s DifferentialEquations.jl\n\n\n\n\n\n\n\ndifferential equation\n\n\njulia\n\n\nDifferentialEquations.jl\n\n\ndiffeqr\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2024\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nUniversal differential equation using Julia\n\n\n\n\n\n\n\nuniversal differential equation\n\n\njulia\n\n\nsub-exponential growth\n\n\n\n\n\n\n\n\n\n\n\nJan 12, 2024\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nTemplate\n\n\n\n\n\n\n\noral cholera vaccine\n\n\nall-or-nothing\n\n\nvaccine efficacy\n\n\n\n\n\n\n\n\n\n\n\nJan 9, 2024\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nFitting a straight line in Julia: Flux machine learning\n\n\n\n\n\n\n\njulia\n\n\nFlux\n\n\nlinear model\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nCholera SEIR equation\n\n\n\n\n\n\n\ncholera\n\n\nsub-Saharan Africa\n\n\n\n\n\n\n\n\n\n\n\nDec 29, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nCritical vaccination threshold\n\n\n\n\n\n\n\nvaccine\n\n\npopulation immunity\n\n\ncritical vaccination threshold\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nGeneration interval\n\n\n\n\n\n\n\ngeneration interval\n\n\nreproduction number\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nGeneration interval, growth rate, reproduction number\n\n\n\n\n\n\n\ngeneration interval\n\n\ngrowth rate\n\n\nreproduction number\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nConvolution\n\n\n\n\n\n\n\nparticle filter\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nIdiosyncrasies and generalities\n\n\n\n\n\n\n\necology\n\n\nidiosyncransy\n\n\ngenerality\n\n\nCOVID-19\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nEuler-Lotka equation\n\n\n\n\n\n\n\nEuler-Lotka\n\n\ndemography\n\n\nsurvival\n\n\nexponential\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nSIR model in Stan: Euler method\n\n\n\n\n\n\n\nR\n\n\nStan\n\n\nEuler method\n\n\nSIR model\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nEstimating a time-to-event distribution in Stan\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nEstimating a time-to-event distribution from right-truncated data\n\n\n\n\n\n\n\nright truncation\n\n\nexponential growth\n\n\nPoisson process\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nEstimating serial interval: doubly interval-censored data\n\n\n\n\n\n\n\nR\n\n\nserial interval\n\n\ninterval censoring\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nEstimating serial interval for a growing epidemic\n\n\n\n\n\n\n\nR\n\n\nserial interval\n\n\ninterval censoring\n\n\n\n\n\n\n\n\n\n\n\nNov 17, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nEstimating serial interval: interval cenoring\n\n\n\n\n\n\n\nR\n\n\nserial interval\n\n\ninterval censoring\n\n\nMLE\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nBranching process model 2\n\n\n\n\n\n\n\nR\n\n\nbranching process\n\n\nfinal epidemic size\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nModeling philosophy\n\n\n\n\n\n\n\nmodeling\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nFinal epidemic size: uniroot vs. optimize\n\n\n\n\n\n\n\nepidemic\n\n\nsize\n\n\nR\n\n\nuniroot\n\n\noptimize\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n\n\n\n\n\nSEIR model\n\n\n\nSEIR\ndeterministic\nstochastic\nGillespie's algorithm\n\n\n\n\n\n\n\n`Nov 9, 2023`{=html}\nJong-Hoon Kim\n\n\n\n\n  \n\n\n\n\nBranching process model\n\n\n\n\n\n\n\nR\n\n\nbranching process\n\n\nfinal epidemic size\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nNegative binomial regression with censored data: POLYMOD data\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\ncontact\n\n\ncensor\n\n\n\n\n\n\n\n\n\n\n\nNov 2, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nApartment transactions in Korea via API provided by the Ministry of Land, Infrastructure, and Transport\n\n\n\n\n\n\n\nR\n\n\nAPI\n\n\napartment\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nConfidence interval using profile likelihood\n\n\n\n\n\n\n\nSEIR\n\n\nprofile likelihood\n\n\nlikelihood ratio\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nSIR model using SymPy\n\n\n\n\n\n\n\nSIR\n\n\nSymPy\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nRegression with censored data: AER::tobit and optim\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\ncensor\n\n\ntobit\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nMulitple regression: POLYMOD data\n\n\n\n\n\n\n\nR\n\n\nregression\n\n\ncontact\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nRegression using optim\n\n\n\n\n\n\n\nR\n\n\noptim\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nExtract raster based on a polygon\n\n\n\n\n\n\n\nR\n\n\nraster\n\n\nshapefile\n\n\ncrop\n\n\nmask\n\n\nsf\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nodin package\n\n\n\n\n\n\n\nODE\n\n\nR\n\n\nodin\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nPubMed search, ChatGPT summary, and sending an email in Python\n\n\n\n\n\n\n\nChatGPT\n\n\nR\n\n\nxml\n\n\nhttr\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nPubMed search, ChatGPT summary, and sending an email in R\n\n\n\n\n\n\n\nChatGPT\n\n\nR\n\n\nxml\n\n\nhttr\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nPolygon 면적 구하기: sf 와 raster 패키지\n\n\n\n\n\n\n\nR\n\n\nshapefile\n\n\nggplot2\n\n\nsf\n\n\nraster\n\n\nRColorBrewer\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nggplot2로 지도 그리기\n\n\n\n\n\n\n\nR\n\n\nmap\n\n\nggplot2\n\n\nsf\n\n\nRColorBrewer\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nWriting a paper: Start with an outline\n\n\n\n\n\n\n\nwriting\n\n\npaper\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nImportance sampling\n\n\n\n\n\n\n\nimportance sampling\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nImportant figures from the book, How to avoid a climate diaster? by Bill Gates\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nRegression toward the mean\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nSurvivor bias\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nEstimating the instantaneous reproduction number using the particle filter\n\n\n\n\n\n\n\nparticle filter\n\n\nCOVID-19\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nMaximum Likelihood and Profile Likelihood for the SEIR model\n\n\n\n\n\n\n\nparameter estimation\n\n\nR\n\n\nmaximum likelihood\n\n\nprofile likelihood\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nBasic reproduction number using SymPy\n\n\n\n\n\n\n\nBasic reproduction number\n\n\nSymPy\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nPopulation Monte Carlo 파퓰레이션 몬테카를로\n\n\n\n\n\n\n\nMonte Carlo\n\n\nR\n\n\nparameter estimation\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nSimple mathematical models with very complicated dynamics\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nSub-exponential growth\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nODE-based SIR models in Stan\n\n\n\n\n\n\n\nR\n\n\nStan\n\n\nODE\n\n\nSIR\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\n감염재생산지수 계산하기\n\n\n\n\n\n\n\nmodeling\n\n\nreproduction number\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nParticle filter using R\n\n\n\n\n\n\n\nparticle filter\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nMultinomial distribution\n\n\n\n\n\n\n\nmultinomial\n\n\nRcpp\n\n\npomp\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nSEIR model using the Nimble pacakge\n\n\n\n\n\n\n\nnimble\n\n\nMCMC\n\n\nposterior predictive check\n\n\ntrace plot\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\nOrigins of major human infectious diseases\n\n\n\n\n\n\n\ninfectious disease\n\n\nemergence\n\n\ndensity\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\n  \n\n\n\n\n\\(R_0\\) and probability of a large outbreak\n\n\n\n\n\n\n\npopulation density\n\n\ninfectious disease\n\n\n\n\n\n\n\n\n\n\n\nMar 4, 2023\n\n\nJong-Hoon Kim\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ode-in-stan/index.html",
    "href": "posts/ode-in-stan/index.html",
    "title": "ODE-based SIR models in Stan",
    "section": "",
    "text": "Stan은 통계 모형 뿐 아니라 ODE 모형을 시물레이션하고 모수를 추정하는 데에도 유용하다. 이 포스팅에서는 일별 감염자 자료가 주어졌을 경우 Stan을 이용하여 SIR 모형의 두 개의 모수 (\\(\\beta, \\gamma\\))를 추정하는 과정을 기술하겠다. 먼저 deSolve 패키지 양식을 따라 SIR 모형을 아래와 같이 구현하고 모형에서 예측되는 일별 감염자 자료 (dayinc) 를 평균으로 하는 거짓 관찰값을 만든다 (yobs).\n\nsir &lt;- function(t, state, parameters) {\n  with(as.list(c(state, parameters)),{\n    # rate of change\n    N &lt;- S + I + R\n    dS &lt;- - beta*S*I/N \n    dI &lt;- + beta*S*I/N - gamma*I\n    dR &lt;- + gamma*I\n    dCI &lt;- + beta*S*I/N \n    \n    # return the rate of change\n    list(c(dS, dI, dR, dCI))\n  }) # end with(as.list ...\n}\n\ny0 &lt;- c(S=999, I=1, R=0, CI=0)\nparms &lt;- c(beta=0.6, gamma=0.4)\ntimes &lt;- seq(0, 40, by = 1)\n\nlibrary(dplyr)\ndeSolve::ode(y=y0, times=times, func=sir, parms=parms) %&gt;% \n  as.data.frame() -&gt; out\n\ndayinc &lt;- diff(out$CI)\nset.seed(42)\nyobs &lt;- rpois(length(dayinc), lambda=dayinc)\n\ndf &lt;- data.frame(time=1:length(dayinc), \n                 model=dayinc,\n                 obs=yobs)\nlibrary(ggplot2)\n# the ggplot theme was adopted from the following website: https://mpopov.com/tutorials/ode-stan-r/\n\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(df)+ \n  geom_line(aes(time, model, color=\"Model\"), linetype=\"dashed\")+\n  geom_point(aes(time, model, color=\"Model\"))+\n  geom_line(aes(time, obs, color=\"Observation\"), linetype=\"dashed\")+\n  geom_point(aes(time, obs, color=\"Observation\"))+\n  labs(x=\"Time (day)\", y=\"Daily incidence\", title=\"Incidence from the SIR model\")+\n  scale_color_manual(\"\", values=c(\"Model\"=\"black\",\"Observation\"=\"firebrick\"))+\n  theme(legend.position=\"bottom\")\n\n\n\n\n아래와 같이 Stan 모형을 만든다. Posterior predictive check 을 하기 위해 generated quantities 블록에 ypred 변수를 넣었다.\n\nstan_code &lt;- \"functions {\n  vector sir(real t,        // time\n             vector y,      // state\n             vector theta  // parameters\n             ) {      \n    vector[4] dydt;\n        \n    real S = y[1];\n    real I = y[2];\n    real R = y[3];\n    real N = S + I + R;\n    \n    real beta = theta[1];\n    real gamma = theta[2];\n    \n    dydt[1] = - beta * S * I / N;\n    dydt[2] = beta * S * I / N - gamma * I;\n    dydt[3] = gamma * I;\n    dydt[4] = beta * S * I / N;\n    \n    return dydt;\n  }\n}\n\ndata {\n  int&lt;lower=1&gt; T;\n  real t0;\n  array[T] real ts; \n  vector[4] y0;\n  int y_obs[T];\n}\n\nparameters {\n  vector&lt;lower=0&gt;[2] theta; // [beta, gamma]\n}\n\nmodel {\n  array[T] vector[4] mu = ode_rk45(sir, y0, t0, ts, theta);\n  real dayinc[T]; // daily incidence\n  dayinc[1] = mu[1, 4] + 1e-12;\n  for (t in 2:T){\n    dayinc[t] = mu[t, 4] - mu[t-1, 4] + 1e-12; \n  }\n  theta ~ exponential(1); // both parameters are on the positive real line\n  y_obs ~ poisson(dayinc); // likelihood\n}\n\ngenerated quantities {\n  array[T] vector[4] mu = ode_rk45(sir, y0, t0, ts, theta);\n  real dayinc[T];\n  dayinc[1] = mu[1, 4] + 1e-12;\n  for (t in 2:T){\n    dayinc[t] = mu[t,4] - mu[t-1,4] + 1e-12;\n  }\n  int ypred[T]; // posterior predictive \n  for (t in 1:T) {\n    ypred[t] = poisson_rng(dayinc[t]);\n  }\n}\n\"\n\n아래와 같이 Stan 모형을 이용해서 샘플링을 한다.\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\n# this is for the stan model in a separate file\n# mod &lt;- stan_model(file=paste0(getwd(),\"/stan/sir_stan.stan\"),\n#                   verbose=TRUE)\nmod &lt;- stan_model(model_code=stan_code, verbose=TRUE)\nT &lt;- 40 # end time unit for the ODE model, also the number of data points\ndata &lt;- list(T=T, t0=0.0, ts=1:T, y0=c(999,1,0,0), y_obs=yobs)\nsmp &lt;- sampling(object=mod, data=data, seed=42, chains=4, iter=2000)\n# saveRDS(smp, \"outputs/stan_smp_20230801.rds\")\n\n모수의 posterior 분포를 살펴보자.\n\n# smp &lt;- readRDS(\"outputs/stan_smp_20230801.rds\")\nsmp &lt;- readRDS(\"stan_smp_20230801.rds\") # file is under the content/post/the_relevant_post_name/index_files/figure_html\ndf &lt;- as.data.frame(smp)\npr &lt;- c(0.5,0.025,0.975)\nd &lt;- as.data.frame(t(apply(df[,grepl(\"^theta.*\", names(df))],\n                           2, quantile, probs=pr)))\nd$name &lt;- c(\"beta\", \"gamma\")\nd$true &lt;- c(0.6, 0.4)\nggplot(d)+ \n  geom_errorbar(aes(x=name, ymin=`2.5%`, ymax=`97.5%`), width=0.0)+\n  geom_point(aes(x=name, y=`50%`, color=\"Estimates\"), size=2)+\n  geom_point(aes(x=name, y=true, col=\"True value\"), size=3)+\n  scale_color_manual(values=c(\"Estimates\"=\"black\",\"True value\"=\"firebrick\"))+\n  labs(x=\"\", y=\"\", title=\"Median estimates with 95% CrI\")+\n  theme(legend.position=\"bottom\", legend.title=element_blank())+\n  scale_x_discrete(breaks=c(\"beta\",\"gamma\"),\n                   labels=c(expression(beta),expression(gamma)))+\n  coord_flip()\n\n\n\n\n마지막으로 posterior predictive check을 통해서 모수 추정을 위해 사용했던 자료와 비교해 보자.\n\n# pr &lt;- c(0.5,0.025,0.975)\nd &lt;- as.data.frame(t(df[,grepl(\"^ypred.*\", names(df))]))\nd$time &lt;- 1:40\ndlong &lt;- tidyr::pivot_longer(d, cols=-time)\ndayincdf &lt;- data.frame(inc=dayinc, time=1:40)\nyobsdf &lt;- data.frame(obs=yobs, time=1:40)\n\nggplot(dlong)+ \n  geom_line(aes(time, value, group=name, color=\"Posterior predictive\"))+\n  geom_line(data=dayincdf, aes(time, inc, color=\"Model\"))+\n  geom_point(data=yobsdf, aes(time, obs, color=\"Observation\"))+\n  geom_line(data=yobsdf, aes(time, obs, color=\"Observation\"), linetype=\"dashed\")+\n  labs(x=\"Time (day)\", y=\"Daily incidence\", title=\"Posterior predictive check\")+\n  scale_color_manual(\"\", values=c(\"Model\"=\"black\",\"Posterior predictive\"=\"grey\",\"Observation\"=\"firebrick\"))+\n  theme(legend.position=\"bottom\")"
  },
  {
    "objectID": "posts/R0-sympy/index.html",
    "href": "posts/R0-sympy/index.html",
    "title": "Population Monte Carlo 파퓰레이션 몬테카를로",
    "section": "",
    "text": "감염병의 전파를 이해하는 데 있어 가장 기본적인 개념이 재감염지수, 특히 기초재감염지수 (\\(\\mathcal{R}_0\\)) 이다. 재감염지수는 한 명의 감염자로부터 생산되는 평균 후속 감염자의 수를 일컫는데 기초재감염지수는 코로나19의 경우 처럼 인구 집단에 면역력을 가진 사람이 없어 모든 사람이 감염될 수 있는 상태하 에서의 재감염지수를 말한다. 기초 재감염 지수는 다음과 같은 수식으로 표현할 수 있다.\n\\[ \\mathcal{R}_0 = \\beta c D \\]\n\\(\\beta\\) 는 한 명의 감염자가 타인을 접촉할 때 상대방을 감염시킬 수 있는 확률, \\(c\\) 는 단위 시간 당 접촉이 일어나는 횟수, \\(D\\) 는 감염 상태가 지속되는 시간을 나타낸다. \\(\\beta\\) 만으로 \\(\\beta c\\) 를 대신해 사용하는 경우도 흔하다. 그 경우 \\(\\beta\\) 는 단위 시간 당 후속 감염자의 수로 표현할 수 있을 것 같다. 미분방정식에 기반한 감염병 모형의 경우는 \\(\\mathcal{R}_0\\)를 어떻게 계산할까? 아래와 같이 SIR 모형을 정의해 보자.\n\\[\\begin{align}\n\\mathrm{d}S/\\mathrm{d}t &= -\\beta I S/N \\\\\n\\mathrm{d}I/\\mathrm{d}t &= \\beta I S/N - \\gamma I\\\\\n\\mathrm{d}R/\\mathrm{d}t &= \\gamma I\n\\end{align}\\]\n위의 정의에서 사용되었던 개념을 적용한다면 \\(\\mathcal{R}_0 = \\beta/\\gamma\\) 라고 할 수 있다. 이는 \\(\\mathcal{R}_0\\)가 감염병이 집단 내에서 유행을 일으킬 수 있는 역치조건임을 이용해도 동일한 결론에 이를 수 있다. (i.e., \\(\\mathrm{d}I/\\mathrm{d}t&gt;0\\))\n위와는 달리 Diekmann et al. 에 의해서 도입된 next generation 방법으로 좀 더 다양한 상황 하에서 \\(\\mathcal{R}_0\\)를 구할 수 있다. 이 방법에서는 \\(\\mathcal{R}_0\\)가 next generation operator의 spectral radius 가 된다. 위 논문 보다는 van den Driessche et al. 가 좀 더 이해하기 쉬운 것 같아 이 방법을 기준으로 살펴보겠다. 그리고 그 계산을 python의 SymPy 라이브러리를 이용해서 구현을 해보겠다. 먼저 간단한 우선 ( SEIR ) 모형의 경우부터 살펴보자.\nNext generation operator \\(G\\) 은 전파를 통해서 생산되는 새로운 감염이 발생하는 속도를 나타내는 행렬 \\(F\\)와 감염이 다른 상태로 변화되는 속도 (V)로 구성되며 다음과 같은 관계를 갖는다. \\(G=FV^{-1}\\). 그리고 \\(R_0\\)는 \\(G\\)의 spectral radius가 된다. 아래 파이썬 구현에서는 \\(\\beta, \\gamma\\) 를 b, g로 나타내었다.\n\nfrom sympy import *\nb, k, g, = symbols('p k g')\nF = Matrix([[0, b],[0, 0]])\nV = Matrix([[k, 0], [-k, g]])\nM = F*(V**-1)\neigval = M.eigenvals(simplify=true)\ninit_printing(use_unicode=True)\neigval\n\n⎧      p   ⎫\n⎨0: 1, ─: 1⎬\n⎩      g   ⎭\n\n#⎧      b   ⎫\n#⎨0: 1, ─: 1⎬\n#⎩      g   ⎭\nlst = list(eigval.keys())\nlst[1]\n\np\n─\ng\n\n#b\n#─\n#g\n\n위에서 언급한 SEIR 모형의 경우는 너무 간단하니 그 보다 조금 더 복잡한 그 예로 다음 연구를 살펴보자. Pitzer et al.의 연구인데 사용된 감염병 모형은 사람 간 직접 전파와 물을 통한 간접 전파 두 가지의 전파 메케니즘을 구현 하였고 (\\(\\lambda_p\\) 와 \\(\\lambda_w\\)) 최초 감염 \\(S_1 \\rightarrow I_1\\) 과 중복 감염\\(R \\rightarrow S_2 \\rightarrow I_2\\) 을 다르게 취급하였다. 부록 (supplementary material)을 보면 사용된 미분식을 볼 수 있다.\n\\[\\begin{align}\n\\mathrm{d}S_1/\\mathrm{d}t &= B + \\epsilon S_2 - (\\lambda_p+\\lambda_w-\\mu)S_1\\\\\n\\mathrm{d}I_1/\\mathrm{d}t &= (\\lambda_p+\\lambda_w)S_1 - (\\delta+\\mu) I_1 \\\\\n\\mathrm{d}R/\\mathrm{d}t &= \\delta(1-\\theta-\\alpha)(I_1+I_2) - (\\omega +\\mu)R \\\\\n\\mathrm{d}C/\\mathrm{d}t &= \\delta\\theta(I_1+I_2) - \\mu C \\\\\n\\mathrm{d}S_2/\\mathrm{d}t &= \\omega R -\\epsilon S_2 - (\\lambda_p+\\lambda_w-\\mu) S_2\\\\\n\\mathrm{d}I_2/\\mathrm{d}t &= (\\lambda_p+\\lambda_w) S_2 - (\\delta+\\mu) I_2 \\\\\n\\mathrm{d}W/\\mathrm{d}t &= \\gamma(I_1+rI_2+rC) - \\xi W\n\\end{align}\\]\n또한 아래와 같이 기초재감염지수도 계산 결과를 보여준다. SymPy를 통해 동일한 결과를 얻을 수 있는지 확인해보자.\n\\[\\begin{align}\nR_0 = \\frac{1}{\\mu+\\delta} \\left(\\beta_p +\\frac{\\gamma \\beta_w}{\\xi}\\right) \\left(1 +\\frac{\\delta\\theta r}{\\mu}\\right)\n\\end{align}\\]\n\np, r, w, N, d, m, t, m, g, x = symbols('p r w N d m t m g x')\nF = Matrix([[p, r*p, r*p, w*N], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\nV = Matrix([[d+m, 0, 0, 0], [0, d+m, 0, 0], [-d*t, -d*t, m, 0], [-g, -r*g, -r*g, x]])\nM = F*(V**-1)\neigval = M.eigenvals(simplify=true)\ninit_printing(use_unicode=True)\neigval \n\n⎧      (N⋅g⋅w + p⋅x)⋅(d⋅r⋅t + m)   ⎫\n⎨0: 3, ─────────────────────────: 1⎬\n⎩             m⋅x⋅(d + m)          ⎭\n\nlst = list(eigval.keys())\nR0_eig = lst[1]\nR0 = (1/(d+m))*(p+N*g*w/x)*(1+(d*t*r)/m) # R0 from the Pitzer (2014)\nsimplify(R0-R0_eig) # 0 for the same expression (symbolic assessment)\n\n0\n\n#0\nR0.equals(R0_eig) # True for the same expression (numerical assessment)\n\nTrue\n\n#True\n\n부록에 보면 기초 재감염지수에 이르는 상세한 과정이 나와있는데 \\(V_{3,3}\\) 에 오류가 있음을 알아냈다. \\(\\delta +\\mu\\) 가 \\(\\mu\\) 로 바뀌어야 한다. 이유는 위 미분식에서 \\(C\\) 식을 보면 알 수 있는데 이는 만성 감염자를 나타내고 따라서 회복을 의미하는 \\(\\delta\\) 가 없어야 한다. 이는 기록하는 과정에서의 오류인 듯 하고 결과로 얻어진 \\(R_0\\) 는 우리가 계산한 결과와 동일하다. 다만, 계산 결과를 식 (18) 에서와 같이 의미있는 구획으로 나누어서 표현하려면 SymPy 결과를 직접 수정하여야 한다."
  },
  {
    "objectID": "posts/simple-model-complicated-dynamics/index.html",
    "href": "posts/simple-model-complicated-dynamics/index.html",
    "title": "Simple mathematical models with very complicated dynamics",
    "section": "",
    "text": "Simple mathematical models with very complicated dynamics\nRobert M. May Nature Vol. 261 June 10, 1976\nThis article discusses a simple first order difference equations that can display very complicated dynamics.\n\\[X_{t+1} = F(X_t)\\]\nIn biological population, the nonlinear function \\(F(x)\\) often has the following properties. \\(F(0)=0\\); \\(F(x)\\) increases monotonically as \\(X\\) increases through the range of \\(0&lt;X&lt;A\\) (with \\(F(x)\\) attaining its maximum value at \\(X=A\\)); \\(F(X)\\) decreases monotonically as \\(X\\) increases beyond \\(X=A\\) \\(N_{t+1} = N_t(a-bN_t)\\)\n\\(X_{t+1} = a X_t (1-X_t)\\)\nX must remain on the interval \\(0&lt;X&lt;1\\); if \\(X\\) ever exceeds unity, subsequent iterations diverge towards \\(-\\infty\\). Furthermore, \\(F(X)\\) attains a maximum value of \\(a/4\\) at \\(X=1/2\\); the equation therefore possesses non-trivial dynamical behaviour only if \\(a&lt;4\\). On the other hand, all trajectories are attracted to \\(X=0\\) if \\(a&lt;1\\).\n\n# function to compute the value at the next time step\n# 0 &lt; x &lt; 1\n# a &lt; 1 for x to go to zero\n# a &gt; 4 leads to x &gt; 1 at one point, which then leads to - infinity\n# 1 &lt; a &lt; 4 for x to exhibit non-trivial dynamics\nx_next &lt;- function(a, x){\n  a*x*(1-x)\n}\n\nx0 = seq(0.01, 0.99, 0.01)\na = c(2.707, 3.414) # values were adopted from the paper by May Nature Vol. 261 June 10, 1976\nxnext = sapply(x0, function(x) x_next(a, x))\n\nplot(x0, xnext[1,], type='l', ylim=c(0,1), xlim=c(0,1),\n     xlab=expression(X[t]), ylab=expression(X[t+1]))\nlines(x0, xnext[2,])\nlines(0:1, 0:1) # line y = x\n\nxstar = 1 - 1/a # points where X(t+1) = X(t)\npoints(xstar[1], xstar[1])\npoints(xstar[2], xstar[2], col=2)\n# slope at the point x given a\ndx &lt;- function(a,x){\n  -2*a*x+a\n}\n\n# function to compute intercept at the given slope b and point x\nintcpt = function(b,x){\n  x - b*x\n}\n\nabline(a=intcpt(b=dx(a=a,x=xstar[1]),x=xstar[1]), b=dx(a=a,x=xstar[1]), lty=2)\nabline(a=intcpt(b=dx(a=a,x=xstar[2]),x=xstar[2]), b=dx(a=a,x=xstar[2]), lty=2, col=2)\n\n\n\n\n\nx_iter &lt;- function(a, x, iter, func){\n  xvec = rep(NA, iter)\n  xvec[1] = x\n  for(i in 2:iter){\n    xvec[i] = func(a, xvec[i-1])  \n  }\n  return(xvec)\n}\nplot(x_iter(2.9, 0.8, 100, x_next), type=\"l\")\n\n\n\n\n\\(X_{t+1} = X_t \\textrm{exp}[r(1-X_t)]\\)\n\nx_next_exp &lt;- function(r, x){\n  x*exp(r*(1-x))\n}\n\nplot(x_iter(2, 0.8, 100, x_next_exp), type=\"l\")\n\n\n\n\n\nx_next2 &lt;- function(r, x){\n x1 &lt;- a*x*(1-x)\n x2 &lt;- a*x1*(1-x1)\n return(x2)\n}\n\nxnext = sapply(x0, function(x) x_next2(a, x))\n\nplot(x0, xnext[1,], type='l', ylim=c(0,1), xlim=c(0,1),\n     xlab=expression(X[t]), ylab=expression(X[t+2]))\nlines(x0, xnext[2,])\nlines(0:1, 0:1) # line y = x"
  },
  {
    "objectID": "posts/regression-toward-mean/index.html",
    "href": "posts/regression-toward-mean/index.html",
    "title": "Regression toward the mean",
    "section": "",
    "text": "In his lecture Joseph Blitzstein talks about two basic statistical phenomena: Regression toward to the mean (RTTM) and survivor bias. The RTTM is today’s topic. Topic was mainly written by GPT 4.\nRegression toward the mean is a statistical phenomenon where extreme data points are likely to be followed by less extreme ones when measured again. In simpler terms, it means that if an extreme observation is observed, the next observation is likely to be closer to the mean or average.\n\nWhy does it happen?\nIt’s primarily a matter of probability. Extreme values are, by definition, rare. So, when you take a second measurement, it’s simply more probable that the new value will be closer to the mean than the previous extreme value was.\n\n\nExamples:\nSports Performance: Imagine a basketball player who has an outstanding game, scoring well above their average number of points. If they’ve played at such an exceptional level, it’s likely that in the next game they will score closer to their average (not necessarily because their skill has decreased, but simply due to the natural variability in performance).\nStudent Test Scores: If a student who typically scores around the average on exams gets an exceptionally high score on one test, they might score closer to their average on a subsequent test. Conversely, if they score exceptionally low on one test, they might score higher the next time.\nInvestment Returns: If a particular stock has an exceptionally good year with returns way above the market average, it might have more modest (or even below-average) returns the following year.\nHeight of Parents and their Children: This is a classic example by Sir Francis Galton. He found that very tall parents tend to have children who are shorter than them (but still above average), and very short parents tend to have children who are taller than them (but still below average). The children’s heights tend to “regress” towards the mean height.\nMedical Treatments: If patients are selected for a clinical trial because they have exceptionally high blood pressure, some of them will probably show a reduction in blood pressure over time even without any treatment. This isn’t because of any therapeutic effect, but simply because their initial measurements were unusually high and subsequent measurements tend to be closer to the mean.\n\n\nMisunderstandings:\nThis concept is often misunderstood. For instance, if a student performs poorly on a test and then improves on the next one, it might be tempting to attribute this improvement to a particular intervention (like tutoring). While the intervention might have had an effect, it’s also possible that some of the improvement was due to regression toward the mean.\nIn research and experimental designs, this phenomenon needs to be taken into account, especially when making causal inferences from observed changes.\nToward the end of his talk, he mentions about the quote by Daniel Kahneman that very clearly explain the concept of RTTM.\nI had the most satisfying Eureka experience of my career while attempting to teach flight instructors that praise is more effective than punishment for promoting skill-learning. …. [A flight instructor objected:] “On many occasions I have praised flight cadets for clean execution of some aerobatic maneuver, and in general when they try it again, they do worse. On the other hand, I have often screamed at cadets for bad execution, and in general they do better the next time. So please don’t tell us that reinforcement works and punishment does not, because the opposite is the case.” …\nThis was a joyous moment, in which I understood an important truth about the world: because we tend to reward others when they do well and punish them when they do badly, and because there is regression to the mean, it is part of the human condition that we are statistically punished for rewarding others and rewarded for punishing them.\nFull details of Kahneman’s quote can be found from the following article"
  },
  {
    "objectID": "posts/max-likelihood/index.html",
    "href": "posts/max-likelihood/index.html",
    "title": "Maximum Likelihood and Profile Likelihood for the SEIR model",
    "section": "",
    "text": "통계학은 많은 부분 확률모형의 모수를 추정하는 (inferential statistics) 과정이고 모수 추정방법으로 가장 많이 사용되는 방법이 maximum likelihood (ML)이다. 이번 포스트는 2014년 출간된 Cole et al.의 Maximum Likelihood, Profile Likelihood, and Penalized Likelihood: A Primer을 차용하여 maximum likelihood (ML) 와 profile likelihood에 대하여 기술하여 보고자 한다.\n\n최대 가능도 (ML)\n잠복기를 추정하기 위해 증상 발현일과 기존 감염자와의 접촉일을 묻는 설문조사를 했다고 하자. \\(n\\) 명을 인터뷰하고 \\(n\\) 개의 관찰값 \\(y_1, y_2, ..., y_n\\) 을 얻었다고 하자. 잠복기의 분포에 대한 확률모형 \\(f (y|\\boldsymbol{\\theta})\\) 은 주어진 모수\\(\\boldsymbol{\\theta}=(\\theta_1, \\theta_2, ..., \\theta_j)\\) 하에서 \\(Y=y\\)가 될 확률을 나타낸다.\n최대 가능도 방법은 미지의 모수 하에서 관찰값의 확률의 나타낸다. 확률 모형 \\(f(y|\\boldsymbol{\\theta})\\)이 주어진 모수 \\(\\boldsymbol{\\theta}\\) 하에서 \\(Y\\)의 확률을 나타내는 반면 최대 가능도 방법은 \\(Y\\)를 관찰값에 고정한 채 \\(\\boldsymbol{\\theta}\\)의 함수로 표현하게 된다. 따라서 확률모형과는 다르게 다음과 같은 식 \\(\\mathcal{L}(\\boldsymbol{\\theta};y_i)\\) 을 사용한다. 즉, 우리가 관심있어 하는 것은 확률모형 \\(f (y|\\boldsymbol{\\theta})\\)이 \\(Y\\) 가 아니고 \\(\\boldsymbol{\\theta}\\) 에 따라 어떻게 변하는가 하는 것이다. \\(\\mathcal{L}(\\boldsymbol{\\theta};y_i)\\) 를 \\(i\\) 번째 관찰값이 가능도에 영향을 미치는 정도라 하고 관찰값이 상호독립적이라고 가정하면 관찰값 전체의 가능도는 아래와 같이 표현할 수 있다.\n\\[\\mathcal{L}(\\boldsymbol{\\theta};\\boldsymbol{y}) = \\prod_{i=1}^{n} \\mathcal{L}(\\boldsymbol{\\theta};y_i) = \\prod_{i=1}^{n} f(y_i;\\boldsymbol{\\theta})\\]\n위 식에서 \\(\\boldsymbol{y}=(y_1, y_2, ..., y_n)\\)을 나타낸다.\n\\(\\mathcal{L}(\\boldsymbol{\\theta};\\boldsymbol{y})\\) 는 \\(\\boldsymbol{\\theta}\\)에 대한 확률을 알 수는 없기 때문에 확률모형이 아닌 가능도 (혹은 우도) 함수라고 한다. ML은 가능도 함수를 최대로 만들어 주는 \\(\\boldsymbol{\\theta}\\)로 모수에 대한 추정치를 정의한다.\n\\[\\hat{\\theta} = \\textrm{argmax}_{\\theta}\\{{\\mathrm{log} \\mathcal{L}(\\theta)}\\}\\]\n위 식에서 \\(\\mathrm{log}\\)를 사용한 이유는 가능도 값이 매우 작은 수가되는 경우가 많고 따라서 컴퓨터를 이용한 계산상의 안정성을 위해서 (i.e., arithmetic underflow 가 일어나지 않게 하기 위해) 실제로는 \\(\\mathrm{log} \\mathcal{L}(\\theta)\\)를 사용하기 때문이다. 추가적으로 많은 최적화 알고리듬의 경우 최소화가 기본값으로 설정되어 있어 최대 가능도법을 구현할 때는 \\(-\\mathrm{log} \\mathcal{L}(\\theta)\\)를 사용하는 경우가 많다.\n최대 가능도법을 이용하여 푸아송 분포의 모수를 추정하는 과정을 살펴보자.\n\n\n푸아송 분포 모수 추정\n위에서 언급했던 잠복기의 예를 살펴보자. 잠복기는 Weibull, Gamma, 혹은 Lognormal 등 두 개의 모수를 가지는 확률모형이 많이 사용되는 데 아래 예에서는 계산상의 편의를 위해서 하나의 모수를 가지는 푸아송 분포를 사용하였다.\n\nset.seed(1220)\nn &lt;- 50 # number of observations\nlamb &lt;- 23 # true parameter value\ny &lt;- rpois(n, lambda=lamb) # observations\nnll_theta &lt;- function(theta){\n  - sum(dpois(y, lambda=theta, log=T)) # negative log likelihood\n}\nres = optimize(f=nll_theta, interval=c(0,1e6))\nres$minimum #\\hat{\\theta} compare w/ lamb\n\n[1] 24.2\n\nexp(- res$minimum) # likelihood\n\n[1] 3.090828e-11\n\n\n다음 번 포스팅에는 ML로 추정된 모수의 신뢰구간을 구하는 방법을 샆펴보자."
  },
  {
    "objectID": "posts/survivor-bias/index.html",
    "href": "posts/survivor-bias/index.html",
    "title": "Survivor bias",
    "section": "",
    "text": "In his lecture titled “The Soul of statistics” Joseph Blitzstein talks about a survivor bias (or conditioning more broadly) Dr. Derek Muller also talks about various examples of Korean houses in Bukchon Hanok Village on his YouTube\nSurvivor bias is today’s topic and the following was written mostly by GPT 4.\nSurvivor Bias: What Remains Tells Only Half the Story\nImagine walking through a forest and noticing the tallest trees. You marvel at their height and strength, thinking that this is the natural order of things. But what about the saplings and smaller trees that didn’t survive? This is the essence of survivor bias.\nWhat is Survivor Bias?\nSurvivor bias, or survivorship bias, is a logical error of focusing on the people or things that made it past some selection process and overlooking those that did not, typically because of their lack of visibility. This can lead to false conclusions in numerous different ways.\nA Classic Example: WWII Airplanes\nDuring World War II, military officials examined planes returning from combat missions to determine where they were most frequently hit by enemy fire. The plan was simple: reinforce these areas to improve the aircraft’s survival rate. The bullet holes were predominantly in the wings, body, and tail. So, it might seem logical to reinforce these parts.\nHowever, a statistician named Abraham Wald pointed out a flaw in this reasoning. The planes they were inspecting had survived. The real question was: where were the bullet holes on the planes that didn’t return? Wald hypothesized that the missing airplanes had been hit in the engine, a critical area absent of damage in the returning planes. By only looking at the survivors, the military had almost made a grave error.\nWhy Does It Matter?\nSurvivor bias can skew our understanding and lead to incorrect conclusions in various fields:\n\nBusiness: When studying successful companies, we might conclude that their practices are best. But what about companies that followed the same practices and failed?\nMedicine: If we only focus on patients who return for follow-up after treatment, we might miss side effects or outcomes in those who didn’t return.\nCulture: Celebrating only the top artists or authors might make us think that a particular style or theme is the key to success, overlooking other potential talents.\n\nOvercoming Survivor Bias\nAwareness is the first step. Whenever you’re examining successes, ask yourself: “What am I not seeing?” Seek out the failures, the unseen, the unreturned. By considering the whole picture, not just the apparent survivors, you get a clearer, more accurate view of reality.\nIn conclusion, while it’s natural to focus on winners and success stories, it’s crucial to remember the unseen and unspoken failures. They often hold the most valuable lessons."
  },
  {
    "objectID": "posts/climate-disaster/index.html",
    "href": "posts/climate-disaster/index.html",
    "title": "Important figures from the book, How to avoid a climate diaster? by Bill Gates",
    "section": "",
    "text": "How to Avoid a Climate Disaster: The Solutions We Have and the Breakthroughs We Need by Bill Gates is a comprehensive and accessible guide on how to tackle the urgent issue of climate change. Gates begins by laying out the scope of the problem, explaining that the world needs to reduce its greenhouse gas emissions to zero to avoid a catastrophe. I’ve compiled important numbers from the book to understand the climate change issues.\n\n51 Billion Tonnes: This is the amount of greenhouse gases, measured in CO2 equivalent (\\(\\mathrm{CO_{2}e}\\)), that humanity adds to the atmosphere every year.\nZero: According to Bill Gates, we need to bring this number down to zero to avoid a climate disaster. And he thinks it is possible through our technological advances. Watch his TED talk, Innovating to zero!. He is a great speaker!\n\n\n\n\nPercentage\nItem\n\n\n\n\n27%\nHow we generate electricity\n\n\n31%\nHow we make things\n\n\n18%\nHow we grow our food\n\n\n16%\nHow we move around\n\n\n6%\nHow we keep warm or cool"
  },
  {
    "objectID": "posts/pubmed-chatgpt_summary/index.html",
    "href": "posts/pubmed-chatgpt_summary/index.html",
    "title": "PubMed search, ChatGPT summary, and sending an email in R",
    "section": "",
    "text": "최신 연구 동향을 잘 알기 위해서 ChatGPT의 요약기능을 사용해보자. PubMed에서 검색을 하고 ChatGPT를 이용하여 초록을 한 두 문장으로 요약하여 그 결과를 이메일로 보내주는 것이다. 이 모든 것을 R에서 쉽게 할 수 있다.\nchatgpt_api_token &lt;- readRDS(\"G:/My Drive/Personal/chatGPT_api_key.rds\")\nlibrary(rentrez)\n\nquery &lt;- \"typhoid\" # the search query\nsearch_results &lt;- entrez_search(db=\"pubmed\", term=query, datetype=\"pdat\", reldate=10) # any other useful parameters?\n# Get the IDs of the articles\nids &lt;- search_results$ids\n# Retrieve the details of the data in xml format\narticle_details &lt;- entrez_fetch(db=\"pubmed\", id=ids, rettype=\"xml\")\nlibrary(xml2)\n# Parse the XML data\ndoc &lt;- read_xml(article_details)\n# Extract the titles and abstracts\ntitles &lt;- xml_text(xml_find_all(doc, \"//ArticleTitle\"))\n# abstracts &lt;- xml_text(xml_find_all(doc, \"//AbstractText\"))\nabstracts &lt;- xml_text(xml_find_all(doc, \"//Abstract\"))\ndois &lt;- xml_text(xml_find_all(doc, \".//PubmedData/ArticleIdList/ArticleId[@IdType='doi']\")) # to get the doi's\nget_completion &lt;- function(prompt, model=\"gpt-3.5-turbo\", temperature=0){\n  response &lt;- httr::POST(\n    url = \"https://api.openai.com/v1/chat/completions\", \n    add_headers(Authorization = paste(\"Bearer\", chatgpt_api_token)),\n    content_type_json(),\n    encode = \"json\",\n    body = list(\n      model = model,\n      temperature = temperature, # this is the degree of randomness of the model's output\n      messages = list(list(\n        role = \"user\", \n        content = prompt\n     ))\n   )\n  )\n  return(content(response)$choices[[1]]$message$content)\n}\n\n\n\nabstract_summary &lt;- rep(NA,length(abstracts))\n# You may want to try various prompts to suit your needs\nfor (i in 1:length(abstracts)) {\n  prompt = paste0(\"Your task is to generate a short summary of a scientific article based on its title and abstract. Summarize the text delimited by triple backticks into one sentence. ``` Title: \", titles[i], \". Abstract: \", abstracts[i], \"```\")\n  abstract_summary[i] &lt;- get_completion(prompt=prompt)\n}\nlibrary(blastula)\n\ncreate_summary &lt;- function(titles, abstract_summary, ids, dois){\n  summary &lt;- sapply(1:length(abstract_summary), function(i) paste0(\"&lt;p&gt;\", \" &lt;b&gt; \", titles[i], \" &lt;/b&gt; \", abstract_summary[i], \" PMID=\", ids[i] , \" DOI=\", dois[i], \"&lt;/p&gt;\"))\n  return(summary)\n}\n\nemail &lt;- compose_email(\n  title = \"Test Email\",\n  body = md(create_summary(titles, abstract_summary, ids, dois)))\n\nemail %&gt;%\n  smtp_send( \n    from = \"kimfinale@gmail.com\",\n    to = \"jonghoon.kim@ivi.int\",\n    subject = \"Daily summary of PubMed search\",\n    # credentials = creds_key(id = \"gmail\"),\n    credentials = creds_file(\"gmail_cred\")\n  )\n\n# email %&gt;%\n#   smtp_send(\n#     from = \"jonghoon.kim@ivi.int\",\n#     to = \"jonghoon.kim@ivi.int\",\n#     subject = \"Testing the `smtp_send()` function\",\n#     credentials = creds_key(id = \"outlook\")\n#   )"
  },
  {
    "objectID": "posts/pubmed-chatgpt_summary/index.html#make-it-into-a-single-function",
    "href": "posts/pubmed-chatgpt_summary/index.html#make-it-into-a-single-function",
    "title": "PubMed search, ChatGPT summary, and sending an email in R",
    "section": "Make it into a single function",
    "text": "Make it into a single function\nMake the above functions into a single function and register it for Windows task scheduler such that it can happen every day.\n\npubmed_search_chatgpt_summary &lt;- \n  function(query=\"typhoid\", reldate=10, num_sentence=1,                                          model=\"gpt-3.5-turbo\", temperature=0){\n  library(rentrez)\n  library(httr)\n  library(xml2)\n  library(blastula)\n  # chatgpt_api_token &lt;- readRDS(\"chatGPT_api_key.rds\")\n  chatgpt_api_token &lt;- readRDS(\"G:/My Drive/Personal/chatGPT_api_key.rds\")\n\n  get_completion &lt;- function(prompt, model=\"gpt-3.5-turbo\",\n                             temperature=0, api_token=chatgpt_api_token){\n  response &lt;- POST(\n    url = \"https://api.openai.com/v1/chat/completions\", \n    add_headers(Authorization = paste(\"Bearer\", api_token)),\n    content_type_json(),\n    encode = \"json\",\n    body = list(\n      model = model,\n      temperature = temperature,\n      messages = list(list(\n        role = \"user\", \n        content = prompt))\n    )\n  )\n  return(content(response)$choices[[1]]$message$content)\n}\n  \n  res &lt;- entrez_search(db=\"pubmed\", term=query, datetype=\"pdat\", reldate=reldate)\n  \n  if (length(res$ids) &gt; 0){ # one or more hits\n    ids &lt;- res$ids\n    details &lt;- entrez_fetch(db=\"pubmed\", id=ids, rettype=\"xml\")\n    doc &lt;- read_xml(details)\n    titles &lt;- xml_text(xml_find_all(doc, \"//ArticleTitle\"))\n    abstracts &lt;- xml_text(xml_find_all(doc, \"//Abstract\"))\n    dois &lt;- xml_text(xml_find_all(doc, \"//PubmedData/ArticleIdList/ArticleId[@IdType='doi']\"))\n    \n    abstract_summary &lt;- rep(NA, length(abstracts))\n\n    for (i in 1:length(abstracts)) {\n      prompt &lt;- paste0(\"Your task is to generate a short summary of a scientific article based on its title and abstract. Summarize the text delimited by triple backticks into \", num_sentence , \" sentence. ``` Title: \", titles[i], \". Abstract: \", abstracts[i], \"```\")\n      abstract_summary[i] &lt;- get_completion(prompt=prompt, model=model, temperature=temperature)\n    }\n  summary &lt;- sapply(1:length(abstract_summary), function(i) paste0(\"&lt;p&gt;\", \" &lt;b&gt; \", titles[i], \" &lt;/b&gt; \", abstract_summary[i], \" PMID=\", ids[i] , \" DOI=\", dois[i], \"&lt;/p&gt;\"))\n\n  email &lt;- compose_email(\n    title = \"Weekly summary of PubMed search\",\n    body =  md(summary))\n  \n  smtp_send(\n    email = email,\n    to = \"jonghoon.kim@ivi.int\",\n    from = \"kimfinale@gmail.com\",\n    subject = \"Daily summary of PubMed search\",\n    # credentials = creds_key(id = \"gmail\")\n    credentials = creds_file(\"gmail_cred\")\n  )\n  }\n}\npubmed_search_chatgpt_summary()\n\n\nlibrary(taskscheduleR)\n# Schedule the script to run daily at a specific time\ntaskscheduler_create(taskname = \"PubMed ChatGPT Summary\",\n                     rscript = \"G:/My Drive/Personal/pubmed_search_chatgpt_summary.R\"),\nschedule = \"DAILY\", starttime = \"08:00\")"
  },
  {
    "objectID": "posts/how-to-write-a-paper/index.html",
    "href": "posts/how-to-write-a-paper/index.html",
    "title": "Writing a paper: Start with an outline",
    "section": "",
    "text": "연구자의 업무 중에 연구 만큼 중요한 것이 글쓰기, 특히 논문 쓰기이다. 논문으로 쓰여지지 못한 연구는 타인에게는 존재하지 않는 것이나 다름 없는 것이다.. Writing a paper by George M. Whitesides 에 논문 쓰기에 유용한 팁이 있어 여기에 기록으로 남긴다. 한 마디로 요약하면 outline (개요)을 이용하는 것이다. 개요를 연구과제의 초기에 작성하여 연구의 계획표로 활용하며 공저자 (주로 제 1저자와 책임저자) 간에 논문에 대한 의견 교환시 개요를 사용하는 것이다. 그리고, 표, 수식, 그림 등이 거의 최종 상태에 가까워지면 개요를 바탕으로 논문 쓰기를 시작한다. 이렇게 하면 불필요한 실험 및 쓰기 등을 줄일 수 있다.\n지금까지 연구 과정을 돌아보니 프로젝트 초기에 논문의 개요를 작성하는 과정을 대체로 하긴 했었고 최종 논문 쓰기에 효과적임을 느끼고는 있었다. 그런데 개요를 이용하여 논문의 구조를 고민하고 그림이나 표를 최종 상태로 만든 후에 눈문 텍스트를 작성하는 일은 하지 않았던 것 같다. 대체로 개요, 그림, 그리고 표가 최종 상태가 되기 전에 텍스트를 작성하여, Whitesides 가 적었듯이 최종 논문에 사용되지 않은 텍스트가 많았던 것 같다. 개요를 이용하여 논문의 구조를 고민하고, 그림, 표를 최종본으로 만드는 과정에 노력을 기울이고 텍스트를 더하는 일은 그 이후에 하는 것이 더 효과적일 것 같다."
  },
  {
    "objectID": "posts/drawing-map/index.html",
    "href": "posts/drawing-map/index.html",
    "title": "ggplot2로 지도 그리기",
    "section": "",
    "text": "ggplot2를 이용하여 지도 그리기를 해보자. 지도는 shapefile에 담겨져 있다고 가정하자. shapefile을 읽는 방법은 여러가지가 있을 수 있는데 sf 패키지의 st_read 혹은 read_sf 함수를 이용한 후 ggplot2의 geom_sf를 이용하여 그리는 것이 가장 쉬운 것 같다.\n\nlibrary(sf)\nkor &lt;- st_read(\"C:/Users/jonghoon.kim/Documents/myblog/posts/drawing-map/gadm41_KOR_1.shp\")\n\nReading layer `gadm41_KOR_1' from data source \n  `C:\\Users\\jonghoon.kim\\Documents\\myblog\\posts\\drawing-map\\gadm41_KOR_1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 17 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 124.6097 ymin: 33.11236 xmax: 131.8715 ymax: 38.61177\nGeodetic CRS:  WGS 84\n\n# kor &lt;- read_sf(dsn=\"C:/Users/jonghoon.kim/Documents/myblog/posts/drawing-map\", layer=\"gadm41_KOR_1\")\nset.seed(42)\n# normalized number of characters of the name of the admin unit (level 1)\nchar_len &lt;- sapply(kor$NAME_1, nchar)\nkor$prop_char &lt;- char_len / max(char_len)\n\nlibrary(ggplot2)\nplt &lt;- ggplot(kor)+\n  geom_sf(aes(fill=prop_char))+\n  scale_fill_viridis_c(name=\"Normalized\\ncharacter length\", limits=c(0,1)) +\n  theme_minimal()+\n  theme(legend.position=\"right\")\n\nplt\n\n\n\n# use color brewer \nlibrary(RColorBrewer)\npal &lt;- brewer.pal(9,\"YlOrBr\")\nplt &lt;- plt + \n  scale_fill_gradientn(name=\"Normalized\\ncharacter length\", colors=pal, limits=c(0,1))\n  \nplt\n\n\n\n# Clear some background stuff\nplt &lt;- plt +\n  theme(panel.background = element_blank(), # bg of the panel\n        plot.background = element_blank(), # bg of the plot\n        legend.background = element_blank(), # get rid of legend bg\n        legend.box.background = element_blank(),\n        panel.spacing = unit(c(0,0,0,0), \"null\"),\n        plot.margin = unit(c(0,0,0,0), \"null\"),\n        axis.line = element_blank(),\n        axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title.x = element_blank(),\n        axis.title.y = element_blank(),\n        panel.border = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        legend.position = \"bottom\",\n        plot.title = element_text(hjust=0.5, size=12))\nplt"
  },
  {
    "objectID": "posts/area-polygon/index.html",
    "href": "posts/area-polygon/index.html",
    "title": "Polygon 면적 구하기: sf 와 raster 패키지",
    "section": "",
    "text": "shapefile에 담겨져 있는 polygon의 면적을 구해보자 raster 패키지의 area 혹은 sf 패키지의 st_area 함수를 이용할 수 있다.\n\nlibrary(sf)\nkor &lt;- st_read(\"C:/Users/jonghoon.kim/Documents/myblog/posts/area-polygon/gadm41_KOR_1.shp\")\n\nReading layer `gadm41_KOR_1' from data source \n  `C:\\Users\\jonghoon.kim\\Documents\\myblog\\posts\\area-polygon\\gadm41_KOR_1.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 17 features and 11 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 124.6097 ymin: 33.11236 xmax: 131.8715 ymax: 38.61177\nGeodetic CRS:  WGS 84\n\n# another way\n# kor &lt;- read_sf(dsn=\"C:/Users/jonghoon.kim/Documents/myblog/posts/area-polygon\", layer=\"gadm41_KOR_1\")\nset.seed(42)\n# 1e6 to get population density per 1 km^2\nkor$area_sqkm1 &lt;- as.numeric(st_area(kor))/1e6\n\n# raster package way\n# library(raster)\n# kor &lt;- shapefile(\"C:/Users/jonghoon.kim/Documents/myblog/posts/area-polygon/gadm41_KOR_1.shp\")\n# raster pkg works for the Spatial* object\n# kor$area_sqkm2 &lt;- raster::area(as(kor, 'Spatial'))/1e6\n\n# plot population density\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\n\nlabels = expression(atop(\"Population density\", per~1~km^2))\nplt &lt;- ggplot(kor)+\n  geom_sf(aes(fill=area_sqkm1))+\n  scale_fill_viridis_c(name=labels) +\n  theme(legend.position=\"right\")\n\n# ggsave(\"southkorea_map.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)  \nplt\n\n\n\n# use RColorBrewer \nlibrary(RColorBrewer)\npal &lt;- brewer.pal(9,\"YlOrBr\")\nplt + scale_fill_gradientn(name=labels, colors=pal)"
  },
  {
    "objectID": "posts/odin/index.html",
    "href": "posts/odin/index.html",
    "title": "odin package",
    "section": "",
    "text": "이번 Vaccine Impact Modeling Consoritum (VIMC) 연례회의에서 odin이라는 패키지에 대해 알게 되었다. deSolve의 업그레이드 버전이라고 보면 될까? R 코드를 C언어로 컴파일하기 때문에 최종 모형의 구동속도가 빠르다. 따라서 모형을 여러번 돌려야 하는 경우 (예를 들어 MCMC) 에 유리하다. pomp 보다도 훨씬 더 빠르다고 했는데 정확한 비교 수치는 잘 기억이 안남. 종종 C++로 모형을 만들었는데 odin 패키지를 사용하면 훨씬 쉬워질 것 같다. 좀 더 살펴보아야 할 텐데 일단 잊지 않기 위해 간단히 SIR 모형만 만들어 보았다.\n\nDeterministic SIR model\n\npath_sir_model &lt;- \"C:/Users/jonghoon.kim/Documents/myblog/posts/odin/sir.R\"\nwriteLines(readLines(path_sir_model))\n\n## Core equations for transitions between compartments:\nupdate(S) &lt;- S - beta * S * I / N\nupdate(I) &lt;- I + beta * S * I / N - gamma * I\nupdate(R) &lt;- R + gamma * I\n\n## Total population size (odin will recompute this at each timestep:\n## automatically)\nN &lt;- S + I + R\n\n## Initial states:\ninitial(S) &lt;- S_ini # will be user-defined\ninitial(I) &lt;- I_ini # will be user-defined\ninitial(R) &lt;- 0\n\n## User defined parameters - default in parentheses:\nS_ini &lt;- user(1000)\nI_ini &lt;- user(1)\nbeta &lt;- user(0.2)\ngamma &lt;- user(0.1)\n\n\nRun the model and plot the results\n\nlibrary(odin)\nsir_generator &lt;- odin::odin(path_sir_model)\n\n\nx &lt;- sir_generator$new()\n# see what the object is like\n# x\nsir_col &lt;- c(\"#8c8cd9\", \"#cc0044\", \"#999966\")\nx_res &lt;- x$run(0:200)\n\npar(mar = c(4.1, 5.1, 0.5, 0.5), las = 1)\nmatplot(x_res[, 1], x_res[, -1], xlab = \"Time\", ylab = \"Number of individuals\",\n        type = \"l\", col = sir_col, lty = 1)\nlegend(\"topright\", lwd = 1, col = sir_col, legend = c(\"S\", \"I\", \"R\"), bty = \"n\")\n\n\n\n\n\n\nStochastic SIR model\n\npath_sir_stoch_model &lt;- \"C:/Users/jonghoon.kim/Documents/myblog/posts/odin/sir_stoch.R\"\nwriteLines(readLines(path_sir_stoch_model))\n\n## Core equations for transitions between compartments:\nupdate(S) &lt;- S - n_SI\nupdate(I) &lt;- I + n_SI - n_IR\nupdate(R) &lt;- R + n_IR\n\n## Individual probabilities of transition:\np_SI &lt;- 1 - exp(-beta * I / N) # S to I\np_IR &lt;- 1 - exp(-gamma) # I to R\n\n## Draws from binomial distributions for numbers changing between\n## compartments:\nn_SI &lt;- rbinom(S, p_SI)\nn_IR &lt;- rbinom(I, p_IR)\n\n## Total population size\nN &lt;- S + I + R\n\n## Initial states:\ninitial(S) &lt;- S_ini\ninitial(I) &lt;- I_ini\ninitial(R) &lt;- 0\n\n## User defined parameters - default in parentheses:\nS_ini &lt;- user(1000)\nI_ini &lt;- user(1)\nbeta &lt;- user(0.2)\ngamma &lt;- user(0.1)\n\n\nRun the model and plot the results\n\nsir_generator &lt;- odin::odin(path_sir_stoch_model)\n\n\nset.seed(42)\nx &lt;- sir_generator$new()\nx_res &lt;- x$run(0:200)\npar(mar = c(4.1, 5.1, 0.5, 0.5), las = 1)\nmatplot(x_res[, 1], x_res[, -1], xlab = \"Time\", ylab = \"Number of individuals\",\n        type = \"l\", col = sir_col, lty = 1)\nlegend(\"topright\", lwd = 1, col = sir_col, legend = c(\"S\", \"I\", \"R\"), bty = \"n\")"
  },
  {
    "objectID": "posts/SIR-sympy/index.html",
    "href": "posts/SIR-sympy/index.html",
    "title": "SIR model using SymPy",
    "section": "",
    "text": "I attempted to replicate some of the simple analytical resutls presented in the book, Mathematical Epidemiology by Brauer et al.\n\\[\n\\begin{align}\n\\mathrm{d}S/\\mathrm{d}t &= -\\beta I S \\\\\n\\mathrm{d}I/\\mathrm{d}t &= \\beta I S - \\gamma I\\\\\n\\end{align}\n\\] The first part is simply to compute \\(dI/dS\\).\n\nfrom sympy import *\n\nR_0, b, g, dIdt, dSdt, S, I = symbols('R_0 b g dIdt dSdt S I')\n\ndSdt = - b*S*I\ndIdt = + b*S*I - g*I\ndSdI = dIdt / dSdt #-(I*S*b - I*g)/(I*S*b)\n\n# b &lt;- R0*g\nsimplify(-(I*S*R_0*g - I*g)/(I*S*R_0*g)) \n\n-1 + 1/(R_0*S)\n\n\nThe second part is integrate the equation, \\(dI/dS\\)\n\nS, I = symbols(\"S I\", cls=Function)\nb, g, t, R_0, S0, I0 = symbols(\"b g t R_0 S0 I0\")\n\neq = Eq(I(t).diff(t), - S(t).diff(t) + (1/R_0)*(1/S(t))*S(t).diff(t))\n\nintegrate(eq, t)\n\nEq(I(t), -S(t) + log(S(t))/R_0)"
  },
  {
    "objectID": "posts/crop-raster-polygon/index.html",
    "href": "posts/crop-raster-polygon/index.html",
    "title": "Extract raster based on a polygon",
    "section": "",
    "text": "raster 이미지의 일부를 추출해보자. 특히, shapefile에 담겨져 있는 polygon에 해당하는 raster 를 추출해보자. raster 패키지의 crop 과 mask 함수를 이용할 수 있다.\n\n# Create some data using meuse \nlibrary(raster)\ndata(meuse)\ncoordinates(meuse) &lt;- ~x+y\nproj4string(meuse) &lt;- CRS(\"+init=epsg:28992\")\ndata(meuse.grid)\ncoordinates(meuse.grid) = ~x+y\nproj4string(meuse.grid) &lt;- CRS(\"+init=epsg:28992\")\ngridded(meuse.grid) = TRUE    \nr &lt;- raster(meuse.grid) \nr[] &lt;- runif(ncell(r))\n\n# Create a polygon\nf &lt;- rgeos::gBuffer(meuse[10,], byid=FALSE, id=NULL, width=250, \n                         joinStyle=\"ROUND\", quadsegs=10)   \n\n# Plot full raster and polygon                       \nplot(r)\nplot(f,add=T)\n\n\n\n# Crop using extent, rasterize polygon and finally, create poly-raster\n#          **** This is the code that you are after ****  \ncr &lt;- crop(r, extent(f), snap=\"out\")                    \nfr &lt;- rasterize(f, cr)   \nlr &lt;- mask(x=cr, mask=fr)\n\n# Plot results\nplot(lr)\nplot(f,add=T)"
  },
  {
    "objectID": "posts/nb-regression-optim/index.html",
    "href": "posts/nb-regression-optim/index.html",
    "title": "Regression using optim",
    "section": "",
    "text": "Data\nI will use the cars data with give the speed of cars and the distances taken to stop.\n\nd &lt;- datasets::cars\nm &lt;- lm(dist ~ speed, data=d)\nsummary(m)\n\n\nCall:\nlm(formula = dist ~ speed, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-29.069  -9.525  -2.272   9.215  43.201 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -17.5791     6.7584  -2.601   0.0123 *  \nspeed         3.9324     0.4155   9.464 1.49e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.38 on 48 degrees of freedom\nMultiple R-squared:  0.6511,    Adjusted R-squared:  0.6438 \nF-statistic: 89.57 on 1 and 48 DF,  p-value: 1.49e-12\n\n\n\n\nPlot\nPlot estimates with confidence and prediction intervals\n\npred &lt;- predict(m, interval=\"prediction\", level=0.95) # prediction interval\nconf &lt;- predict(m, interval=\"confidence\", level=0.95) # confidence interval\n\nmdat &lt;- m$model\nmdat$pred_estimate &lt;- pred[,1]\nmdat$pred_lb &lt;- pred[,2]\nmdat$pred_ub &lt;- pred[,3]\nmdat$conf_estimate &lt;- conf[,1]\nmdat$conf_lb &lt;- conf[,2]\nmdat$conf_ub &lt;- conf[,3]\n\nmdat$residuals &lt;- residuals(m)\nlibrary(ggplot2)\nlibrary(dplyr)\n\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\npltcar &lt;- mdat %&gt;% \n  ggplot(aes(speed, dist))+\n  # geom_point(aes(size = abs(m$residuals)))+\n  geom_point(aes(color=\"Data\"), size = 1)+\n  geom_line(aes(y=pred_estimate, color=\"Model\"))+\n  geom_ribbon(aes(ymax=pred_ub, ymin=pred_lb, fill=\"Model\"), alpha=0.2)+\n  # geom_line(aes(y=conf_estimate), color=\"steelblue\")+\n  geom_ribbon(aes(ymax=conf_ub, ymin=conf_lb, fill=\"Model\"), alpha=0.5)+\n  scale_color_manual(\"\", values=c(\"Data\"=\"firebrick\"))+\n  scale_fill_manual(\"\", values=c(\"Model\"=\"steelblue\"))+\n  labs(x=\"Speed\", y=\"Distance\", title=\"Speed and Stopping Distances of Cars\")+\n  theme(legend.position=\"bottom\")\nggsave(\"plot_car.png\", pltcar)\npltcar\n\n\n\n\n\n\noptim function\nNow let’s take an alternative approach to write down the likelihood function and maximize it using the optim function\n\n# define our likelihood function we like to optimize\nnegloglik &lt;- function(par, y, X){\n  sigma &lt;- par[1]\n  beta &lt;- par[-length(par)]\n  mu &lt;- X %*% beta \n  - sum(dnorm(y, mean=mu, sd=sigma, log=TRUE), na.rm=T)\n}\n\nX = model.matrix(m)\ninit = c(coef(m), sigma=summary(m)$sigma)\n# check\n# negloglik(par=init, y=d$y, X=X)\nfit &lt;- optim(par=init, \n             fn=negloglik, \n             y=d$dist, \n             X=X, \n             control=list(reltol=1e-6))\n\nLet’s compare the results.\n\nfit$par\n\n(Intercept)       speed       sigma \n -17.579095    3.932409   15.379587 \n\ncoef(m)\n\n(Intercept)       speed \n -17.579095    3.932409"
  },
  {
    "objectID": "posts/nb-censored-regression/index.html",
    "href": "posts/nb-censored-regression/index.html",
    "title": "Negative binomial regression with censored data: POLYMOD data",
    "section": "",
    "text": "This post describes my attempt to reproduce Table 1 of the paper, Social Contacts and Mixing Patterns Relevant to the Spread of Infectious Diseases. Data were downloaded from Social Contact Data, which was hosted in zenodo. I used the version 1.1. In summary, I wasn’t successful at reproducing the table exactly but still wanted to document the processes that I went through.\n\nData preparation\n\nlibrary(data.table)\nd1 &lt;- fread(\"2008_Mossong_POLYMOD_participant_common.csv\")\n# d1 &lt;- fread(\"data/2008_Mossong_POLYMOD_participant_common.csv\")\nd2 &lt;- fread(\"2008_Mossong_POLYMOD_contact_common.csv\")\n# d2 &lt;- fread(\"data/2008_Mossong_POLYMOD_contact_common.csv\")\nlibrary(dplyr)\n# count the number of contacts for each participant using part_id variable\nd2 |&gt; group_by(part_id) |&gt;\n  summarize(contacts = n()) -&gt; d2_contacts\nd12 &lt;- left_join(d1, d2_contacts, by=\"part_id\")\n# add household information\nd3 &lt;- fread(\"2008_Mossong_POLYMOD_hh_common.csv\")\n# d3 &lt;- fread(\"data/2008_Mossong_POLYMOD_hh_common.csv\")\nd123 &lt;- left_join(d12, d3, by=\"hh_id\")\n# add day of week information\nd4 &lt;- fread(\"2008_Mossong_POLYMOD_sday.csv\")\n# d4 &lt;- fread(\"data/2008_Mossong_POLYMOD_sday.csv\")\ndat &lt;- left_join(d123, d4, by=\"part_id\")\n\n\n\nData manipulation\nCategorize the age group into different 10 age groups: 0-4, 5-9, 10-14, 15-19, and 20 to 70 by 10 years and 70 and above\n\nage_grp_label &lt;- c(\"0-4\",\"5-9\",\"10-14\",\"15-19\",\"20-29\",\"30-39\",\"40-49\",\"50-59\",\"60-69\",\"70+\")\n\nclassify_age &lt;- function(d){\n  d$age_grp &lt;- \"Missing\"\n  for (i in 1:nrow(d)) {\n    if(!is.na(d$part_age[i])){\n      if(d$part_age[i] &lt; 5){\n        d$age_grp[i] &lt;- age_grp_label[1]\n      }\n      else if (d$part_age[i] &gt;= 5 && d$part_age[i] &lt; 20){\n        for(j in 1:3){\n          if(d$part_age[i] &gt;= 5*j && d$part_age[i] &lt; (5*j+5)){\n            d$age_grp[i] &lt;- age_grp_label[j+1]\n          }\n        }\n      }\n      else if (d$part_age[i] &gt;= 20 && d$part_age[i] &lt; 70){\n        for (k in 1:5){\n          if (d$part_age[i] &gt;= (10+10*k) && d$part_age[i] &lt; (20+10*k)){\n            d$age_grp[i] &lt;- age_grp_label[k+4]\n          }\n        }\n      } \n      else {\n        d$age_grp[i] &lt;- age_grp_label[10]\n      }\n    } \n  }\n  return(d)\n}\n\ndat &lt;- classify_age(dat)\n\nCompare the number of participants by age group (the third column of Table 1)\n\ndat |&gt; group_by(age_grp) |&gt; summarize(npart=n()) -&gt; npart_ag\nnpart_ag$true &lt;- c(660,661,713,685,879,815,908,906,728,270,65) # hard-coded using the values in Table 1.\nnpart_ag\n\n# A tibble: 11 × 3\n   age_grp npart  true\n   &lt;chr&gt;   &lt;int&gt; &lt;dbl&gt;\n 1 0-4       660   660\n 2 10-14     713   661\n 3 15-19     685   713\n 4 20-29     879   685\n 5 30-39     815   879\n 6 40-49     908   815\n 7 5-9       661   908\n 8 50-59     906   906\n 9 60-69     728   728\n10 70+       270   270\n11 Missing    65    65\n\n\nCategorize the household size\n\nclassify_household &lt;- function(d){\n  d$hh_size_grp &lt;- \"Missing\"\n  d$hh_size_grp &lt;- ifelse(d$hh_size &gt; 5, \"6+\", as.character(d$hh_size))\n  return(d)\n}\ndat &lt;- classify_household(dat)\n\nClassify gender\n\nclassify_gender &lt;- function(d) {\n  d$gender &lt;- \"Missing\"\n  d$gender &lt;- ifelse(d$part_gender == \"M\", \"M\", ifelse(d$part_gender == \"F\", \"F\", d$gender))\n  \n  return(d)\n}\n\ndat &lt;- classify_gender(dat)\n\nClassify day of week\n\ndayofweek_label &lt;- c(\"Sunday\",\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\")\n\nclassify_dayofweek &lt;- function(d) {\n  d$dayofweek_f &lt;- \"Missing\"\n  for (i in 1:nrow(d)) {\n    day = d$dayofweek[i]\n    if (!is.na(day)) {\n      d$dayofweek_f[i] &lt;- dayofweek_label[day+1]\n    }\n  }\n  return(d)\n}\ndat &lt;- classify_dayofweek(dat)\n# change names for conveninence\ndat$dayofweek_integer &lt;- dat$dayofweek\ndat$dayofweek &lt;- dat$dayofweek_f\n\nMake categorical variables factor for regression Set reference groups relevel(x, ref=ref) as in Table 1\n\n# set the categorical variables as factor for regression\ndat$age_grp &lt;- factor(dat$age_grp, levels=c(age_grp_label,\"Missing\"))\ndat$age_grp &lt;- relevel(dat$age_grp, ref=\"0-4\")\ndat$gender &lt;- factor(dat$gender, levels=c(\"F\",\"M\",\"Missing\"))\ndat$gender &lt;- relevel(dat$gender, ref = \"F\")\ndat$dayofweek &lt;- factor(dat$dayofweek, levels=c(dayofweek_label,\"Missing\"))\ndat$dayofweek &lt;- relevel(dat$dayofweek, ref = \"Sunday\")\ndat$hh_size_grp &lt;- as.factor(dat$hh_size_grp)\ndat$hh_size_grp &lt;- relevel(dat$hh_size_grp, ref=\"1\")\ndat$country &lt;- factor(dat$country, levels=c(\"BE\",\"DE\",\"FI\",\"GB\",\"IT\",\"LU\",\"NL\",\"PL\"))\ndat$country &lt;- relevel(dat$country, ref=\"BE\")\n\n# fwrite(dat, \"POLYMOD_2017.csv\")\n\nAssign weights to individual participants based on the supplementary Table 2. My approach was to identify a row and a column for the relevant weight based on the age and household size. Weight of an age group for the sample was calculated by dividing the proportion of the age group in the population (in the census) with the proportion of the age group in the sample.\n\nfind_age_row_column &lt;- function(d) {\n  d$age_row &lt;- NA\n  for (i in 1:nrow(d)) {\n    ag &lt;- d$part_age[i]\n    if(!is.na(ag)){\n      for (j in 1:14) {\n        if (ag &gt;= (j-1)*5 & ag &lt; (j-1)*5+5) {\n          d$age_row[i] &lt;- j\n          break\n        }\n        else if (ag &gt;= 70) {\n          d$age_row[i] &lt;- 15\n          break\n        }\n        else{}\n      }\n    }\n  }\n  \n  d$hh_col &lt;- NA\n  for (i in 1:nrow(d)) {\n    hs &lt;- d$hh_size[i]\n    if(!is.na(hs)){\n      for (j in 1:4) {\n        if (hs == j) {\n          d$hh_col[i] &lt;- j\n          break\n        }\n        else if (hs &gt; 4) {\n          d$hh_col[i] &lt;- 5\n        }\n        else{}\n      }\n    }\n  }\n  return(d)\n}\n\ndat &lt;- find_age_row_column(dat)\n# wlist &lt;- rio::import_list(\"data/sampling_weight.xlsx\")\nwlist &lt;- rio::import_list(\"sampling_weight.xlsx\")\n\nclassify_weight &lt;- function(d){\n  d$wt &lt;- NA\n  cnames &lt;- names(wlist)\n  for (i in 1:length(cnames)) {\n    wtable &lt;- wlist[[i]]\n    w1 &lt;- wtable[wtable$`Household size` == \"Ratio C/S\",] # sampling weight\n    W &lt;- w1[!is.na(w1$`Household size`),] # remove the first row\n    # View(W)\n    for (j in 1:nrow(d)){\n      if(d$country[j] == cnames[i]) {\n        d$wt[j] &lt;- W[d$age_row[j], d$hh_col[j]+2]\n      }\n    }\n  }\n  return(d)\n}\n\n# grep(\"-\", as.character(d$wt), value = T)\n# hist(as.numeric(d$wt))\ndat &lt;- classify_weight(dat)\ndat$wt &lt;- as.numeric(dat$wt)\n\n\n\nData for fitting only complete cases\nThere are missing values for contacts ($n$=36) and weight ($n$=65). It is not clear how those observations were treated in the model. This may be a reason why I can’t reproduce the results in Table 1.\n\ndat_ &lt;- dat\n## would imputation for the 36 observations make a difference?\n# dat$contacts_ori &lt;- dat$contacts\n# dat$contacts &lt;- ifelse(is.na(dat$contacts), round(mean(dat$contacts, na.rm=T)), dat$contacts)\n\nmodel_var &lt;- c(\"contacts\", \"age_grp\", \"gender\", \"dayofweek\", \"hh_size_grp\", \"country\", \"wt\")\ndat &lt;- dat[,..model_var]\n# dat &lt;- dat[complete.cases(dat),]\ndat &lt;- dat[!is.na(contacts),]\n\n\n\nNegBin regression: no censoring and no weighting\n\nlibrary(MASS)\nm &lt;- glm.nb(contacts ~ age_grp + gender + dayofweek + hh_size_grp + country, data = dat)\n# summary(m4)\nexp(m$coefficients)\n\n       (Intercept)         age_grp5-9       age_grp10-14       age_grp15-19 \n         5.6764958          1.4022341          1.6768688          1.6824225 \n      age_grp20-29       age_grp30-39       age_grp40-49       age_grp50-59 \n         1.4458386          1.4168460          1.3868642          1.2940456 \n      age_grp60-69         age_grp70+     age_grpMissing            genderM \n         1.0520198          0.8120547          1.0338752          0.9941243 \n     genderMissing    dayofweekMonday   dayofweekTuesday dayofweekWednesday \n         1.3434436          1.3179053          1.3998594          1.3876252 \n dayofweekThursday    dayofweekFriday  dayofweekSaturday   dayofweekMissing \n         1.3858679          1.4403113          1.1542837          1.2179674 \n      hh_size_grp2       hh_size_grp3       hh_size_grp4       hh_size_grp5 \n         1.0990550          1.1255471          1.2592436          1.3360603 \n     hh_size_grp6+          countryDE          countryFI          countryGB \n         1.4601697          0.7036079          0.9483997          0.9797998 \n         countryIT          countryLU          countryNL          countryPL \n         1.6351704          1.4081732          1.2493105          1.3223343 \n\n\nRegression that account for censored number of contacts. The paper reads: The data were right censored at 29 contacts for all countries because of a limited number of possible diary entries in some countries\n\\[\n\\text{log }L = \\sum_{i=1}^n w_i\\left(\\delta_i~\\text{log} \\left(P\\left(Y=y_i|X\\right)\\right) + \\left(1-\\delta_i\\right)~\\text{log} \\left(1-\\sum_{i=1}^{28}P(Y=y_i|X)\\right)\n\\right)\n\\] , where\n\\[\n\\begin{equation}\n  \\delta_i=\\begin{cases}\n    1, & \\text{if$~y_i&lt;29$}.\\\\\n    0, & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}\n\\]\n\n\nTake censoring into account\n\nX = model.matrix(m)\n# X &lt;- model.matrix(~ age_grp + gender + dayofweek + hh_size_grp + country, data=dat)\n\n# ini = c(coef(m1), log_theta = log(summary(m1)$theta))\ninit = c(coef(m), size=summary(m)$theta)\n\nnegll_censor &lt;- function(par, y, X, ul=400) {\n  # parameters\n  size = par[length(par)]\n  beta = par[-length(par)]\n  # create indicator depending on chosen limit\n  indicator = y &lt; ul\n  # linear predictor\n  mu = exp(X %*% beta)\n  # log likelihood\n  ll = sum(indicator * dnbinom(y, mu=mu, size=size, log=T) +\n             (1-indicator) * log(1-pnbinom(ul, mu=mu, size=size)), na.rm=T)\n  return(-ll)\n}\n\n# you can check if two methods (glm.nb vs. optim) match by setting ul high (e.g., 100)\nfit1 &lt;- optim(par=init,\n            negll_censor,\n            y = dat$contacts,\n            X = X,\n            ul = 29,\n            method = \"Nelder-Mead\",\n            control = list(maxit=1e3, reltol=1e-10))\nexp(fit1$par)\n\n       (Intercept)         age_grp5-9       age_grp10-14       age_grp15-19 \n         5.6047431          1.4335817          1.7328973          1.7131792 \n      age_grp20-29       age_grp30-39       age_grp40-49       age_grp50-59 \n         1.4411616          1.4214491          1.3811132          1.2825334 \n      age_grp60-69         age_grp70+     age_grpMissing            genderM \n         1.0518372          0.8182114          1.0369387          0.9825757 \n     genderMissing    dayofweekMonday   dayofweekTuesday dayofweekWednesday \n         1.3945323          1.3504467          1.4340414          1.4146111 \n dayofweekThursday    dayofweekFriday  dayofweekSaturday   dayofweekMissing \n         1.4265521          1.4638406          1.1583928          1.2481137 \n      hh_size_grp2       hh_size_grp3       hh_size_grp4       hh_size_grp5 \n         1.0879976          1.1266973          1.2623833          1.3638927 \n     hh_size_grp6+          countryDE          countryFI          countryGB \n         1.4878961          0.6934848          0.9561751          1.0065923 \n         countryIT          countryLU          countryNL          countryPL \n         1.6915581          1.3931403          1.2521232          1.3348934 \n              size \n        18.9737736 \n\n\n\n\nTake censoring & weighting into account\n\nX = model.matrix(m)\n# X &lt;- model.matrix(~ age_grp + gender + dayofweek + hh_size_grp + country, data=dat) # full matrix\n# ini = c(coef(m1), log_theta = log(summary(m1)$theta))\ninit = c(coef(m), size=summary(m)$theta)\n\nnegll_censor_weight &lt;- function(par, y, X, wt, ul=400) {\n  # parameters\n  size = par[length(par)]\n  beta = par[-length(par)]\n  # create indicator depending on chosen limit\n  indicator = y &lt; ul\n  # linear predictor\n  mu = exp(X %*% beta)\n  # log likelihood\n  ll = sum(wt*(indicator * dnbinom(y, mu=mu, size=size, log=T)\n  + (1-indicator) * log(1-pnbinom(ul, mu=mu, size=size))), na.rm=T)\n\n  return(-ll)\n}\n\nfit2 &lt;- optim(par=init,\n            negll_censor_weight,\n            y = dat$contacts,\n            X = X,\n            wt = dat$wt,\n            ul = 29,\n            method = \"Nelder-Mead\",\n            control = list(maxit=1e3, reltol=1e-10))\n\nexp(fit2$par)\n\n       (Intercept)         age_grp5-9       age_grp10-14       age_grp15-19 \n         5.6120605          1.4242420          1.7386774          1.7031201 \n      age_grp20-29       age_grp30-39       age_grp40-49       age_grp50-59 \n         1.4590198          1.4488638          1.3920936          1.3145593 \n      age_grp60-69         age_grp70+     age_grpMissing            genderM \n         1.0690678          0.8092529          0.9944973          0.9919151 \n     genderMissing    dayofweekMonday   dayofweekTuesday dayofweekWednesday \n         1.0153858          1.3261171          1.3904167          1.3853431 \n dayofweekThursday    dayofweekFriday  dayofweekSaturday   dayofweekMissing \n         1.4001935          1.4212925          1.1905515          1.2452143 \n      hh_size_grp2       hh_size_grp3       hh_size_grp4       hh_size_grp5 \n         1.1014090          1.1254771          1.2788309          1.3717150 \n     hh_size_grp6+          countryDE          countryFI          countryGB \n         1.4784786          0.6889027          0.9561525          0.9925714 \n         countryIT          countryLU          countryNL          countryPL \n         1.6631404          1.4198698          1.3256452          1.3635879 \n              size \n        17.7725195 \n\n\n\nda &lt;- data.frame(\n  parm=c(\"0-4\", \"5-9\", \"10-14\", \"15-19\", \"20-29\", \"30-39\", \"40-49\", \n\"50-59\", \"60-69\", \"70+\", \"Missing\", \n\"F\",\"M\",\"Missing\",\n\"1\", \"2\", \"3\", \"4\", \"5\", \"6+\",\n\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \n\"Saturday\", \"Missing\",\n\"BE\", \"DE\", \"FI\", \"GB\", \"IT\", \"LU\", \"NL\", \"PL\"),\nest = c(1.00, 1.42, 1.73, 1.68, 1.45, 1.45, 1.38, 1.31, 1.06, 0.81,0.91, \n        1.00, 0.99, 1.57, \n        1.00, 1.17, 1.20, 1.36, 1.46, 1.56,\n        1.00, 1.33, 1.39, 1.38, 1.41, 1.43, 1.20, 1.24,\n        1.00, 0.70, 0.94, 0.99, 1.66, 1.42, 1.34, 1.37))\n\nda$myest = round(c(1.00, exp(fit2$par)[2:11], \n             1.00, exp(fit2$par)[12:13],\n             1.00, exp(fit2$par)[21:25],\n             1.00, exp(fit2$par)[14:20],\n             1.00, exp(fit2$par)[26:32]), digits=2)\nda\n\n        parm  est myest\n1        0-4 1.00  1.00\n2        5-9 1.42  1.42\n3      10-14 1.73  1.74\n4      15-19 1.68  1.70\n5      20-29 1.45  1.46\n6      30-39 1.45  1.45\n7      40-49 1.38  1.39\n8      50-59 1.31  1.31\n9      60-69 1.06  1.07\n10       70+ 0.81  0.81\n11   Missing 0.91  0.99\n12         F 1.00  1.00\n13         M 0.99  0.99\n14   Missing 1.57  1.02\n15         1 1.00  1.00\n16         2 1.17  1.10\n17         3 1.20  1.13\n18         4 1.36  1.28\n19         5 1.46  1.37\n20        6+ 1.56  1.48\n21    Sunday 1.00  1.00\n22    Monday 1.33  1.33\n23   Tuesday 1.39  1.39\n24 Wednesday 1.38  1.39\n25  Thursday 1.41  1.40\n26    Friday 1.43  1.42\n27  Saturday 1.20  1.19\n28   Missing 1.24  1.25\n29        BE 1.00  1.00\n30        DE 0.70  0.69\n31        FI 0.94  0.96\n32        GB 0.99  0.99\n33        IT 1.66  1.66\n34        LU 1.42  1.42\n35        NL 1.34  1.33\n36        PL 1.37  1.36"
  },
  {
    "objectID": "posts/censored-regression-optim/index.html",
    "href": "posts/censored-regression-optim/index.html",
    "title": "Regression with censored data: AER::tobit and optim",
    "section": "",
    "text": "The following example was adapted from the Tobit model in Model Estimation by Example. The dataset contains 200 observations. The academic aptitude variable is apt, the reading and math test scores are read and math, respectively. The variable prog is the type of program the student is in, it is a categorical (nominal) variable that takes on three values, academic (prog = 1), general (prog = 2), and vocational (prog = 3). The variable id is an identification variable. More details of the dataset available at https://stats.oarc.ucla.edu/r/dae/tobit-models/.\n\nlibrary(data.table)\ndat = fread(\"https://stats.idre.ucla.edu/stat/data/tobit.csv\")\ndat[, prog := as.factor(prog)]\ndat\n\n      id read math       prog apt\n  1:   1   34   40 vocational 352\n  2:   2   39   33 vocational 449\n  3:   3   63   48    general 648\n  4:   4   44   41    general 501\n  5:   5   47   43    general 762\n ---                             \n196: 196   44   49    general 539\n197: 197   50   50    general 594\n198: 198   47   51    general 616\n199: 199   52   50    general 558\n200: 200   68   75    general 800\n\n\nFollowing codes were borrowed from the UCLA Advanced Research Computing\n\n# function that gives the density of normal distribution\n# for given mean and sd, scaled to be on a count metric\n# for the histogram: count = density * sample size * bin width\nf &lt;- function(x, var, bw = 15) {\n  dnorm(x, mean = mean(var), sd(var)) * length(var) * bw\n}\nlibrary(ggplot2)\n# setup base plot\np &lt;- ggplot(dat, aes(x = apt, fill=prog))\n# histogram, coloured by proportion in different programs\n# with a normal distribution overlayed\np &lt;- p + stat_bin(binwidth=15) + \n  stat_function(fun = f, size = 1,\n    args = list(var = dat$apt))\n\nggsave(\"apt_censored.png\", p)\n\nLooking at the above histogram, we can see the censoring in the values of apt, that is, there are far more cases with scores of 750 to 800 than one would expect looking at the rest of the distribution. Below is an alternative histogram that further highlights the excess of cases where apt=800.\nNote on the difference between truncation and censoring: With censored variables, all of the observations are in the dataset, but we don’t know the “true” values of some of them. With truncation some of the observations are not included in the analysis because of the value of the variable.\nThe Tobit model can be used for such a case. It is a class of regression models in which the observed range of the dependent variable is censored in some way, according to the [Wikipedia article] (https://en.wikipedia.org/wiki/Tobit_model). The possible maximum score 800, which\n\ntobit = AER::tobit(\n  apt ~ read + math + prog,\n  data = dat,\n  left = -Inf,\n  right = 800\n)\n\nTo account for censoring, likelihood function is modified to so that it reflects the unequal sampling probability for each observation depending on whether the latent dependent variable fell above or below the determined threshold. It appears that this approach was first proposed by James Tobin. For a sample that, as in Tobin’s original case, was censored from below at zero, the sampling probability for each non-limit observation is simply the height of the appropriate density function. For any limit observation, it is the cumulative distribution, i.e. the integral below zero of the appropriate density function. The likelihood function is thus a mixture of densities and cumulative distribution functions, according to the Wikipedia article.\n\\[\n\\text{log }L = \\sum_{i=1}^n w_i\\left(\\delta_i~\\text{log} \\left(P\\left(Y=y_i|X\\right)\\right) + \\left(1-\\delta_i\\right)~\\text{log} \\left(1-\\sum_{i=1}^{l_U}P(Y=y_i|X)\\right)\n\\right)\n\\] , where \\(l_U\\) represents the upper limit and\n\\[\n\\begin{equation}\n  \\delta_i=\\begin{cases}\n    1, & \\text{if }y_i &lt; l_U.\\\\\n    0, & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}\n\\]\n\nLog likelihood accounting for censoring\n\nnegloglik &lt;- function(par, y, X, ul=100) {\n  # parameters\n  sd = par[length(par)]\n  beta = par[-length(par)]\n  # create indicator depending on chosen limit\n  indicator = y &lt; ul\n  # linear predictor\n  mu = X %*% beta\n  # log likelihood\n  loglik = indicator * dnorm(y, mean=mu, sd=sd, log=T) +\n             (1-indicator) * log(1-pnorm(ul, mean=mu, sd=sd))\n\n  sumloglik = sum(loglik, na.rm=T)\n  return(-sumloglik)\n}\n\n\n\noptim\n\n# Setup data and initial values.\nmod = lm(apt ~ read + math + prog, data = dat)\nX = model.matrix(mod)\ninit = c(coef(mod), sigma=summary(mod)$sigma)\n\n# negloglik(par=init, y=acad_apt$apt, X=X, ul=800)\n\nfit &lt;- optim(par = init,\n            fn = negloglik,\n            y = dat$apt,\n            X = X,\n            ul = 800)\n\ncoef(tobit)\n\n   (Intercept)           read           math    proggeneral progvocational \n    209.565971       2.697939       5.914485     -12.714763     -46.143904 \n\n(fit$par)\n\n   (Intercept)           read           math    proggeneral progvocational \n    211.054867       2.813491       5.763177     -12.354492     -45.847701 \n         sigma \n     65.652919 \n\n\n\n\noptim control parameters\nBy adjusting control parameters of the optim function, results can match more closely. Below is such an example.\n\nfit &lt;- optim(par = init,\n            fn = negloglik,\n            y = dat$apt,\n            X = X,\n            ul = 800,\n            method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\ncoef(tobit)\n\n   (Intercept)           read           math    proggeneral progvocational \n    209.565971       2.697939       5.914485     -12.714763     -46.143904 \n\n(fit$par)\n\n   (Intercept)           read           math    proggeneral progvocational \n    209.565967       2.697937       5.914486     -12.714777     -46.143838 \n         sigma \n     65.676724"
  },
  {
    "objectID": "posts/POLYMOD-multiple-regression/index.html",
    "href": "posts/POLYMOD-multiple-regression/index.html",
    "title": "Mulitple regression: POLYMOD data",
    "section": "",
    "text": "This post describes my attempt to reproduce Table 1 of the paper, Social Contacts and Mixing Patterns Relevant to the Spread of Infectious Diseases. Data were downloaded from Social Contact Data, which was hosted in zenodo. I used the version 1.1. In summary, I wasn’t successful at reproducing the table exatly but still wanted to document the processes that I went through.\n\nData preparation\n\nlibrary(data.table)\n\nd1 &lt;- fread(\"2008_Mossong_POLYMOD_participant_common.csv\")\nd2 &lt;- fread(\"2008_Mossong_POLYMOD_contact_common.csv\")\nlibrary(dplyr)\n# count the number of contacts for each participant using part_id variable\nd2 |&gt; group_by(part_id) |&gt;\n  summarize(contacts = n()) -&gt; d2_contacts\nd12 &lt;- left_join(d1, d2_contacts, by=\"part_id\")\n# add household information\nd3 &lt;- fread(\"2008_Mossong_POLYMOD_hh_common.csv\")\nd123 &lt;- left_join(d12, d3, by=\"hh_id\")\n# add day of week information\nd4 &lt;- fread(\"2008_Mossong_POLYMOD_sday.csv\")\ndat &lt;- left_join(d123, d4, by=\"part_id\")\n\n\n\nData manipulation\nCategorize the age group into different 10 age groups: 0-4, 5-9, 10-14, 15-19, and 20 to 70 by 10 years and 70 and above\n\nclassify_age &lt;- function(d){\n  d$age_grp &lt;- 99\n  for (i in 1:nrow(d)) {\n    if(!is.na(d$part_age[i])){\n      if(d$part_age[i] &lt; 5){\n        d$age_grp[i] &lt;- 0\n      }\n      else if (d$part_age[i] &gt;= 5 && d$part_age[i] &lt; 20){\n        for(j in 1:3){\n          if(d$part_age[i] &gt;= 5*j && d$part_age[i] &lt; (5*j+5)){\n            d$age_grp[i] &lt;- j\n          }\n        }\n      }\n      else if (d$part_age[i] &gt;= 20 && d$part_age[i] &lt; 70){\n        for (k in 1:5){\n          if (d$part_age[i] &gt;= (10+10*k) && d$part_age[i] &lt; (20+10*k)){\n            d$age_grp[i] &lt;- k+3\n          }\n        }\n      } \n      else {\n        d$age_grp[i] &lt;- 9\n      }\n    } \n  }\n  d\n}\n\ndat &lt;- classify_age(dat)\n\n\nlibrary(ggplot2)\ndat |&gt; ggplot(aes(x=contacts)) + \n  geom_histogram(binwidth=5)\n\n\n\n\nCompare the number of participants by age group (the third column)\n\ndat |&gt; \n  group_by(age_grp) |&gt; \n  summarize(npart=n(),\n            avg_contacts = round(sum(contacts, na.rm=T) / npart, digits=2)) -&gt; dat_ag\n\ndat_ag$npart_true &lt;- c(660,661,713,685,879,815,908,906,728,270,65)\ndat_ag$avg_contacts_true &lt;- c(10.21,14.81, 18.22,17.58,13.57,14.14,13.83,12.30,9.21,6.89,9.63)\ndat_ag\n\n# A tibble: 11 × 5\n   age_grp npart avg_contacts npart_true avg_contacts_true\n     &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;      &lt;dbl&gt;             &lt;dbl&gt;\n 1       0   660        10.2         660             10.2 \n 2       1   661        14.8         661             14.8 \n 3       2   713        18.2         713             18.2 \n 4       3   685        17.6         685             17.6 \n 5       4   879        13.6         879             13.6 \n 6       5   815        14.1         815             14.1 \n 7       6   908        13.8         908             13.8 \n 8       7   906        12.3         906             12.3 \n 9       8   728         9.21        728              9.21\n10       9   270         6.89        270              6.89\n11      99    65         9.63         65              9.63\n\n\nCategorize the household size\n\nclassify_hh &lt;- function(d){\n  d$hh_size_grp &lt;- ifelse(d$hh_size &gt; 4, 5, ifelse(!is.na(d$hh_size), d$hh_size, 99))\n  d\n}\ndat &lt;- classify_hh(dat)\n\nMake categorical variables factor for regression\n\n# set the categorical variables as factor for regression\ndat$age_grp &lt;- as.factor(dat$age_grp)\ndat$hh_size_grp &lt;- as.factor(dat$hh_size_grp)\ndat$gender &lt;- as.factor(dat$part_gender)\ndat$dayofweek &lt;- as.factor(dat$dayofweek)\ndata.table::fwrite(dat, \"POLYMOD_2017.csv\")\n\n\n\ndata for fitting only complete cases\n\ndat_ &lt;- dat\ndat &lt;- dat[complete.cases(dat),]\n# set the maximum number of contacts 29\ndat$contacts = ifelse(dat$contacts &gt; 29, 29, dat$contacts)\n\n\n\nNegBin regression\nNegative binomial regression was modeled using MASS::glm.nb function\n\nlibrary(MASS)\nm1 &lt;- glm.nb(contacts ~ age_grp, data = dat)\nsummary(m1)\n\n\nCall:\nglm.nb(formula = contacts ~ age_grp, data = dat, init.theta = 2.895747992, \n    link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.30861    0.02626  87.919  &lt; 2e-16 ***\nage_grp1     0.32714    0.03648   8.967  &lt; 2e-16 ***\nage_grp2     0.49578    0.03556  13.943  &lt; 2e-16 ***\nage_grp3     0.46119    0.03600  12.812  &lt; 2e-16 ***\nage_grp4     0.24010    0.03442   6.975 3.05e-12 ***\nage_grp5     0.28582    0.03483   8.206 2.29e-16 ***\nage_grp6     0.24679    0.03418   7.220 5.21e-13 ***\nage_grp7     0.14499    0.03429   4.228 2.35e-05 ***\nage_grp8    -0.09882    0.03656  -2.703  0.00687 ** \nage_grp9    -0.36270    0.05032  -7.208 5.66e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.8957) family taken to be 1)\n\n    Null deviance: 8055.6  on 7068  degrees of freedom\nResidual deviance: 7390.5  on 7059  degrees of freedom\nAIC: 47802\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.8957 \n          Std. Err.:  0.0597 \n\n 2 x log-likelihood:  -47780.1930 \n\nexp(m1$coefficients)\n\n(Intercept)    age_grp1    age_grp2    age_grp3    age_grp4    age_grp5 \n 10.0604651   1.3869981   1.6417727   1.5859529   1.2713769   1.3308557 \n   age_grp6    age_grp7    age_grp8    age_grp9 \n  1.2799167   1.1560315   0.9059023   0.6957929 \n\nm5 &lt;- glm.nb(contacts ~ age_grp + gender + hh_size_grp + country + dayofweek, data = dat)\nsummary(m5)\n\n\nCall:\nglm.nb(formula = contacts ~ age_grp + gender + hh_size_grp + \n    country + dayofweek, data = dat, init.theta = 3.763576898, \n    link = log)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.75583    0.19669   8.927  &lt; 2e-16 ***\nage_grp1      0.30090    0.03321   9.060  &lt; 2e-16 ***\nage_grp2      0.45196    0.03238  13.956  &lt; 2e-16 ***\nage_grp3      0.44934    0.03271  13.738  &lt; 2e-16 ***\nage_grp4      0.31340    0.03190   9.824  &lt; 2e-16 ***\nage_grp5      0.31068    0.03213   9.668  &lt; 2e-16 ***\nage_grp6      0.27568    0.03146   8.763  &lt; 2e-16 ***\nage_grp7      0.21717    0.03260   6.661 2.72e-11 ***\nage_grp8      0.03470    0.03617   0.959 0.337387    \nage_grp9     -0.19923    0.04890  -4.074 4.62e-05 ***\ngenderF      -0.01755    0.19240  -0.091 0.927340    \ngenderM      -0.02768    0.19241  -0.144 0.885592    \nhh_size_grp2  0.09645    0.02827   3.412 0.000646 ***\nhh_size_grp3  0.11854    0.02964   3.999 6.36e-05 ***\nhh_size_grp4  0.22913    0.02979   7.692 1.45e-14 ***\nhh_size_grp5  0.30497    0.03213   9.492  &lt; 2e-16 ***\ncountryDE    -0.31802    0.02839 -11.202  &lt; 2e-16 ***\ncountryFI    -0.01234    0.02921  -0.423 0.672627    \ncountryGB     0.02936    0.02918   1.006 0.314328    \ncountryIT     0.43331    0.02960  14.637  &lt; 2e-16 ***\ncountryLU     0.28600    0.02862   9.994  &lt; 2e-16 ***\ncountryNL     0.20892    0.04306   4.851 1.23e-06 ***\ncountryPL     0.25610    0.02860   8.956  &lt; 2e-16 ***\ndayofweek1    0.27813    0.02810   9.898  &lt; 2e-16 ***\ndayofweek2    0.32401    0.02758  11.747  &lt; 2e-16 ***\ndayofweek3    0.31329    0.02812  11.140  &lt; 2e-16 ***\ndayofweek4    0.31717    0.02774  11.434  &lt; 2e-16 ***\ndayofweek5    0.34216    0.02745  12.465  &lt; 2e-16 ***\ndayofweek6    0.14626    0.02887   5.066 4.07e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.7636) family taken to be 1)\n\n    Null deviance: 9794.6  on 7068  degrees of freedom\nResidual deviance: 7372.6  on 7040  degrees of freedom\nAIC: 46396\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.7636 \n          Std. Err.:  0.0843 \n\n 2 x log-likelihood:  -46336.2000 \n\nexp(m5$coefficients)\n\n (Intercept)     age_grp1     age_grp2     age_grp3     age_grp4     age_grp5 \n   5.7882277    1.3510687    1.5713835    1.5672725    1.3680647    1.3643573 \n    age_grp6     age_grp7     age_grp8     age_grp9      genderF      genderM \n   1.3174272    1.2425566    1.0353045    0.8193574    0.9826079    0.9726948 \nhh_size_grp2 hh_size_grp3 hh_size_grp4 hh_size_grp5    countryDE    countryFI \n   1.1012549    1.1258570    1.2575100    1.3565872    0.7275872    0.9877342 \n   countryGB    countryIT    countryLU    countryNL    countryPL   dayofweek1 \n   1.0297905    1.5423491    1.3310865    1.2323420    1.2918884    1.3206527 \n  dayofweek2   dayofweek3   dayofweek4   dayofweek5   dayofweek6 \n   1.3826643    1.3679193    1.3732405    1.4079890    1.1575004"
  },
  {
    "objectID": "posts/apartment-data-api/index.html",
    "href": "posts/apartment-data-api/index.html",
    "title": "Apartment transactions in Korea via API provided by the Ministry of Land, Infrastructure, and Transport",
    "section": "",
    "text": "Data preparation\n\nlibrary(XML)\nlibrary(RCurl)\nlibrary(dplyr)\n\n# service_key &lt;- readRDS(\"data/apartment_key_datagokr.rds\")\nservice_key &lt;- readRDS(\"apartment_key_datagokr.rds\")\ndatlist &lt;- vector(\"list\", 12)\n\n# combine the data in 2022\nfor (m in 1:12){\n  if (m &lt; 10) {\n    dt &lt;- paste0(\"20220\", m)\n  } else {\n    dt &lt;- paste0(\"2022\", m)\n  } \n  uri &lt;- paste0(\"http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev?LAWD_CD=11110&DEAL_YMD=\", dt, \"&serviceKey=\", service_key)\n\n  xml_doc &lt;- xmlTreeParse(uri, useInternalNodes = TRUE, encoding = \"UTF-8\")\n  root_node &lt;- xmlRoot(xml_doc)\n  xml_data &lt;- xmlToDataFrame(nodes = getNodeSet(root_node, '//item'))\n\n  datlist[[m]] &lt;- xml_data\n}\n\nd &lt;- do.call('rbind', datlist)\n\nI will plot the price\n\n# million won\nd$price &lt;- as.numeric(gsub('\\\\,', \"\", d$거래금액)) / 100\nd$area_sq_meter &lt;- as.numeric(d$전용면적) # q\nd$area_category &lt;- NA\n\nfor(i in 1:nrow(d)) {\n  ar &lt;- d$area_sq_meter[i]\n  if(ar &lt; 50){\n    d$area_category[i] &lt;- \"&lt;50\"\n  }\n  else if(ar &gt;= 50 & ar &lt; 80) {\n    d$area_category[i] &lt;- \"50-80\"\n  }\n  else if(ar &gt;= 80 & ar &lt; 100) {\n    d$area_category[i] &lt;- \"80-100\"\n  }\n  else if(ar &gt;= 100) {\n    d$area_category[i] &lt;- \"&gt;100\"\n  }\n}\n\nd$area_category &lt;- factor(d$area_category, levels=c(\"&lt;50\", \"50-80\", \"80-100\",\"&gt;100\"))\n                             \nd$levels &lt;- as.numeric(d$층)\n\nlibrary(ggplot2)\n\nd |&gt; as.data.frame() |&gt; \n  ggplot()+\n  geom_point(aes(area_category, price, color=levels)) +\n  labs(x=parse(text=paste0(\"Area~(m^2)\")), y=\"Price (million won)\", color=parse(text=paste0(\"Levels\")))+\n  theme_bw()+\n  ggtitle(\"Apartment price in Jongno-gu, Seoul, 2022\") \n\n\n\n# ggsave(\"apt_price.png\", width=3.4*1.5, height=2.7*1.5, units=\"in\")"
  },
  {
    "objectID": "posts/rootsolve_optimize/index.html",
    "href": "posts/rootsolve_optimize/index.html",
    "title": "Final epidemic size: uniroot vs. optimize",
    "section": "",
    "text": "Final size of an epidemic\nMiller 2012 shows that the final size of an epidmic for a well-mixed population can be derived in the following way. We divide the population into susceptible, infected, and recovered fractions: \\(S(t), I(t), and R(t)\\) respectively. Assuming a large population, constant transmission and recovery rates, and mass action mixing, we have\n\\[\\dot{S}= -\\beta IS, ~\\dot{I}=\\beta IS -\\gamma I, ~\\dot{R}=\\gamma I\\] We can remove \\(\\dot{R}\\) since \\(S+I+R=1\\). From the equation, we can have the following relationship.\n\\[\\frac{dS}{dI}= -1 + \\frac{\\gamma}{\\beta S}\\] Solving this equation gives the following: \\[ I(t) = -S(t) + \\frac{\\gamma}{\\beta} \\text{ln} S(t) + C\\]\nWe can find \\(C=1\\) using the initial conditions (\\(I\\rightarrow 0, S\\rightarrow 0\\)). Then, using \\(I(\\infty)=0\\) gives the following relationship\n\\[S(\\infty) = 1 − \\text{exp}\\left[-R_0\\left(1-S(\\infty)\\right)\\right]\\] Using the \\(R(\\infty)=1-S(\\infty)\\), we can get the following equation for the final size of an epidemic, \\(R(\\infty)\\):\n\\[R(\\infty) = 1 − \\text{exp}\\left[-R_0R(\\infty)\\right]\\] Let’s use the above relationship to compute the final epidemic size nuerically\n\nfinal_size &lt;- function(R, R0){\n  R - 1 + exp(-R0*R)\n}\n# lower bound set at 0.1 to avoid R=0, which is also a solution\nuniroot(final_size, interval=c(0.1,1), R0=2)$root\n\n[1] 0.796811\n\ndf &lt;- data.frame(R0vec = c(1.1, seq(1.2, 4, by=0.1))) # as  \ndf$sizevec = sapply(df$R0vec, function(x) uniroot(final_size, interval=c(0.1,1), R0=x)$root)\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(df, aes(R0vec, sizevec)) +\n  geom_line(linetype=\"dashed\")+\n  geom_point()+\n  labs(x=parse(text=\"R[0]\"), y=\"Final epidemic size\")\n\n\n\n# ggsave(\"epidemicsize_R.png\", p, units=\"in\", width=3.4, height=2.7)\n\nInstead of uniroot, optimize function can be used to find the solution for the above equation. However, optimize gives the correct answer when the function was squared.\n\noptimize(final_size, interval=c(0.1,1), R0=2)\n\n$minimum\n[1] 0.3465758\n\n$objective\n[1] -0.1534264\n\nfinal_size_sq &lt;- function(R, R0){\n  (R - 1 + exp(-R0*R))^2\n}\noptimize(final_size_sq, interval=c(0.1,1), R0=2)\n\n$minimum\n[1] 0.7968155\n\n$objective\n[1] 3.933157e-12"
  },
  {
    "objectID": "posts/branching_process/index.html",
    "href": "posts/branching_process/index.html",
    "title": "Branching process model",
    "section": "",
    "text": "Branching process model\nIn the branching process model, the number of secondary infections is realized as a random number (e.g., Poission or Negative binomial).\n\nset.seed(42)\nR0_mean &lt;- 2\npopsize = 1000 # population size for each iteration\nnrun &lt;- 1000 # number of iterations to compute the mean\noutbreaks &lt;- rep(NA, nrun) # to store outbreak size for each iteration\ninit_inf &lt;- 1 # initially infected people\n\nfor (r in seq_len(nrun)) {\n  pop &lt;- data.frame(id=1:popsize)\n  pop$status &lt;- \"S\"\n  pop$status[1:init_inf] &lt;- \"I\"\n  nS &lt;- sum(pop$status == \"S\")\n  nI &lt;- sum(pop$status == \"I\")\n  N &lt;- nrow(pop)\n  cnt &lt;- init_inf + 1 # infecteds are placed from the first position\n  while (nI &gt; 0 & nS &gt; 0) {\n    row &lt;- which(pop$status == \"I\")\n    nI &lt;- length(row)\n    for (i in seq_len(nI)) {\n      pop$status[row[i]] &lt;- \"R\"\n      offspring &lt;- rpois(1, lambda=R0_mean*nS/N)\n      nS = nS - offspring\n      for (k in seq_len(offspring)) {\n        pop$status[cnt] &lt;- \"I\" \n        cnt &lt;- cnt + 1\n      }\n    }\n  }\n  outbreaks[r] = popsize - sum(pop$status == \"S\")\n}\n\nhist(outbreaks) # minor and major outbreaks\n\n\n\nsum(outbreaks&gt;200)/nrun # freq of major outbreaks\n\n[1] 0.792\n\nmean(outbreaks[outbreaks&gt;200])/popsize # outbreak size of the only major outbreaks\n\n[1] 0.7966742\n\nmax(outbreaks) # maximum outbreak size\n\n[1] 853\n\n\n\n\nFinal epidemic size\nTo make sure that my branching process model makes sense, let’s compare the final size of an epidemic. As shown in the previous post, for the \\(SIR\\) model in a well-mixed population, the final epidemic size, \\(R(\\infty)\\) is given as follows: \\[R(\\infty) = 1 − \\text{exp}\\left[-R_0R(\\infty)\\right]\\]\n\n# final size of an epidemic, R\nfinal_size &lt;- function(R, R0){\n  R - 1 + exp(-R0*R)\n}\n# lower bound set at a positive number to avoid R=0, which is also a solution\nuniroot(final_size, interval=c(1e-3,1), R0=R0_mean)$root\n\n[1] 0.7968115\n\n\n\n\nNegative binomial distribution\nWhat would happen if I allow the negative binomial distribution for the offspring\n\nset.seed(42)\nR0_mean &lt;- 2\nR0_size &lt;- 0.2 # loosely based on the estimate for Ebola (see Kucharski et al. 2016 https://wwwnc.cdc.gov/eid/article/22/1/15-1410_article)\npopsize = 1000 # population size for each iteration\nnrun &lt;- 1000 # number of iterations to compute the mean\noutbreaks &lt;- rep(NA, nrun) # to store outbreak size for each iteration\ninit_inf &lt;- 1 # initially infected people\n\nfor (r in seq_len(nrun)) {\n  pop &lt;- data.frame(id=1:popsize)\n  pop$status &lt;- \"S\"\n  pop$status[1:init_inf] &lt;- \"I\"\n  nS &lt;- sum(pop$status == \"S\")\n  nI &lt;- sum(pop$status == \"I\")\n  N &lt;- nrow(pop)\n  cnt &lt;- init_inf + 1 # infecteds are placed from the first position\n  while (nI &gt; 0 & nS &gt; 0) {\n    row &lt;- which(pop$status == \"I\")\n    nI &lt;- length(row)\n    for (i in seq_len(nI)) {\n      pop$status[row[i]] &lt;- \"R\"\n      offspring &lt;- rnbinom(1, mu=R0_mean*nS/N, size=R0_size)\n      nS = nS - offspring\n      for (k in seq_len(offspring)) {\n        pop$status[cnt] &lt;- \"I\" \n        cnt &lt;- cnt + 1\n      }\n    }\n  }\n  outbreaks[r] = popsize - sum(pop$status == \"S\")\n}\n\nhist(outbreaks) # minor and major outbreaks\nsum(outbreaks&gt;200)/nrun # freq of major outbreaks\nmean(outbreaks[outbreaks&gt;200])/popsize # only major outbreaks\nmax(outbreaks) # maximum outbreak size"
  },
  {
    "objectID": "posts/branching_process2/index.html",
    "href": "posts/branching_process2/index.html",
    "title": "Branching process model 2",
    "section": "",
    "text": "Branching process model\nIn the branching process model, the number of secondary infections is realized as a random number (e.g., Poission or Negative binomial).\n\nR0_mean &lt;- 2\npopsize = 1000 # population size for each iteration\nnrun &lt;- 1000 # number of iterations to compute the mean\noutbreaks &lt;- vector(\"list\", nrun) # to store outbreak size for each iteration\ninit_inf &lt;- 1 # initially infected people\n\nfor (r in 1:nrun) {\n  pop &lt;- data.frame(id=1:popsize)\n  pop$status &lt;- \"S\"\n  pop$status[1:init_inf] &lt;- \"I\"\n  pop$run_id &lt;- r\n  pop$time_inf &lt;- NA\n  pop$time_inf[1:init_inf] &lt;- 0\n  nS &lt;- sum(pop$status == \"S\")\n  nI &lt;- sum(pop$status == \"I\")\n  N &lt;- nrow(pop)\n  cnt &lt;- init_inf + 1 # infecteds are placed from the first position\n  while (nI &gt; 0 & nS &gt; 0) {\n    row &lt;- which(pop$status == \"I\")\n    nI &lt;- length(row)\n    for (i in seq_len(nI)) {\n      # cat(\"i =\", i, \"\\n\")\n      pop$status[row[i]] &lt;- \"R\"\n      offspring &lt;- rpois(1, lambda=R0_mean*nS/N)\n      nS = nS - offspring\n      for (k in seq_len(offspring)) {\n        pop$status[cnt] &lt;- \"I\"\n        pop$time_inf[cnt] &lt;- pop$time_inf[row[i]] + \n          rgamma(1, shape=2, rate=1/3)\n        cnt &lt;- cnt + 1\n      }\n    }\n  }\n  outbreaks[[r]] &lt;- pop\n}\n\noutbreak_size &lt;- sapply(outbreaks, function(x) nrow(x) - sum(x$status==\"S\"))\nhist(outbreak_size) # minor and major outbreaks\nmajor_outbreaks = outbreak_size &gt; 200\nmean(outbreak_size[major_outbreaks])/popsize # only major outbreaks\n\n[1] 0.7960679\n\noutbks = purrr::list_rbind(outbreaks)\nlibrary(tidyverse)\n\n\n\noutbks[major_outbreaks,] |&gt; \n  filter(!is.na(time_inf)) |&gt; \n  ggplot()+\n    geom_histogram(aes(x=time_inf))"
  },
  {
    "objectID": "posts/interval-censoring/index.html",
    "href": "posts/interval-censoring/index.html",
    "title": "Estimating serial interval: interval cenoring",
    "section": "",
    "text": "Vanilla maximum likelihood estimation\nSuppose dates of onsets of infectors, \\(t^{A}\\), and infectees, \\(t^{B}\\), are given as specific dates. Then the likelihood function for the serial interval may be written down as follows:\n\\[\\mathcal{L} = \\prod_{i=1}^{n} f(t^{B}_i - t^\n{A}_i)\\] , where \\(f\\) is a probability density function for the serial interval, which we assume follows a Gamma distribution.\n\nset.seed(42)\nn &lt;- 100\nshape_true &lt;- 2.2\nscale_true &lt;- 3.3\n\nonset_infector &lt;- sample(20:30, size=n, replace=TRUE)\nonset_infectee &lt;- onset_infector + rgamma(n, shape=shape_true, scale=scale_true)\nnll &lt;- function(parms, x) -sum(dgamma(x, shape=parms[[1]], scale=parms[[2]], log=TRUE))\nres1 = optim(par=c(1,2), fn=nll, x=onset_infectee - onset_infector, method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\n\n\nMLE with interval censoring\nNow suppose that the dates of onset of the infectors are given as intervals. In this case, the above likelihood function may be modified as follows:\n\\[\\mathcal{L} = \\prod_{i=1}^{n} \\int_{t^{A}_{Li}}^{t^{A}_{Ri}} g(\\tau) f(t^{B}_i-\\tau) ~\\text{d}\\tau\\] , where \\(t^A_L, t^A_R, t^B\\) present the times for lower end and upper end of the interval for the time of onset of the infector, and the onset time of the infectee, respectively. \\(g(x)\\) represents the probability density function for the time of symptom onset of the infector, which we assume follows a uniform distribution.\nThis is a simplified version of doubly interval-censored data analysis, which was discussed by Reich et al 2009. The same concept has recently been applied to estimation of serial interval of CVODI-19 by Nishiura et al. 2020. I will cover the doubly interval-censored data in a future post.\n\nset.seed(42)\nL &lt;- - sample(1:5, n, replace=TRUE)\nR &lt;- - 4*L # this will lead to potentially shorter serial interval\nAL &lt;- onset_infector + L\nAR &lt;- onset_infector + R\n\n# x\nnll_interval_censor &lt;- function(parms, AL, AR, t){\n  -sum(log(dunif(AL, min=AL, max=AR)*(pgamma(t-AL, shape=parms[[1]], scale=parms[[2]]) - pgamma(t-AR, shape=parms[[1]], scale=parms[[2]]))))\n}\n\nres2 = optim(par=c(1,2), fn=nll_interval_censor, AL=AL, AR=AR, t=onset_infectee, method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\nx1 &lt;- rgamma(1e3, shape=res1$par[[1]], scale=res1$par[[2]])\nx2 &lt;- rgamma(1e3, shape=res2$par[[1]], scale=res2$par[[2]])\nsummary(x1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1677  3.7743  6.2386  7.3706 10.0134 36.0535 \n\nsummary(x2)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.05973  2.01266  3.88932  4.85497  6.77875 20.17723 \n\ndf = data.frame(model=rep(c(\"No censoring\", \"Interval censoring\"), each=1e3), val=c(x1,x2))\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\ndf |&gt; ggplot(aes(x=val, fill=model))+\n  geom_density(alpha=0.2) +\n  labs(x=\"value\", y=\"density\")+\n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())"
  },
  {
    "objectID": "posts/double-interval-censoring/index.html",
    "href": "posts/double-interval-censoring/index.html",
    "title": "Estimating serial interval: doubly interval-censored data",
    "section": "",
    "text": "We start simple. Our task is to estimate parameters of a probability density function used to model the serial interval. Suppose dates of onsets of infectors, \\(A\\), and infectees, \\(B\\), are given as specific dates. Then the likelihood function for the serial interval may be written down as follows:\n\\[\\mathcal{L}(X;\\theta) = \\prod_{i=1}^{n} f_{\\theta}(B_i - A_i)\\], where \\(f\\) is the probability density function for the serial interval with the unknown parameters, \\(\\theta\\).\nNow suppose that the dates of symptom onset of the infectors are given as intervals. We can use the following argument also for the case where dates of symptom onset of the infectees are given as intervals. In this case, the above likelihood function may be modified as follows:\n\\[\\mathcal{L}(X;\\theta) = \\prod_{i=1}^{n} \\int_{A^L_i}^{A^R_i} f_{\\theta}(B_i-a) ~\\text{d}a\\] , where \\(A^L, A^R, B\\) present the times for lower end and upper bound on the potential dates of symptom onset of the infector, and the symptom onset time of the infectee, respectively.\nNow suppose that both the dates of onset of the infectors and infectees are given as intervals. This is so called the doubly interval-censored data discussed by Reich et al 2009. The likelihood function may be given as follows:\n\\[\\mathcal{L}(X;\\theta,\\lambda) = \\prod_{i=1}^{n} \\int_{A^L_i}^{A^R_i} \\int_{B^L_i}^{B^R_i} h_{\\lambda}(a) f_{\\theta}(b-a) ~\\text{d}b \\text{d}a\\] , where \\(A^L, A^R, B^L, B^R\\) present the times for left and right boundaires on the possible onset times of the infector, \\(A\\), and the infectee, \\(B\\), respectively. \\(h_{\\lambda}(x)\\) represents the probability density function for the time of symptom onset of the infector, which we assume follows a uniform distribution.\nMore detailed analyses of doubly interval-censored data were discussed by Reich et al 2009. The same concept has recently been applied to estimation of serial interval of CVODI-19 by Nishiura et al. 2020.\nIn the following codes, we create the fake data set and add intervals such that the serial intervals may become shorter.\n\nset.seed(42)\nn &lt;- 100\nshape_true &lt;- 2.2\nscale_true &lt;- 3.3\n\nonset_infector &lt;- sample(20:30, size=n, replace=TRUE)\nonset_infectee &lt;- onset_infector + rgamma(n, shape=shape_true, scale=scale_true)\nnll &lt;- function(parms, x) -sum(dgamma(x, shape=parms[[1]], scale=parms[[2]], log=TRUE))\nres1 = optim(par=c(1,2), fn=nll, x=onset_infectee - onset_infector, method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\n# singly interval-censored data\ntau &lt;- sample(1:5, n, replace=TRUE)\nAL &lt;- onset_infector \nAR &lt;- onset_infector + 2*tau # this will lead to shorter serial interval\n\nnll_single_censor &lt;- function(parms, AL, AR, B){\n  -sum(log(pgamma(B-AL, shape=parms[[1]], scale=parms[[2]]) - pgamma(B-AR, shape=parms[[1]], scale=parms[[2]])))\n}\n\nres2 = optim(par=c(1,2), fn=nll_single_censor, AL=AL, AR=AR, \n             B=onset_infectee, method=\"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\n# doubly interval-censored data\nBL &lt;- onset_infectee - 2*tau # this will lead to even shorter serial interval\nBR &lt;- onset_infectee\n\nnll_double_censor &lt;- function(parms, AL, AR, BL, BR){\n  -sum(log(dunif(AL, min=AL, max=AR)*(pgamma(BR-AL, shape=parms[[1]], scale=parms[[2]]) - pgamma(BL-AR, shape=parms[[1]], scale=parms[[2]]))))\n}\n\nres3 = optim(par=c(1,2), fn=nll_double_censor, AL=AL, AR=AR,\n             BL=BL, BR=BR, method=\"Nelder-Mead\",\n            control=list(maxit=2e4, reltol=1e-15))\n\nx1 &lt;- rgamma(1e3, shape=res1$par[[1]], scale=res1$par[[2]])\nx2 &lt;- rgamma(1e3, shape=res2$par[[1]], scale=res2$par[[2]])\nx3 &lt;- rgamma(1e3, shape=res3$par[[1]], scale=res3$par[[2]])\nsummary(x1)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1677  3.8165  6.4303  7.5411 10.3268 36.0535 \n\nsummary(x2)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.03203  1.37089  3.37988  4.70497  6.48863 38.89572 \n\nsummary(x3)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.00001  0.36330  1.48009  2.92934  4.04038 46.12810 \n\ndf = data.frame(model=rep(c(\"No censoring\", \"Singly interval-censored\",\n                            \"Doubly interval-censored\"), each=1e3),\n                val=c(x1,x2, x3))\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\ndf |&gt; ggplot(aes(x=val, fill=model))+\n  geom_density(alpha=0.2) +\n  labs(x=\"value\", y=\"density\")+\n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())\n\n\n\n# ggsave(\"double_interval_censor.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/right-truncation/index.html",
    "href": "posts/right-truncation/index.html",
    "title": "Estimating serial interval for a growing epidemic",
    "section": "",
    "text": "In this case, the above likelihood function may be modified as follows:\n\\[\\mathcal{L}(X;\\theta) = \\prod_{i=1}^{n} f_{\\theta}(B_i-A_i)\\] , where \\(A^L, A^R, B\\) present the times for lower end and upper bound on the potential dates of symptom onset of the infector, and the symptom onset time of the infectee, respectively.\n\\[f^t_{\\theta}(B_i-A_i) = \\frac{f_{\\theta}(B_i-A_i)}{F(T-A_i)}\\]\n\nset.seed(42)\nn &lt;- 1000\nshape_true &lt;- 2.2\nscale_true &lt;- 3.3\n\ndf &lt;- data.frame(A=sample(0:30, size=n, replace=TRUE))\nsi &lt;- rgamma(n, shape=shape_true, scale=scale_true)\ndf$B &lt;- df$A + si\nmax(df$B)\n\n[1] 57.03661\n\nsummary(df$B)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3967 13.9763 22.4103 22.4031 30.3811 57.0366 \n\nTmax &lt;- 35\nunder_Tmax &lt;- df$B &lt; Tmax\n\nnewdf &lt;- df[under_Tmax,]\n\nnll &lt;- function(parms, A, B) -sum(dgamma(B-A, shape=parms[[1]], scale=parms[[2]], log=TRUE))\nres1 = optim(par=c(1,2), fn=nll, A=newdf$A, B=newdf$B,\n             method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\nres1\n\n$par\n[1] 2.366494 2.735134\n\n$value\n[1] 2389.103\n\n$counts\nfunction gradient \n     101       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\nnll_right_trunc &lt;- function(parms, A, B, Tmax) -sum(log(dgamma(B-A, shape=parms[[1]], scale=parms[[2]])/pgamma(Tmax-A, shape=parms[[1]], scale=parms[[2]])))\n\nres2 = optim(par=c(1,2), \n             fn=nll_right_trunc, \n             A=newdf$A, \n             B=newdf$B,\n             Tmax=Tmax,\n             method = \"Nelder-Mead\",\n             control = list(maxit=2e4, reltol=1e-15))\nres2\n\n$par\n[1] 2.230993 3.230118\n\n$value\n[1] 2317.955\n\n$counts\nfunction gradient \n     115       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\nn &lt;- 1e5\nx1 &lt;- rgamma(n, shape=res1$par[[1]], scale=res1$par[[2]])\nx2 &lt;- rgamma(n, shape=res2$par[[1]], scale=res2$par[[2]])\n\nsummary(x1)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n 0.01066  3.36629  5.58653  6.47582  8.62659 44.32536 \n\nsummary(x2)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0263  3.6574  6.1697  7.2078  9.6218 43.8174 \n\ndf = data.frame(model=rep(c(\"No truncation\", \"Right truncated\"), each=n), val=c(x1,x2))\n\nlibrary(ggplot2)\nextrafont::loadfonts()\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\ndf |&gt; ggplot(aes(x=val, fill=model))+\n  geom_density(alpha=0.2) +\n  labs(x=\"value\", y=\"density\")+\n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())\n\n\n\n# ggsave(\"right_trunc.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/right-truncation2/index.html",
    "href": "posts/right-truncation2/index.html",
    "title": "Estimating a time-to-event distribution from right-truncated data",
    "section": "",
    "text": "Seamen writes: Data on time to an event are said to be right truncated if they come from a set of individuals who have been randomly sampled from a population using a sampling mechanism that selects only individuals who have experienced the event by a given time, called the truncation time.\nThe analysis of right-truncated data requires statistical methods that account for the fact that each of the sampled individuals must have experienced the final event by their truncation time.\nLet’s assume that we are interested in estimating the serial interval and let \\(X\\) and \\(Y\\) denote the times of symptom onset of the infector and the infectee, with \\(0\\leq X \\leq Y\\). Let \\(T=Y-X\\) denote the serial interval. Let \\(f_T^∗(t)\\) and \\(F_T^∗(t)\\) denote, respectively, the probability density (or mass) function of \\(T\\) and the distribution function of \\(T\\). We obtain an i.i.d. sample, \\((x_1, t_1), . . . , (x_n, t_n)\\), from the probability distribution of \\((X, T)\\) given \\(X + T ≤ \\tau\\) for some \\(\\tau &gt; 0\\).\n\\[f_{X, T}(x,t|X+T \\leq \\tau) = \\frac{f_X(x) f^*_T(t)I(x+t \\leq \\tau)}{\\int_0^{\\tau} f_X(x') F^*_T(\\tau-x')\\text{d}x'}\\] , where \\(f_X (x)\\) denotes the conditional probability density (or mass) function of \\(X\\) given \\(X \\leq \\tau\\), and \\(I(\\cdot)\\) denotes the indicator function. If \\(X\\) and \\(T\\) are discrete, we shall assume, without loss of generality, that \\(\\tau\\) is an integer. We should like to estimate \\(F^∗_T(t)\\)\nIf we assume no truncation (i.e., \\(F^*_T(\\tau-x)=1~ \\forall x\\))\n\nDuring an initial period of an epidemic\nSeamen provides two approaches to formulating a likelihood that can be applied during the initial period of an epidemic. The first approach is: \\[f_X (x;r) =  \\frac{\\text{exp}(rx)}{\\int_0^{\\tau}\\text{exp}(rs)ds }=\\frac{r\\text{exp}(rx)}{\\text{exp}(r\\tau) - 1 } \\propto \\text{exp}(rx)\\], and \\(T \\sim \\text{Gamma}(\\theta_1, \\theta_2)\\).\n\n\nExperiment\nLet’s first create samples where infectees (\\(X\\)) are created through a non-homogeneous Poisson process where rate, \\(h\\), is modeled as an exponential growth with a rate, \\(r\\), (i.e., \\(h=\\text{exp}(rt)\\).\n\nset.seed(42)\nn &lt;- 1000\ntmax &lt;- 30 # first events that happened only before time 30\nr &lt;- 0.14 # growth rate\nX &lt;- vector(\"double\", n)\ni &lt;- 1\nct &lt;- 0\n# generate sample through a nonhomogeneous Poisson process\nwhile (ct &lt; tmax) {\n  t &lt;- rexp(1, rate=exp(r*ct))\n  ct &lt;- ct + t\n  X[i] &lt;- ct\n  i &lt;- i+1\n}\nX &lt;- X[X &gt; 0]\n\n# parameters for the serial interval\nshape_true &lt;- 2.2\nscale_true &lt;- 3.3\n\ndf &lt;- data.frame(X=X)\nsi &lt;- rgamma(length(X), shape=shape_true, scale=scale_true)\ndf$Y &lt;- df$X + si\n\ntmax &lt;- 33 # truncation time\nunder_tmax &lt;- df$Y &lt; tmax \nnewdf &lt;- df[under_tmax,]\n\nnll &lt;- function(parms, X, Y) -sum(dgamma(Y-X, shape=parms[[1]], scale=parms[[2]], log=TRUE))\nres1 = optim(par=c(1,2), fn=nll, X=newdf$X, Y=newdf$Y,\n             method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\n# \nnll_right_trunc &lt;- function(parms, X, Y, tmax) -sum(log(dgamma(Y-X, shape=parms[[1]], scale=parms[[2]])/pgamma(tmax-X, shape=parms[[1]], scale=parms[[2]])))\n\n# the following would not work. why?\n# nll_right_trunc &lt;- function(parms, X, Y, tmax) -sum(log(dgamma(Y-X, shape=parms[[1]], scale=parms[[2]])/pgamma(tmax, shape=parms[[1]], scale=parms[[2]])))\n\nres2 = optim(par=c(1,2), \n             fn=nll_right_trunc, \n             X=newdf$X, \n             Y=newdf$Y,\n             tmax=tmax,\n             method = \"Nelder-Mead\",\n             control = list(maxit=2e4, reltol=1e-15))\n\n\n\nExponential growth for \\(X\\)\n\nnumerator_func &lt;- function(x, y, parms){\n  exp(r*x)*dgamma(y-x, shape=parms[[1]], scale=parms[[2]])\n}\n\n# using the full probability density function\n# numerator_func &lt;- function(x, y, parms){\n#   exp(r*x)/(exp(r*tmax)-1)*dgamma(y-x, shape=parms[[1]], scale=parms[[2]])\n# }\n\n# uniform distribution - same as the vanilla truncation assumption\n# numerator_func &lt;- function(x, y, parms){\n#   dgamma(y-x, shape=parms[[1]], scale=parms[[2]])\n# }\n\n# NB: x is not used in this formuation but is included as params to make consistent w/ other alternative formulations\ndenominator_func &lt;- function(t, x, parms, tmax) {\n  exp(r*t)*pgamma(tmax-t, shape=parms[[1]], scale=parms[[2]])\n}\n\n# the following would not give the correct answer. why?\n# denominator_func &lt;- function(t, x, parms, tmax) {\n#   exp(r*t)*pgamma(tmax-x-t, shape=parms[[1]], scale=parms[[2]])\n# }\n\n# same as the vanialla truncation model\n# denominator_func &lt;- function(t, x, parms, tmax) {\n#   dgamma(tmax-x-t, shape=parms[[1]], scale=parms[[2]])\n# }\n\n# single likelihood\nll_right_trunc_exp_growth &lt;- function(parms,x,y,tmax){\n  log(numerator_func(x=x, y=y, parms=parms)) - log(integrate(denominator_func,lower=0, upper=tmax, x=x, parms=parms, tmax=tmax)$value)\n}\n\n# sum of negative log likelihoods\nnll_right_trunc_exp_growth &lt;- function(parms, X, Y, tmax){\n  sll &lt;- 0\n  for(i in seq_along(X)) {\n    sll &lt;- sll + ll_right_trunc_exp_growth(parms=parms,x=X[i],y=Y[i],tmax=tmax)\n  }\n  return(-sll)\n}\n\nres3 = optim(par=c(1,2), \n             fn=nll_right_trunc_exp_growth, \n             X=newdf$X, \n             Y=newdf$Y,\n             tmax=tmax,\n             method = \"Nelder-Mead\",\n             control = list(maxit=2e4, reltol=1e-15))\n\nparmdf &lt;- data.frame(true = c(shape_true, scale_true, shape_true*scale_true))\nparmdf$no_trunc &lt;- c(res1$par, prod(res1$par))\nparmdf$trunc &lt;- c(res2$par, prod(res2$par))\nparmdf$trunc_exp_growth &lt;- c(res3$par, prod(res3$par))\n\nparmdf\n\n  true no_trunc    trunc trunc_exp_growth\n1 2.20 2.115872 1.932797         2.100084\n2 3.30 2.273549 3.684688         3.427339\n3 7.26 4.810538 7.121754         7.197701\n\n\nParameter estimates based on the methods that account for right truncation and exponential growth appear to match better with the true values than the those based on the method that accounts only for right truncation.\n\n\nPlot\nLet’s plot the distribution\n\nn &lt;- 1e5\nx0 &lt;- rgamma(n, shape=shape_true, scale=scale_true)\nx1 &lt;- rgamma(n, shape=res1$par[[1]], scale=res1$par[[2]])\nx2 &lt;- rgamma(n, shape=res2$par[[1]], scale=res2$par[[2]])\nx3 &lt;- rgamma(n, shape=res3$par[[1]], scale=res3$par[[2]])\n\ndf = data.frame(model=rep(c(\"True\",\"No truncation\", \"Right truncated\", \"Right truncated, exp growth\"), each=n), val=c(x0,x1,x2,x3))\n\nlibrary(ggplot2)\nextrafont::loadfonts()\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\ndf |&gt; ggplot(aes(x=val, fill=model))+\n  geom_density(alpha=0.2) +\n  labs(x=\"value\", y=\"density\")+\n  theme(legend.position = \"bottom\", \n        legend.title = element_blank())\n\n\n\n# ggsave(\"right_trunc2.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/truncation-in-stan/index.html",
    "href": "posts/truncation-in-stan/index.html",
    "title": "Estimating a time-to-event distribution in Stan",
    "section": "",
    "text": "Stan instead of optim\nAs in the previous post, let’s create a sample through a non-homogeneous process for the infection events and a Gamma distribution for the serial (or generation) interval.\n\nset.seed(42)\nn &lt;- 1000\ntmax &lt;- 30 # maximum time of first event\nr &lt;- 0.14 # growth rate\nX &lt;- vector(\"double\", n)\ni &lt;- 1\nct &lt;- 0\n# generate sample through a nonhomogeneous Poisson process\nwhile (ct &lt; tmax) {\n  t &lt;- rexp(1, rate=exp(r*ct))\n  ct &lt;- ct + t\n  X[i] &lt;- ct\n  i &lt;- i+1\n}\nX &lt;- X[X &gt; 0]\n\n# parameters for the serial interval\nshape_true &lt;- 2.2\nscale_true &lt;- 3.3\n\ndf &lt;- data.frame(X=X)\nsi &lt;- rgamma(length(X), shape=shape_true, scale=scale_true)\ndf$Y &lt;- df$X + si\n\ntau &lt;- 33 # truncation time\nunder_tau &lt;- df$Y &lt; tau \nnewdf &lt;- df[under_tau,]\n\nnumerator_func &lt;- function(x, y, parms){\n  exp(r*x)*dgamma(y-x, shape=parms[[1]], scale=parms[[2]])\n}\n\ndenominator_func &lt;- function(t, parms, tmax) {\n  exp(r*t)*pgamma(tmax-t, shape=parms[[1]], scale=parms[[2]])\n}\n\n# single likelihood\nll_right_trunc_exp_growth &lt;- function(parms,x,y,tmax){\n  log(numerator_func(x=x, y=y, parms=parms)) - log(integrate(denominator_func,lower=0, upper=tmax, parms=parms, tmax=tmax)$value)\n}\n\n# sum of negative log likelihoods\nnll_right_trunc_exp_growth &lt;- function(parms, X, Y, tmax){\n  sll &lt;- 0\n  for(i in seq_along(X)) {\n    sll &lt;- sll + ll_right_trunc_exp_growth(parms=parms,x=X[i],y=Y[i],tmax=tmax)\n  }\n  return(-sll)\n}\n\nres_optim = optim(par=c(1,2), \n             fn=nll_right_trunc_exp_growth, \n             X=newdf$X, \n             Y=newdf$Y,\n             tmax=tmax,\n             method = \"Nelder-Mead\",\n             control = list(maxit=2e4, reltol=1e-15))\n\n\nStan program\nGamma distribution accounting for truncation\n\nstan_code &lt;- \"\ndata {\n    int&lt;lower = 0&gt; N; // number of records\n    vector&lt;lower = 0&gt;[N] X;\n    vector&lt;lower = 0&gt;[N] Y;\n    real tau;\n}\n\nparameters {\n    real shape;\n    real scale;\n}\n\nmodel {\n    shape ~ exponential(0.1);\n    scale ~ exponential(0.1);\n    target += gamma_lpdf(Y - X | shape, 1/scale) - gamma_lcdf(tau-X | shape, 1/scale);\n}\"    \n\nGamma distribution accounting for truncation and exponential growth of infections\n\nstan_code &lt;- \"\nfunctions {\n  real denominator_density(real x,\n                           real xc,                \n                           array[] real theta,     \n                           array[] real x_r,                        \n                           array[] int x_i){\n    real shape = theta[1];\n    real scale = theta[2];\n  \n    return exp(0.14 * x) * gamma_cdf(33 - x, shape, 1/scale);\n  }\n}\ndata {\n    int&lt;lower = 0&gt; N; // number of records\n    vector&lt;lower = 0&gt;[N] X;\n    vector&lt;lower = 0&gt;[N] Y;\n    real tau;\n    real r;\n}\n\ntransformed data{     \n  array[0] real x_r;\n  array[0] int x_i;  \n} \n\nparameters {\n    real shape;\n    real scale;\n}\n\ntransformed parameters {\n  vector[N] log_exp_r;\n  for (n in 1:N)\n    log_exp_r[n] = log(exp(r*X[n]));\n}\n\nmodel {\n    shape ~ exponential(0.1);\n    scale ~ exponential(0.1);\n    \n    for (i in 1:N)\n      target += log(exp(r*X[i])) + gamma_lpdf(Y[i] - X[i] | shape, 1/scale) -                          log(integrate_1d(denominator_density, 0, tau,\n                           {shape, scale}, x_r, x_i, 1e-2));\n     \n}\"\n\n\n\nCompile and sample\nintegrate_1d(denominator_density, 0, tau, {shape, scale}, x_r, x_i, 1e-3) cause errors. Four of the two samplers generated samples if the rel_tol is increased to 1e-2 for seed=42.\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\nmod &lt;- stan_model(model_code=stan_code, verbose=TRUE)\ndata &lt;- list(N=nrow(newdf), X=newdf$X, Y=newdf$Y, tau=tau, r=r)\n# smp &lt;- sampling(object=mod, data=data, seed=33, chains=4, iter=2000)\n# saveRDS(smp, \"stan_trunc_smp_20231124.rds\")\n\nLet’s explore the posterior distribution.\n\nsmp &lt;- readRDS(\"stan_trunc_smp_20231124.rds\")\ndf &lt;- as.data.frame(smp)\npr &lt;- c(0.5,0.025,0.975)\nd &lt;- as.data.frame(t(apply(df[,c(\"shape\", \"scale\")],\n                           2, quantile, probs=pr)))\n\nd$name &lt;- c(\"shape\", \"scale\")\nd$true &lt;- c(shape_true, scale_true)\nd$optim &lt;- res_optim$par\nd\n\n           50%     2.5%    97.5%  name true    optim\nshape 2.085903 1.760453 2.448298 shape  2.2 2.090817\nscale 3.484215 2.680270 4.770225 scale  3.3 3.482427\n\n\nLet’s plot the results.\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\nggplot(d)+ \n  geom_errorbar(aes(x=name, ymin=`2.5%`, ymax=`97.5%`), width=0.0)+\n  geom_point(aes(x=name, y=`50%`, color=\"Stan\"), size=3)+\n  geom_point(aes(x=name, y=true, col=\"True value\"), size=3)+\n  geom_point(aes(x=name, y=optim, col=\"Optim\"), size=3)+\n  scale_color_manual(values=c(\"Stan\"=\"black\",\n                              \"True value\"=\"firebrick\", \"Optim\"=\"steelblue\"))+\n  labs(x=\"\", y=\"\", title=\"Median estimates with 95% CrI\")+\n  theme(legend.position=\"bottom\", legend.title=element_blank())+\n  scale_x_discrete(breaks=c(\"shape\",\"scale\"),\n                   labels=c(expression(theta[1]),expression(theta[2])))+\n  coord_flip()\n\n\n\n# ggsave(\"right_trunc_stan.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)\n\n\nd &lt;- df[, c(\"shape\",\"scale\")]\ndlong &lt;- tidyr::pivot_longer(d, cols=c(\"shape\",\"scale\"),\n                             names_to=\"param\")        \ndlong$param &lt;- as.factor(dlong$param)\nlibrary(dplyr)\nggplot(dlong)+ \n  geom_histogram(aes(x=value))+\n  facet_wrap(~param, nrow=1, scales = \"free_x\")+\n  geom_vline(data=filter(dlong, param ==\"shape\"), aes(xintercept=shape_true), color=\"firebrick\", linewidth=1.2) +\n  geom_vline(data=filter(dlong, param ==\"scale\"), aes(xintercept=scale_true), color=\"firebrick\", linewidth=1.2)"
  },
  {
    "objectID": "posts/importance-sampling/index.html",
    "href": "posts/importance-sampling/index.html",
    "title": "Importance sampling",
    "section": "",
    "text": "Importance sampling\nImportance sampling is a Monte Carlo method for evaluating properties of a particular distribution, while only having samples generated from a different distribution than the distribution of interest.\nSuppose we want to compute the expectation of an arbitrary function \\(f\\) of a random variable \\(Y\\), which is distributed according to the distribution \\(p\\): \\[ E_p[f(Y)] := \\int f(y) p(y) dy\\]\nIn case the integration becomes difficult, we can use a Monte Carlo method. \\[ E^{MC} := \\frac{1}{N} \\sum_{i=1}^N f(y^{(i)})\\] By the law of large numbers, this estimate will almost surely converge to the true value as the number \\(N\\) of particles (i.e., sampled values) increases.\nAlthough this appears straightforward, sampling from the target distribution, \\(p\\) is not always possible or efficient. Importance sampling bypasses this difficulty by sampling particles from an arbitrary “instrumental distribution” \\(q\\) and weighting the particles by accounting for they were sampled from \\(q\\) but not from \\(p\\).\nImportance sampling fundamental identity\n\\[ E_p[f(Y)] := \\int \\frac{f(y)}{q(y)} q(y) p(y) dy = E_q[w(Y) f(Y)]\\] where we define the importance weight \\(w(y) = \\frac{p(y)}{q(y)}\\)\nLet’s see an example in which we create Gamma-distributed sample from the exponentially distributed sample.\n\nset.seed(42)\nr &lt;- 0.01 # low rate for a wide coverage\nx &lt;- rexp(1e4, rate=r)\n# dgamma(x, shape=3, rate=1) target distribution\nwt = dgamma(x, shape=3, rate=1) / dexp(x, rate=r)\n\nplot(x, wt)\n\n\n\nW = wt/sum(wt)\n\nids &lt;- sample(1:length(x), prob=W, replace=T)\nnewx &lt;- x[ids]\nd &lt;- data.frame(wt=wt, x=x, W=W)\nd &lt;- d[order(x),]\ny = rgamma(length(x), shape=3, rate=1)\n\nd &lt;- data.frame(mean_exp=mean(x),\n                sum_wt_x=sum(W*x),\n                mean_important=mean(newx),\n                true_mean = mean(y))\n\nd\n\n  mean_exp sum_wt_x mean_important true_mean\n1 100.6511  3.01221       3.046269  3.035831\n\n\n\ndf &lt;- data.frame(name=rep(c(\"Target dist\",\"Intrumental dist\",\"Importance sample\"), each=1e4), \n                 value=c(y,x,newx))\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\nggplot(df)+\n  geom_histogram(aes(x=value))+\n  facet_wrap(~name, nrow=1, scales=\"free_x\")\n\n\n\n# ggsave(\"importance_sampling.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/seir-models/index.html",
    "href": "posts/seir-models/index.html",
    "title": "SEIR model",
    "section": "",
    "text": "Susceptible-Exposed-Infective-Recovered (SEIR) 모형\nSEIR 모형은 잠복기가 어느 정도 긴 감염병 (예를 들어 코로나19)의 전파를 모형하는 데 사용한다. 이번 포스트에서는 SEIR 모형을 만드는 방법을 알아본다. 결정론적 (deterministic) 그리고 확률론적 (stochastic) 방법으로 SEIR 모형을 R언어로 만들어 본다.\n\nDeterministic model\n결정론적 모형은 주로 미분식 (differential equation)을 이용하여 구현한다. \\[\\begin{equation} \\begin{split}  \\frac{dS}{dt} &= - \\beta S\\frac{I}{N}\\\\ \\frac{dE}{dt} &= \\beta S\\frac{I}{N} - \\epsilon E\\\\ \\frac{dI}{dt} &= \\epsilon E - \\gamma I\\\\ \\frac{dR}{dt} &= \\gamma I \\end{split} \\end{equation}\\]\n\nseir_ode &lt;- function(t, y, params) {\n  # state variables \n  S &lt;- y[1]; E &lt;- y[2]; I &lt;- y[3]; R &lt;- y[4];\n  beta &lt;- params[[\"beta\"]] # beta = transmission rate\n  epsilon &lt;- params[[\"epsilon\"]] # 1/epsilon = latent period\n  gamma &lt;- params[[\"gamma\"]] # 1/gamma = duration of infectiousness\n  \n  N &lt;- S + E + I + R # total population size\n  muSE &lt;- beta * S * I / N # rate from S to E\n  muEI &lt;- epsilon * E # rate from E to I, i.e., 1/epsilon = latent period\n  muIR &lt;- gamma * I # rate from I to R\n  \n  dS &lt;- - muSE # rate of change for S\n  dE &lt;- muSE - muEI # rate of change for E\n  dI &lt;- muEI - muIR # rate of change for I\n  dR &lt;- muIR # rate of change for R\n  \n  return(list(c(dS, dE, dI, dR))) # return as a list to use deSolve package\n}\n\n미분식을 적분하여 SEIR 변수들의 시간에 따른 추이를 살펴보자. 적분은 deSolve 패키지의 ode 함수를 이용한다.\n\nI0 &lt;- 0.01 # initially infected people\ny0 &lt;- c(S = 1 - I0, E = 0, I = I0, R = 0) # initial values for state variables\nparams &lt;- list() # parameter input for the SIR model\nparams$epsilon &lt;- 0.5\nparams$gamma &lt;- 0.2\nparams$beta &lt;- 0.4  \ntend &lt;- 100 # simulation end time 50 days\ntimes &lt;- seq(0, tend, by = 1) # daily output for 150 days\n\n# ODE integration using the deSolve package\nlibrary(deSolve)\nlibrary(dplyr) # to use %&gt;%\node(y=y0, times=times, func=seir_ode, parms=params) %&gt;%\n  as.data.frame() -&gt; out\nlibrary(tidyr) # turn the data into a long format for easier plot\noutlong &lt;- out %&gt;% pivot_longer(cols=2:5, names_to = \"State\")\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\n\nggplot(outlong, aes(x=time, y=value, color=State)) +\n  geom_line(linewidth = 1.2) +\n  labs(x = 'Time (day)', y = 'Proportion')+\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") \n\n\n\n\n\n\n확률론적 모형\n두 가지 방식으로 확률론적 모형을 구현하여 본다. 첫번째는 \\(\\tau\\)-leaping 방법과 유사하나 푸아송 분포 대신 binomial 분포를 사용한다. 푸아송 분포와 달리 상한선이 정해지므로 각 상태 변수가 음수로 가는 것을 막을 수 있는 잇점이 있다. S에서 E로 단위 시간 \\(\\delta\\) 동안 이동하는 수는 아래와 같이 정해진다. \\[\\begin{equation} \\begin{split}  \\Delta N_{SE} &= \\textrm{Binomial}\\left( S(t), 1-\\textrm{exp}[{-r_{SE}\\delta }]\\right) \\\\ S(t+\\delta) &= S(t) - \\Delta N_{SE}\\ \\end{split} \\end{equation}\\] 비슷한 방법으로 \\(E\\)에서 \\(I\\) 그리고 \\(I\\)에서 \\(R\\)로 변하는 수를 계산하여 아래와 같이 구현한다.\n\nseir_stoch_step &lt;- function (y, params, delta) {\n  \n  beta &lt;- params[[\"beta\"]]\n  epsilon &lt;- params[[\"epsilon\"]]\n  gamma &lt;- params[[\"gamma\"]] \n\n  S &lt;- y[\"S\"]; E &lt;- y[\"E\"]; I &lt;- y[\"I\"]; R &lt;- y[\"R\"];\n\n  N &lt;- S + E + I + R\n  rSE &lt;- beta * I / N\n  rEI &lt;- epsilon\n  rIR &lt;- gamma\n  # number of events over the time step, delta, modeled as binomial random variable     \n  nSE &lt;- rbinom(1, S, 1 - exp(- rSE * delta))\n  nEI &lt;- rbinom(1, E, 1 - exp(- rEI * delta))\n  nIR &lt;- rbinom(1, I, 1 - exp(- rIR * delta))\n\n  dSdt &lt;- - nSE\n  dEdt &lt;- nSE - nEI\n  dIdt &lt;- nEI - nIR\n  dRdt &lt;- nIR\n  dCEdt &lt;- nSE\n  dCIdt &lt;- nEI\n\n return (list(c(dSdt, dEdt, dIdt, dRdt)))\n}\n\n위 함수는 한 번의\\(\\delta\\)동안 변화를 출력하기 때문에 원하는 기간 동안 연속해서 계산하기 위해 아래와 같은 함수를 추가적으로 만든다.\n\nstoch_solve &lt;- function(func, y, times, params, delta) {\n  # times indicate the times for which we want to see outputs\n  out &lt;- data.frame(matrix(NA, nrow = length(times), ncol = (length(y)+1)))\n  out[1, ] &lt;- c(times[1], y)\n  row &lt;- 2\n  \n  substeps &lt;- round((times[2]-times[1])/delta)\n  for (t in 1:(length(times)-1)) {\n    for (t2 in 1:substeps) {\n      y &lt;- y + unlist(func(y, params, delta))\n    }\n    out[row, ] &lt;- c(t, y)\n    row &lt;- row + 1\n  }\n  names(out) &lt;- c(\"time\", names(y))\n  return (out)\n}\n\n위 stoch_solve 함수를 이용하여 계산하고 플롯팅을 해본다. ODE 모형의 결과는 proportion으로 주어져 있으니 1,000을 곱한 후 비교하면 결과가 크게 다르지 않음을 알 수 있다. stoch_solve를 여려 번 실행하여 평균을 비교하면 그리고\\(\\delta\\)을 작게 할 수록 ODE 모형의 결과와 가까워진다.\n\nres &lt;- stoch_solve(func = seir_stoch_step, y=1000*y0, times=0:100, params = params, delta=0.2)\nreslong &lt;- pivot_longer(res, cols=2:5, names_to = \"State\")\n\nggplot(reslong, aes(x = time, y = value, color = State)) +\n  geom_line(linewidth = 1.2) +\n  labs(x = 'Time (day)', y = 'Number')+\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") \n\n\n\n\n\n\nGillespie algorithm\n위에서 기술한 확률론적 방법은 우리가 이미 정한 time interval \\(\\delta\\)에 따라 오차가 발생하는 반면 Gillespie algorithm 을 이용해서 통계적으로 정확한 stochastic simulation 을 할 수 있다.\n\nseir_gillespie &lt;- function(y, params) {\n  S &lt;- y[\"S\"]\n  E &lt;- y[\"E\"]\n  I &lt;- y[\"I\"]\n  R &lt;- y[\"R\"]\n  \n  beta &lt;- params[[\"beta\"]]\n  epsilon &lt;- params[[\"epsilon\"]]\n  gamma &lt;- params[[\"gamma\"]]\n  \n  N &lt;- S + E + I + R\n  event_occurred &lt;- FALSE\n  tau &lt;- 0\n  if (I &gt; 0 & S &gt; 0) {## no need to proceed if no one is infectious or no one is susceptible\n    rate_StoE &lt;- beta * S * I / N\n    rate_EtoI &lt;- epsilon * E\n    rate_ItoR &lt;- gamma * I\n    \n    rate_all &lt;- c(rate_StoE, rate_EtoI, rate_ItoR) # event rates\n    tau &lt;- rexp(1, rate = sum(rate_all)) # time to the next event\n    event &lt;- sample(length(rate_all), 1, prob = rate_all) # next event\n    if (event == 1) {\n      S &lt;- S - 1\n      E &lt;- E + 1\n    }\n    else if (event == 2) {\n      E &lt;- E - 1\n      I &lt;- I + 1\n    }\n    else if (event == 3) {\n      I &lt;- I - 1\n      R &lt;- R + 1\n    }\n    event_occurred &lt;- TRUE;\n  }\n  return (list(y = c(S, E, I, R),\n               tau = tau,\n               event_occurred = event_occurred))\n}\n\nseir_gillespie는 한 번의 event 후 결과를 출력하므로 아래와 같이 추가적인 함수를 구성하여 시물레이션을 한다.\n\nrun_seir_gillespie &lt;- function(func, tend, y, params, report_dt = 1) {\n  res &lt;- data.frame(time = 0, t(y)) # store the simulation results\n  t &lt;- 0\n  yt &lt;- y\n  while (t &lt; tend) {\n    sim &lt;- func(y = yt, params = params) # one event according to the Gillespie algorithm\n    t &lt;- t + sim$tau\n    yt &lt;- sim$y\n    if (t &gt;= report_dt) { # add to the result only when the t is reaches report dt\n      res &lt;- rbind(res, c(t, t(yt)))\n      report_dt &lt;- report_dt  + 1\n    }\n    if (!sim$event_occurred)\n      break\n  }\n  return (res)\n}\n\n시물레이션 결과를 플롯팅 한다.\n\nres &lt;- run_seir_gillespie(func = seir_gillespie, \n                     tend = tend, \n                     y = y0 * 1000, \n                     params = params, \n                     report_dt = 1)\n\nreslong &lt;- pivot_longer(res, cols=2:5, names_to = \"State\")\n\nggplot(reslong, aes(x = time, y = value, color = State)) +\n  geom_line(linewidth = 1.2) +\n  labs(x = 'Time (day)', y = 'Number')+\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\") + \n  ggtitle(\"Gillespie algorithm\")\n\n\n\n# ggsave(\"gillespie.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/profile-likelihood/index.html",
    "href": "posts/profile-likelihood/index.html",
    "title": "Confidence interval using profile likelihood",
    "section": "",
    "text": "감염병 수리 모형 모수(parameter)의 신뢰구간 (confidence interval)구하기 - profile likelihood\n수리 모형을 이용하여 연구를 하게되면 관찰값을 이용하여 모형의 모수를 보정하는 과정을 거치게 된다. 이 과정을 소위 결과 (관찰값)로 부터 원인 (모형)을 알아내는 과정이라 하여 inverse problem 이라 부르기도 한다. 이 글에서는 \\(SEIR\\) 모형과 중국 우한 에서의 초기 코로나-19 발열자 자료를 이용하여 모형의 모수 (기초재감염지수)와 신뢰구간을 구해본다. 모수는 푸아송 (Poisson) 분포를 이용한 최대 우도 (maximum likelihood) 방법으로 그리고 신뢰구간은 profile likelihood 방법을 사용한다.\n아래에 SEIR 모형의 R 코드는 이전에 사용했던 모형에 변수 \\(C\\)를 추가하였는데 이는 누적 발열자수를 나타내고 일별 발열자 수를 쉽게 구하기 위함이다.\n\n# ODE-based SEIR model\nseir &lt;- function(t, y, params) {\n  S &lt;- y[1]; E &lt;- y[2]; I &lt;- y[3]; R &lt;- y[4]; C &lt;- y[5]\n  beta &lt;- params[\"beta\"]\n  sigma &lt;- params[\"sigma\"]\n  gamma &lt;- params[\"gamma\"]\n  \n  muSE &lt;- beta * I / (S + E + I + R)\n  muEI &lt;- sigma\n  muIR &lt;- gamma\n  \n  dS &lt;- - muSE*S\n  dE &lt;-  muSE*S - muEI*E\n  dI &lt;-  muEI*E - muIR*I\n  dR &lt;-  muIR*I\n  dC &lt;-  muEI*E ## cumulative symtom onset\n  \n  return(list(c(dS, dE, dI, dR, dC)))\n}\n\n\n# daily symptomatic case\ndaily_case &lt;- function(params=NULL) { \n  y0 &lt;- c(S = 11e6 - 1, E = 0, I = 1, R = 0, C = 1) # initial values (Wuhan population size)\n  times &lt;- seq(from = 0, to = 35, by = 1)\n  if(is.null(params)){\n    params &lt;- c(beta = 2.5/4.5, sigma = 1/5.2, gamma = 1/4.5)\n  }\n  out &lt;- ode(y = y0, times = times, func = seir, parms = params)\n  x &lt;- as.data.frame(out) \n  n &lt;- nrow(x)\n  daily = c(0, diff(x[,\"C\"]))\n  return (daily)\n} \n\n우한에서 발생한 초기 일별 코로나19 환자수는 Kucharski et al. (2020) Lancet 에 보고된 자료를 기반으로 하였다.\n\nwuhan &lt;- \n  data.frame(date = seq(as.Date(\"2019-12-13\"), \n                        as.Date(\"2020-01-16\"), by = \"day\"),\n             case = c(0,0,0,0,0,0,0,2,2,3,0,1,1,0,0,1,0,1,2,\n                      3,4,3,3,1,2,5,6,8,3,8,8,5,17,7,13))\n\n일별 발열자수 \\(y_t\\)가 푸아송 분포를 따른다고 가정하고 우도 함수를 아래와 같이 정의 한다.\n\\[ y_t \\sim \\mathrm{Poisson}(Y_t)\\] \\[\\mathcal{L}(\\theta) = \\prod_{t=1}^{n} f(y_t \\vert \\theta) = \\prod_{t=1}^{n} \\frac{Y_t^{y_t} e^{-Y_t}}{y_t!}\\]\n우도 계산식을 아래와 같이 R로 구현할 수 있다. 물론 우도함수는 수치 안정성을 위해서 log 를 취한 값을 사용하고 (즉 log likelihood) 최적화 알고리듬은 최소값을 찾기 때문에 음의값으로 치환한 negative log likelihood를 사용한다.\n\nnegloglik &lt;- function(par) {\n  params &lt;- c(beta = par, sigma = 1/5.2, gamma = 1/4.5)\n  model &lt;- daily_case(params = params)\n  - sum(dpois(x = wuhan$case, lambda = model, log = TRUE)) # sum of negative log likelihood\n}\n\nSEIR 모형에는 세 개의 모수 (\\(\\beta, \\sigma, \\gamma\\))가 있는데 \\(\\sigma, \\gamma\\)는 각각 잠복기와 회복까지 걸리는 시간을 나타내고 환자들을 관찰하여 그 값을 추정할 수 있는 경우가 많다. 이에 반해 \\(\\beta\\)는 수리 모형의 예측값을 관찰된 유행 곡선과 비교하여 추정한다. 이 과정이 negloglik 함수에 구현된 것이고 optim 함수를 사용하여 negloglik를 최소화하는 \\(\\beta\\)를 구한다.\n\\[ \\hat{\\theta} = \\underset{\\theta}{\\mathrm{argmax}}~\\{{\\mathrm{log} \\mathcal{L}(\\theta)} \\}\\]\n\nlibrary(deSolve) # negloglik includes ODE model to be integrated using deSolve\nfit &lt;- optim(negloglik, par=c(0.1), method=\"Brent\", lower=0, upper=10)\n(theta &lt;- fit$par)\n\n[1] 0.5735032\n\ngamma &lt;- 1/4.5; (R0 &lt;- theta/gamma)\n\n[1] 2.580765\n\n\n95% 신뢰 구간은 Log likelihood 가 asymptotically 아래의 조건을 만족한다는 사실을 이용하여 계산할 수 있다 (Wilks’ theorem). \\[ 2 (\\mathrm {log} \\mathcal {L} (\\hat{\\theta}) - \\mathrm{log}\\mathcal{L}(\\theta_0)) \\sim \\chi^2_1\\]\n\nprof_b &lt;- expand.grid(b = seq(0.5, 0.7, length = 1000))\nprof_b$loglik &lt;- -sapply(prof_b$b, negloglik)\nmaxloglik &lt;- - fit$value\ncutoff &lt;- maxloglik - qchisq(p=0.95,df=1)/2\n(limits &lt;- range(subset(prof_b, loglik &gt; cutoff)$b)) # 95% confidence interval\n\n[1] 0.5518519 0.5940941\n\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\nlibrary(dplyr)\nprof_b |&gt; as.data.frame() |&gt; \n  ggplot(aes(b,loglik))+\n  geom_line()+\n  geom_vline(xintercept=fit$par, color=\"steelblue\", linewidth=1)+\n  geom_vline(xintercept=limits, color=\"steelblue\", linetype=\"dotted\", linewidth=1)+\n  geom_hline(yintercept=maxloglik, color=\"steelblue\", linewidth=1)+\n  geom_hline(yintercept=cutoff, color=\"steelblue\", linewidth=1)+\n  scale_y_continuous(limits= c(maxloglik-6, maxloglik+1))+\n  labs(x=expression(beta), y=\"Log likelihood\")\n\n\n\n# ggsave(\"profile_lik.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)\n\nbbmle 패키지는 confint 함수로 profile likelihood를 이용하여 신뢰구간을 구하는 방법을 제공해준다.\n\nlibrary(bbmle)\nbbfit &lt;- mle2(minuslogl=negloglik, start=list(par=0.1), method=\"L-BFGS-B\", lower=c(par=1e-6), control=list(maxit=5000))\nconfint(bbfit)\n\n    2.5 %    97.5 % \n0.5517962 0.5942085"
  },
  {
    "objectID": "posts/particle-filter/index.html",
    "href": "posts/particle-filter/index.html",
    "title": "Particle filter using R",
    "section": "",
    "text": "A simple particle filter in R\nThe following example was adapted from the post in RPubs.\n\nSimulate the data\nGenerate \\(y_{1:T}\\) as a sequence of noisy observations of a latent variable \\(x_{1:T}\\).\n\n# create a data set: x (latent variable) and y (observation)\nset.seed(42) # to make it reproducible  (lots of random numbers follow)\nT &lt;- 50 # number of observations\nx &lt;- rep(NA, T) # latent variable\ny &lt;- rep(NA, T) # observed values\nsx &lt;- 2.2 # standard deviation for x\nsy &lt;- 0.3 # standard deviation for y\nx[1] &lt;- rnorm(1, 0, 1)\ny[1] &lt;- rnorm(1, x[1], sy)\n\nfor (t in seq(2, T)) {\n  x[t] &lt;- rnorm(1, x[t-1], sx)\n  y[t] &lt;- rnorm(1, x[t], sy)\n}\nx_true &lt;- x\nobs &lt;- y\n\n\n\nImplement a particle filter (sequential Monte Carlo)\n\n# particle filter -----------------------------------------------------------\nT &lt;- length(y) # number of observations\nN &lt;- 100 # number of particles\n# to store prior distributions for variables correspond to latent variable x\nx_prior &lt;- matrix(nrow=N, ncol = T) \nx_post &lt;- matrix(nrow=N, ncol = T)  # posterior distributions\nweights &lt;- matrix(nrow=N, ncol = T) # weights used to draw posterior sample\nW &lt;- matrix(nrow =  N, ncol = T) # normalized weights\nA &lt;- matrix(nrow =  N, ncol = T) # indices based on the normalized weights\nx_prior[, 1] &lt;- rnorm(N, 0, sx)# initial X from a normal distribution\n# calculate weights, normal likelihood\nweights[, 1] &lt;- dnorm(obs[1], x_prior[, 1], sy)\nW[, 1] &lt;- weights[, 1]/sum(weights[, 1])# normalise weights\n# indices based on the weighted resampling with replacement \nA[, 1] &lt;- sample(1:N, prob = W[1:N, 1], replace = T) \nx_post[, 1] &lt;- x_prior[A[, 1], 1] # posterior distribution using the indices\n\nfor (t in seq(2, T)) {\n  x_prior[, t] &lt;- rnorm(N, x_post[, t-1], sx) # prior x_{t} based on x_{t-1}\n  weights[, t] &lt;- dnorm(obs[t], x_prior[, t], sy) # calculate weights \n  W[, t] &lt;- weights[, t]/sum(weights[, t]) # normalise weights\n  A[, t] &lt;- sample(1:N, prob = W[1:N, t], replace = T) # indices\n  x_post[, t] &lt;- x_prior[A[, t], t] # posterior samples\n}\n\n\n\n\nSummarize results\nCalculate the mean and 2.5\\(^\\textrm{th}\\) and 97.\\(^\\textrm{th}\\) percentile of the posterior sample as a means to get 95% credible interval.\n\nx_means &lt;- apply(x_post, 2, mean) # posterior mean\nx_quantiles &lt;- apply(x_post, 2, function(x) quantile(x, probs = c(0.025, 0.975))) # 95% credible interval\ndf &lt;- data.frame(t = seq(1, T),\n                 x_mean = x_means,\n                 x_lb = x_quantiles[1, ],\n                 x_ub = x_quantiles[2, ],\n                 x_true = x_true, # latent variables\n                 y = y) # observed values\n\n\nPlot the results\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nggplot(df, aes(x = t)) +\n  geom_ribbon(aes(ymin = x_lb, ymax = x_ub, fill=\"95% CrI\"), alpha=0.5) +\n  geom_line(aes(y=x_mean, color=\"Posterior mean\")) +\n  geom_line(aes(y=x_true, color=\"True\")) +\n  geom_point(aes(y=x_mean, color=\"Posterior mean\")) +\n  geom_point(aes(y=x_true, color=\"True\")) +\n  labs(y=\"values\", x=\"index\") + \n  scale_colour_manual(\"\", values=c(\"Posterior mean\"=\"firebrick\",\n                                   \"True\"=\"darkgrey\")) +\n  scale_fill_manual(\"\", values=\"firebrick\")+\n  theme(legend.position = \"bottom\")\n\n\n\n# ggsave(\"particle_filter.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/particle_filter_COVID19/index.html",
    "href": "posts/particle_filter_COVID19/index.html",
    "title": "Estimating the instantaneous reproduction number using the particle filter",
    "section": "",
    "text": "A simple particle filter in R\n파티클 필터 (particle filter) 를 이용하여 잠재 변수 (latent variable)를 추정하는 과정을 지난 글에서 다루었다. 관찰값들이 코로나 19 일별 감염자일때 감염병 수리 모형을 이용하여 일별 감염재생산지수 (\\((R_t\\)) 를 추정한다. 아래 글은 2020년 Kucharski et al. 논문에 사용되었던 방법을 차용하였다. 이해를 돕기 위해 모형을 단순화 하였고 가상의 데이타를 만들어 내는 과정을 더하였다. 우선 SEIR 모형을 이용해서 가상의 데이타 (일별 감염자 수)를 만든다. 누적 감염자 (cumulative incidence) 를 나타내는 CI라는 변수의 일별 차이를 계산하여 일별 감염자 수를 계산한다. 보통의 SEIR 모형에서는 \\(\\beta\\)가 상수로 취급 되지만 아래 모형에서는 일별 감염 재생산지수 \\(R_t = \\beta (t) \\times D\\) \\(D\\)는 감염 기간)가 방역 정책, 활동 변화 등 이유로 인해 시간에 따라 변화한다고 가정하기 때문에 시간에 따른 함수 \\(\\beta(t)\\)로 표현한다. 우리가 추정 하고자 하는 \\(R_t\\)를 미리 정의하고 이로 부터 \\(\\beta(t)\\) 를 계산하고 이를 SEIR 모형에 적용하여 가상의 데이타를 만든다.\n아래와 같은 방식으로 SEIR 모형을 만든다. 본래 미분식으로 정의하고 deSolve 패키지의 ode 함수 등을 이용하여 적분할 수 있으나 이 글에서는 간단하게 Euler 방법을 사용한다.\n\nSEIR_Euler &lt;- function (params = NULL,\n                        y = NULL,\n                        tbegin = 0,\n                        tend = 1,\n                        dt = 0.2) {\n  \n  M &lt;- matrix(NA, nrow=(tend-tbegin+1), ncol=length(y)) # output matrix\n  M[1,] &lt;- y # initial values for the first row\n  \n  S &lt;- y[1]; E &lt;- y[2]; I &lt;- y[3]; R &lt;- y[4]; CI &lt;- y[5]\n  N &lt;- S + E + I + R\n  epsilon &lt;- params[[\"epsilon\"]]\n  gamma &lt;- params[[\"gamma\"]]\n  Rt &lt;- params[[\"Rt\"]] # daily reproduction number\n  \n  for (t in seq(tbegin, tend, by=1)) { # for each day\n    for (i in seq(dt, 1, dt)) { # sub-intervals that can vary\n      # beta is already adjusted by N \n      # t is not an integer\n      beta &lt;- Rt[floor(t+1+dt)] * gamma # transmission rate\n      S_to_E &lt;- beta * I * dt\n      E_to_I &lt;- E * epsilon * dt\n      I_to_R &lt;- I * gamma * dt\n      \n      # update state variables\n      S &lt;- S - S_to_E\n      E &lt;- E + S_to_E - E_to_I\n      I &lt;- I + E_to_I - I_to_R\n      R &lt;- R + I_to_R\n      CI &lt;- CI + S_to_E\n    }\n    # output for each day\n    M[t+1, 1] &lt;- S \n    M[t+1, 2] &lt;- E\n    M[t+1, 3] &lt;- I\n    M[t+1, 4] &lt;- R\n    M[t+1, 5] &lt;- CI\n  }\n  return(M)\n}\n\n일별 감염자 수를 플롯해본다.\n\n# pre-defined Rt\nRt_true &lt;- c(rep(1.2, 15), 0.5*sin(0.1*pi*0:32) + 1.2, rep(0.9, 100))\nI0 &lt;- 100 # initially infected people\ny0 &lt;- c(S = 1e7-I0, E = 0, I = I0, R = 0, CI = 0) # initial values for state variables\nparams &lt;- list() # input parameters for the SEIR model\nparams$Rt &lt;- Rt_true\nparams$epsilon &lt;- 0.5 # 1/epsilon = latent period\nparams$gamma &lt;- 0.2 # 1/gamma = duration of infectiousness\ntend &lt;- 50 # simulation end time 50 days\n\nres1 &lt;- SEIR_Euler(params = params, y=y0, tend=50) # run the model\nres1 &lt;- as.data.frame(res1)\nres1$daily_infected &lt;- c(0, diff(res1$V5))\nres1$time &lt;- 0:tend\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(res1, aes(x = time, y = daily_infected)) +\n  geom_line(size = 1.2) +\n  labs(x = 'Time (day)', y = 'Daily infected')\n\n\n\n\n푸아송 분포를 이용하여 가상의 데이타를 만든다.\n\n# Create the data assunming observations are poisson random variable\nset.seed(42)\nfakedata &lt;- data.frame(daily_infected = rpois(nrow(res1), lambda = res1$daily_infected))\n\n일별 변화를 계산하는 SEIR 전파 모형, 행의 수는 파티클 수와 같다.\n\n# stochastic differential equation (with beta(t) moves according to a geometric Brownian motion) are modeled using the Euler-Maruyama method.\n# daily change is modeled using the subinterval dt\nSEIR_step &lt;- function (params = NULL,\n                       y = NULL,\n                       tbegin = 0,\n                       tend = 1,\n                       dt = 0.2,\n                       beta = NULL) {\n  # daily infection reset to zero to hold values from tbegin to tend\n  y[, c(\"CI\")] &lt;- 0\n  \n  S &lt;- y[, \"S\"]\n  E &lt;- y[, \"E\"]\n  I &lt;- y[, \"I\"]\n  R &lt;- y[, \"R\"]\n  daily_infected &lt;- y[, \"CI\"]\n  \n  N &lt;- S + E + I + R\n  epsilon &lt;- params[[\"epsilon\"]]\n  gamma &lt;- params[[\"gamma\"]]\n  \n  for (i in seq((tbegin + dt), tend, dt)) {\n    # beta is already assumed to be adjusted by N such that it can\n    # be translated to Rt by multiplying the duration of infectiousness\n    S_to_E &lt;- beta * I * dt\n    E_to_I &lt;- E * epsilon * dt\n    I_to_R &lt;- I * gamma * dt\n    # Process model for SEIR\n    S &lt;- S - S_to_E\n    E &lt;- E + S_to_E - E_to_I\n    I &lt;- I + E_to_I - I_to_R\n    R &lt;- R + I_to_R\n    daily_infected &lt;- daily_infected + S_to_E\n  }\n  y[, \"S\"] &lt;- S\n  y[, \"E\"] &lt;- E\n  y[, \"I\"] &lt;- I\n  y[, \"R\"] &lt;- R\n  y[, \"CI\"] &lt;- daily_infected\n  \n  return(y)\n}\n\n파티클 필터링 함수\n\npfilter &lt;- function (params, # parameters\n                     y, # initial values of state variables\n                     data, # input data set\n                     npart = 1000, # number of particles\n                     tend = NULL, # simulation stop time\n                     dt = 0.2) {\n  \n  # Assumptions - using daily growth rate\n  nstatevar &lt;- length(y) # number of state variables\n  if(is.null(tend)) {\n    tend = nrow(data)\n  }\n  # to store state variables\n  latent_var &lt;- array(0,\n                      dim = c(npart, tend, nstatevar),\n                      dimnames = list(NULL, NULL, names(y)))\n  # latent_var[, 1, ] &lt;- y\n  for (nm in names(y)) { # initial value\n    latent_var[, 1, nm] &lt;- y[[nm]]\n  }\n  ## parameters \n  gamma &lt;- params[[\"gamma\"]]\n  beta0 &lt;- params[[\"R0\"]] * gamma\n  beta_sd &lt;- params[[\"betavol\"]]\n  beta &lt;- matrix(rnorm(npart * tend, mean = 0, sd = beta_sd), nrow = tend)\n  beta[1,] &lt;- beta0 # this is updated at t=2\n  \n  wt &lt;- matrix(NA, nrow = npart, ncol = tend) # weight (likelihood)\n  wt[, 1] &lt;- 1 / npart  # initial weights\n  W &lt;- matrix(NA, nrow = npart, ncol = tend) # normalized weights\n  A &lt;- matrix(NA, nrow = npart, ncol = tend) # Resample according to the normalized weight\n  \n  for (t in 2:tend) {# begin particle loop\n    # beta changes according to a Geometric Brownian motion \n    beta[t, ] &lt;- beta[t-1, ] * exp(beta[t, ])\n    # run process model\n    latent_var[, t, ] &lt;- SEIR_step(params = params,\n                                   y = latent_var[, t-1, ],\n                                   tbegin = t-1,\n                                   tend = t,\n                                   dt = dt,\n                                   beta = beta[t,])\n    # calculate weights (likelihood)\n    # wt[, t] &lt;- assign_weights(var = latent_var, t = t, data = data)\n    \n    case_expected &lt;- latent_var[, t, \"CI\"]\n    case_data &lt;- round(unlist(data[t, \"daily_infected\"]))\n    expected_val &lt;- pmax(0, case_expected) # make sure that the value is not negative\n    log_lik &lt;- dpois(round(case_data), lambda = expected_val, log = T)\n    wt[, t] &lt;- exp(log_lik)\n    # normalize particle weights\n    W[, t] &lt;- wt[, t] / sum(wt[, t])\n    # resample particles by sampling parent particles according to weights\n    A[, t] &lt;- sample(1:npart, prob = W[1:npart, t], replace = T)\n    # Resample particles for corresponding variables\n    latent_var[, t,] &lt;- latent_var[A[, t], t,]\n    beta[t,] &lt;- beta[t, A[, t]] #- needed for random walk on beta\n  } # end particle loop\n  \n  # Marginal likelihoods\n  lik_values &lt;- rep(NA, tend)\n  for (t in 1:tend) {\n    lik_values[t] &lt;- log(sum(wt[1:npart, t])) # log-likelihoods\n  }\n  # averaged log likelihoods log(L/(npart^tend))\n  loglik &lt;- - tend * log(npart) + sum(lik_values)\n  \n  return (list(lik_marginal = lik_values,\n               lik_overall_average = loglik,\n               latent_var_filtered = latent_var,\n               beta_filtered = beta,\n               W = W, A = A))\n}\n\n일별 변화를 계산하는 SEIR 전파 모형, 행의 수는 파티클 수와 같다.\n\nparams$R0 &lt;- 2\nparams$betavol &lt;- 0.3\nsample &lt;- pfilter(params=params, # parameters\n                     y=y0, # initial values of state variables\n                     data=fakedata, # input data set\n                     npart = 1000, # number of particles\n                     tend = tend, # simulation stop time\n                     dt = 0.2) \nobserved &lt;- fakedata$daily_infected[2:nrow(fakedata)]\n\n\nPlot the results\n\n# draw incidence plot\ndaily_inc_summary &lt;- t(apply(sample$latent_var_filtered[,,5], 2, quantile,\n            probs=c(0.025, 0.5, 0.975)))\ndf &lt;- cbind(data.frame(time=1:(nrow(res1)-1), observed = observed), daily_inc_summary)\n\n\nggplot(df, aes(x=time)) +\n  geom_ribbon(aes(ymin=`2.5%`, ymax=`97.5%`), fill=\"steelblue\", alpha=0.8)+\n  geom_line(aes(y=`50%`), color=\"steelblue\")+\n  geom_point(aes(y=observed), color = \"darkred\")+\n  labs(x=\"Time\", y=\"Daily incidence\")\n\n\n\n# draw daily Rt plot\ndur &lt;- 1/params$gamma\ndaily_Rt_summary &lt;- t(apply(sample$beta_filtered * dur, 1, quantile,\n                            probs=c(0.025, 0.5, 0.975)))  \ndf &lt;- cbind(data.frame(time=1:(nrow(res1)-1), true_Rt = Rt_true[2:51]), daily_Rt_summary)\nggplot(df, aes(x=time)) +\n  geom_ribbon(aes(ymin=`2.5%`, ymax=`97.5%`), fill=\"darkgreen\", alpha=0.7)+\n  geom_line(aes(y=`50%`), color=\"darkgreen\")+\n  geom_point(aes(y=true_Rt), color = \"black\") + \n  labs(x=\"Time\", y=expression(italic(R[t])))\n\n\n\n# ggsave(\"particle_filter_covid.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/0_template/index.html",
    "href": "posts/0_template/index.html",
    "title": "Template",
    "section": "",
    "text": "A simple particle filter in R\nThe following example was adapted from the post in RPubs.\n\nSimulate the data\nGenerate \\(y_{1:T}\\) as a sequence of noisy observations of a latent variable \\(x_{1:T}\\).\n\\[\n\\begin{align}\n\\mathrm{d}S/\\mathrm{d}t &= -\\beta I S \\\\\n\\mathrm{d}I/\\mathrm{d}t &= \\beta I S - \\gamma I\\\\\n\\end{align}\n\\] The first part is simply to compute \\(dI/dS\\).\n\nfrom sympy import *\n\nR_0, b, g, dIdt, dSdt, S, I = symbols('R_0 b g dIdt dSdt S I')\n\ndSdt = - b*S*I\ndIdt = + b*S*I - g*I\ndSdI = dIdt / dSdt #-(I*S*b - I*g)/(I*S*b)\n\n# b &lt;- R0*g\nsimplify(-(I*S*R_0*g - I*g)/(I*S*R_0*g)) \n\n-1 + 1/(R_0*S)\n\n\n\n# create a data set: x (latent variable) and y (observation)\nset.seed(42) # to make it reproducible  (lots of random numbers follow)\nT &lt;- 50 # number of observations\nx &lt;- rep(NA, T) # latent variable\ny &lt;- rep(NA, T) # observed values\nsx &lt;- 2.2 # standard deviation for x\nsy &lt;- 0.3 # standard deviation for y\nx[1] &lt;- rnorm(1, 0, 1)\ny[1] &lt;- rnorm(1, x[1], sy)\n\nfor (t in seq(2, T)) {\n  x[t] &lt;- rnorm(1, x[t-1], sx)\n  y[t] &lt;- rnorm(1, x[t], sy)\n}\nx_true &lt;- x\nobs &lt;- y\n\n\n\nImplement a particle filter (sequential Monte Carlo)\n\n# particle filter -----------------------------------------------------------\nT &lt;- length(y) # number of observations\nN &lt;- 100 # number of particles\n# to store prior distributions for variables correspond to latent variable x\nx_prior &lt;- matrix(nrow=N, ncol = T) \nx_post &lt;- matrix(nrow=N, ncol = T)  # posterior distributions\nweights &lt;- matrix(nrow=N, ncol = T) # weights used to draw posterior sample\nW &lt;- matrix(nrow =  N, ncol = T) # normalized weights\nA &lt;- matrix(nrow =  N, ncol = T) # indices based on the normalized weights\nx_prior[, 1] &lt;- rnorm(N, 0, sx)# initial X from a normal distribution\n# calculate weights, normal likelihood\nweights[, 1] &lt;- dnorm(obs[1], x_prior[, 1], sy)\nW[, 1] &lt;- weights[, 1]/sum(weights[, 1])# normalise weights\n# indices based on the weighted resampling with replacement \nA[, 1] &lt;- sample(1:N, prob = W[1:N, 1], replace = T) \nx_post[, 1] &lt;- x_prior[A[, 1], 1] # posterior distribution using the indices\n\nfor (t in seq(2, T)) {\n  x_prior[, t] &lt;- rnorm(N, x_post[, t-1], sx) # prior x_{t} based on x_{t-1}\n  weights[, t] &lt;- dnorm(obs[t], x_prior[, t], sy) # calculate weights \n  W[, t] &lt;- weights[, t]/sum(weights[, t]) # normalise weights\n  A[, t] &lt;- sample(1:N, prob = W[1:N, t], replace = T) # indices\n  x_post[, t] &lt;- x_prior[A[, t], t] # posterior samples\n}\n\n\n\n\nSummarize results\nCalculate the mean and 2.5\\(^\\textrm{th}\\) and 97.\\(^\\textrm{th}\\) percentile of the posterior sample as a means to get 95% credible interval.\n\nx_means &lt;- apply(x_post, 2, mean) # posterior mean\nx_quantiles &lt;- apply(x_post, 2, function(x) quantile(x, probs = c(0.025, 0.975))) # 95% credible interval\ndf &lt;- data.frame(t = seq(1, T),\n                 x_mean = x_means,\n                 x_lb = x_quantiles[1, ],\n                 x_ub = x_quantiles[2, ],\n                 x_true = x_true, # latent variables\n                 y = y) # observed values\n\n\nPlot the results\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nggplot(df, aes(x = t)) +\n  geom_ribbon(aes(ymin = x_lb, ymax = x_ub, fill=\"95% CrI\"), alpha=0.5) +\n  geom_line(aes(y=x_mean, color=\"Posterior mean\")) +\n  geom_line(aes(y=x_true, color=\"True\")) +\n  geom_point(aes(y=x_mean, color=\"Posterior mean\")) +\n  geom_point(aes(y=x_true, color=\"True\")) +\n  labs(y=\"values\", x=\"index\") + \n  scale_colour_manual(\"\", values=c(\"Posterior mean\"=\"firebrick\",\n                                   \"True\"=\"darkgrey\")) +\n  scale_fill_manual(\"\", values=\"firebrick\")+\n  theme(legend.position = \"bottom\")\n\n\n\n# ggsave(\"particle_filter.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/nimble/index.html",
    "href": "posts/nimble/index.html",
    "title": "SEIR model using the Nimble pacakge",
    "section": "",
    "text": "감염병 수리 모형을 개발하는 데 있어 가장 근본적인 질문 중 하나는 주어진 관찰값 (시계열)하에서 어떤 모형을 선택하고 그 모수의 값을 어떻게 결정하는가이다. 모형을 선택하는 과정은 따로 다루기로 하고 여기서는 일반적으로 사용되는 감염병 수리 모형 (i.e., SIR)을 사용할 때 모수를 추정하는 과정에 대해서 이야기해보자. 최대 가능도 (maximum likelihood) 방법에 대해서는 전에 언급하였다. 모수를 추정하는 여러 방법 중에 마르코프 연쇄 몬테카를로 (Markov Chain Monte Carlo; MCMC) 방법이 적절한 모수의 값을 찾아내고 그 값의 불확실성 (uncertainty)를 나타내는 데 가장 널리 쓰이는 방법 중의 하나이다. MCMC 알고리듬을 직접 작성해서 사용한는 것도 원리를 이해하는 데에는 도움이 되지만 이미 다양한 통계 패키지에서 MCMC가 사용되고 있으므로 기존 패키지를 사용하는 것도 합리적인 방법이 될 수 있다. 회귀 분석 등 통계모형의 경우BUGS (Bayesian Inference Using Gibbs Sampling) 혹은 JAGS (Just Another Gibbs Sampler), Stan, 그리고 NIMBLE 등에 구현된 MCMC를 사용하는 것이 많이 보편화 되어 있다.\n이 중 NIMBLE 은 R 패키지 nimble을 이용해서 사용할 수 있고 패키지에서 제공하는 함수 기능을 이용해서 감염병 수리 모형을 구현하고 MCMC 까지 할 수 있다. syntax 또한 R과 유사해서 R를 사용하는 사람에게는 Stan 보다 더 접근이 용이한 것 같다. 아래에는 nimble 함수 기능을 이용하여 Euler 방법에 기반한 SIR 모형을 구현한 예이다.\n\nlibrary(nimble)\nsir_incidence &lt;- nimbleFunction(\n  run = function(beta = double(0)) {\n    tend &lt;- 100 # 100 days of simulation\n    dt &lt;- 0.1 # time step of 0.1 day\n    \n    # initial condition\n    St &lt;- 999 \n    It &lt;- 1\n    Rt &lt;- 0\n    CIt &lt;- 0\n    \n    # create vectors for the state variables\n    S &lt;- rep(0, tend)\n    I &lt;- rep(0, tend)\n    R &lt;- rep(0, tend) \n    CI &lt;- rep(0, tend) # cumulative incidence\n    \n    # first elements of the vectors are initial conditions\n    S[1] &lt;- St\n    I[1] &lt;- It\n    R[1] &lt;- Rt\n    CI[1] &lt;- CIt\n  \n    gamma &lt;- 0.2 # 1/gamma = duration of infectiousness\n    \n    for (i in 2:tend) { # each day\n      for (j in 1:ceiling(1/dt)) { # time steps per day\n        Nt &lt;- St + It + Rt                     # total population size\n        rate_StoI &lt;- St * beta * It / Nt * dt  # transition rate from S to I\n        rate_ItoR &lt;- gamma * It * dt           # transition rate from I to R\n  \n        dS &lt;- - rate_StoI            # rate of change for S\n        dI &lt;- rate_StoI - rate_ItoR  # rate of change for I\n        dR &lt;- rate_ItoR              # rate of change for R\n        dCI &lt;- rate_StoI             # rate of change for cumulative incidence\n      \n        St &lt;- St + dS                # update the St\n        It &lt;- It + dI                # update the It \n        Rt &lt;- Rt + dR                # update the Rt\n        CIt &lt;- CIt + dCI             # update the CIt\n      }\n      S[i] &lt;- St                     # put St in the vector \n      I[i] &lt;- It                     # put It in the vector \n      R[i] &lt;- Rt                     # put Rt in the vector \n      CI[i] &lt;- CIt                   # put CIt in the vector \n   }\n   # daily incidence from cumulative incidence\n   inc &lt;- CI[2:tend] - CI[1:(tend-1)] \n   return(inc) \n   returnType(double(1)) # return type\n }\n)\n\n모수 추정을 위해서 푸아송 분포를 이용하여 거짓 관찰값 (Y) 을 만들어보자.\n\n# create observation\nbeta &lt;- 0.4 # true beta\nX &lt;- sir_incidence(beta) # true daily incidence\nY &lt;- rpois(length(X), lambda=X) # Poisson-distributed observation\n\n아래와 같이 prior distribution, likelihood, 그리고 posterior predictive check위해서 ypred 도 함께 구현한다.\n\n# BUGS style code\ncode &lt;- nimbleCode({\n  beta ~ T(dnorm(0, sd = 2), 0, 2)   # prior for beta truncated at 0 and 2\n  mu[1:N] &lt;- sir_incidence(beta)     # daily incidence from the model\n  for (i in 1:N) {\n    y[i] ~ dpois(mu[i])              # likelihood\n    ypred[i] ~ dpois(mu[i])          # posterior predictive value\n  }\n})\n\n아래와 같이 초기 조건을 설정하고 모형을 구성한다. 빠른 실행을 위해서 컴파일 한다.\n\n# constants, data, and initial values\nconstants &lt;- list(N = length(Y))      # number of observation\ndata &lt;- list(y = Y)                   # observation\ninits &lt;- list(beta = 0.1)             # starting point for beta  \n\n# create the model object\nsir_model &lt;- nimbleModel(code = code,\n                         constants = constants,\n                         data = data,\n                         inits = inits,\n                         check = FALSE)\n\nsirMCMC &lt;- buildMCMC(sir_model, monitors=c('beta','ypred'))\nCsir &lt;- compileNimble(sir_model)\nCsirMCMC &lt;- compileNimble(sirMCMC, project=Csir)\n\n# thining interval was chosen based on previous analyses of ACF\nsamples &lt;- runMCMC(CsirMCMC, niter=5000, thin=10, nburnin=1000)\n# saveRDS(samples, \"samples_nimble_20231125e.rds\")\n\n\nsamples &lt;- readRDS(\"samples_nimble_20231125e.rds\")\nplot(samples[,1], type=\"l\", ylab=expression(beta), xlab=\"Iterations\")\n\n\n\n\n\\(\\beta\\) 의 posterior distribution과 거짓 자료를 만들기 위해 사용했던 \\(\\beta\\)값 (빨간색)을 비교해보자.\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nsamples |&gt; \n  as.data.frame() |&gt; \n  ggplot()+\n  geom_histogram(aes(x=beta, fill=\"posterior\"))+\n  geom_vline(aes(xintercept=0.4, color=\"true\"), linewidth=1.2)+\n  labs(x=expression(beta), y=\"frequency\")+\n  scale_fill_manual(\"\", values=c(\"posterior\"=\"grey\"))+\n  scale_color_manual(\"\", values=c(\"true\"=\"firebrick\"))+\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nPosterior predictive check\n\n# posterior predictive check\nnsamp &lt;- nrow(samples)\ndf &lt;- data.frame(time=rep(1:99, nsamp+1), \n                 name=c(rep(1:nsamp, each=99),rep(\"data\",99)))\ndf$value &lt;- c(c(t(samples[,2:100])), Y)\n\n\nlibrary(dplyr)\nggplot(df)+\n  geom_line(data=filter(df, name!=\"data\"), aes(x=time, y=value, group=name,\n            color=\"posterior predictive values\"))+\n  geom_point(data=filter(df, name==\"data\"), aes(x=time, y=value, color=\"data\"),\n             size=1.2) +\n  labs(x='Time (day)', y='Daily infected')+\n  scale_color_manual(\"\",values=c(\"posterior predictive values\"=\"grey\",\n                                 \"data\"=\"firebrick\"))+\n  theme(legend.position = \"bottom\")\n\n\n\n# ggsave(\"nimble_ppc_incidence.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/euler_multinomial/index.html",
    "href": "posts/euler_multinomial/index.html",
    "title": "Multinomial distribution",
    "section": "",
    "text": "A simple particle filter in R\nWhen implementing a model of stochastic disease transmission, one has to deal with a situation in which multiple events are possible. For example, susceptible people may become infected, remain susceptible, or die from other causes. In R, one could use rmultinorm as long as one can assign a probability for each event. Here, however, we implement a function from scratch. One way is to follow the approach of Aaron King, author of the pomp package. His method is implemented in C and I adapted it to R and C++ while removing many of its auxiliary functions (e.g., checking the validity of the inputs).\n\nreulermultinom2 &lt;- function (m=2, size, rate, dt) {\n  trans &lt;- matrix(NA, nrow=m, ncol=length(rate))\n  p &lt;- 0.0 # total event rate\n  if ((size &lt; 0.0) | (dt &lt; 0.0) | (floor(size+0.5) != size)) {\n    for (k in seq_along(rate)) {\n      trans[k] = NaN\n    }\n    return(trans)\n  }\n  if (sum(rate &lt; 0.0) &gt; 0){\n    stop(\"Negative rates  are not allowed\")\n  }\n  else {\n    p &lt;- sum(rate)\n  }\n  if (p &gt; 0.0) {\n    for (i in 1:m) {\n      tmpsize &lt;- rbinom(1, size = size, prob = (1-exp(-p*dt))) # total number of events\n      tmpp &lt;- p\n      for (k in 1:(length(rate)-1)) {\n        trans[i, k] = rbinom(1, tmpsize, rate[k]/tmpp)\n        tmpsize = tmpsize - trans[i, k];\n        tmpp = tmpp - rate[k];\n      }\n      trans[i, length(rate)] = tmpsize;\n    }    \n  } \n  \n  return(trans)\n}\n\nLet’s compare it with the original function provided in the pomp package\n\nx &lt;- t(pomp::reulermultinom(1e5, 100, rate=c(1,2), dt=0.05))\ny &lt;- reulermultinom2(1e5, 100, rate=c(1,2), dt=0.05)\nxy &lt;- as.data.frame(cbind(x, y))\nnames(xy) &lt;- c(\"pomp_var1\", \"pomp_var2\", \"var1\", \"var2\")\napply(xy, 2, summary)\n\n        pomp_var1 pomp_var2     var1     var2\nMin.      0.00000   0.00000  0.00000  0.00000\n1st Qu.   3.00000   7.00000  3.00000  7.00000\nMedian    4.00000   9.00000  4.00000  9.00000\nMean      4.63943   9.28385  4.64695  9.29948\n3rd Qu.   6.00000  11.00000  6.00000 11.00000\nMax.     15.00000  24.00000 15.00000 23.00000\n\n\nThe speed difference is quite substantial.\n\nlibrary(microbenchmark)\nmicrobenchmark(pomp::reulermultinom(100, 100, rate=c(1,2), dt=0.05), reulermultinom2(100, 100, rate=c(1,2), dt=0.05))\n\nUnit: microseconds\n                                                      expr   min     lq    mean\n pomp::reulermultinom(100, 100, rate = c(1, 2), dt = 0.05)  39.2  40.30  44.024\n      reulermultinom2(100, 100, rate = c(1, 2), dt = 0.05) 374.6 385.85 504.970\n median     uq    max neval cld\n  41.75  44.25   91.4   100  a \n 399.00 530.05 5534.6   100   b\n\n\nRewrite the function in C++ using Rcpp.\n\nRcpp::cppFunction(\"NumericMatrix reulermultinom_cpp(int m, double size, NumericVector rate, double dt) {\n  int ncol = rate.size();\n  NumericMatrix trans(m, ncol);\n  double p = sum(rate); //total event rate\n  for (int i = 0; i &lt; m; i++) { \n    double tmpp = p;\n    double tmpsize = R::rbinom(size, (1-exp(-tmpp*dt))); // total number of events\n    for (int k = 0; k &lt; (ncol-1); k++) {\n      double tr = R::rbinom(tmpsize, rate(k)/tmpp);\n      trans(i, k) = tr;\n      tmpsize = tmpsize - trans(i, k);\n      tmpp = tmpp - rate(k);\n    }\n    trans(i, (ncol-1)) = tmpsize;\n  }    \n  return(trans);\n}\")\n\n\nlibrary(microbenchmark)\nmicrobenchmark(pomp::reulermultinom(1e5, 100, rate=c(1,2), dt=0.05), reulermultinom_cpp(1e5, 100, rate=c(1,2), dt=0.05))\n\nUnit: milliseconds\n                                                        expr     min       lq\n pomp::reulermultinom(1e+05, 100, rate = c(1, 2), dt = 0.05) 77.3161 81.80885\n   reulermultinom_cpp(1e+05, 100, rate = c(1, 2), dt = 0.05) 74.5419 80.61165\n     mean  median       uq      max neval cld\n 89.34946 88.0295 94.05625 137.9836   100   a\n 87.42233 85.1177 92.16820 113.2549   100   a\n\n\n\nx &lt;- t(pomp::reulermultinom(1e5, 100, rate=c(1,2), dt=0.05))\ny &lt;- reulermultinom_cpp(1e5, 100, rate=c(1,2), dt=0.05)\nxy &lt;- as.data.frame(cbind(x, y))\nnames(xy) &lt;- c(\"pomp_var1\", \"pomp_var2\", \"var1\", \"var2\")\napply(xy, 2, summary)\n\n        pomp_var1 pomp_var2     var1     var2\nMin.       0.0000   0.00000  0.00000  0.00000\n1st Qu.    3.0000   7.00000  3.00000  7.00000\nMedian     4.0000   9.00000  4.00000  9.00000\nMean       4.6421   9.29104  4.64224  9.28226\n3rd Qu.    6.0000  11.00000  6.00000 11.00000\nMax.      17.0000  22.00000 16.00000 25.00000\n\nlibrary(tidyr)\nxy |&gt; pivot_longer(cols=1:4, names_to=\"var\") -&gt; xylong\nlibrary(ggplot2)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nextrafont::loadfonts()\nggplot(xylong)+\n  geom_violin(aes(x=var, y=value))+\n  facet_wrap(~var, nrow=1, scales=\"free_x\")\n\n\n\n# ggsave(\"multinomial.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/stan_sir_transformed_parameters/index.html",
    "href": "posts/stan_sir_transformed_parameters/index.html",
    "title": "SIR model in Stan: Euler method",
    "section": "",
    "text": "SIR model in Stan\nI developed a SIR model and solved it an Euler method and generated a fake data as a sequence of noisy observation of daily incidence. I could have used the ODE solving routine available in Stan as in my previous post. However, an SIR model solved via the Euler method can be extended mor easily (e.g., stochatic model). I also suspect it would be easier to combine with the other statistical modeling techinques (e.g., hierarchical model), which I am going to post later.\n\nSimulate the data\nGenerate \\(y_{1:N}\\) as a sequence of noisy observations of a daily incidence. Almost the same Stan model is used twice: once to create fake data and the second time to estimate parameters via HMC.\n\n\nStan model to create fake data\n\nstan_code_data &lt;- \"\ndata {\n  int&lt;lower=0&gt; N; // length of the data\n  int&lt;lower=0&gt; iter_per_day;\n  real dt;\n  real&lt;lower=0&gt; S0;\n  real&lt;lower=0&gt; I0;\n  real&lt;lower=0&gt; R0;\n  real&lt;lower=0&gt; CI0;\n  real&lt;lower=0&gt; phi;\n  real&lt;lower=0&gt; gamma;\n  real&lt;lower=0&gt; beta;\n}\n\nparameters {\n\n}\n\n\ntransformed parameters {\n  vector&lt;lower=0&gt;[N] daily_inf;\n  vector&lt;lower=0&gt;[N+1] S;\n  vector&lt;lower=0&gt;[N+1] I;\n  vector&lt;lower=0&gt;[N+1] R;\n  vector&lt;lower=0&gt;[N+1] CI;\n  \n  real&lt;lower=0&gt; st; // susceptible at time t\n  real&lt;lower=0&gt; it;\n  real&lt;lower=0&gt; rt;\n  real&lt;lower=0&gt; cit;  // cumulative infections at time t\n  real&lt;lower=0&gt; n; // total population size\n  real&lt;lower=0&gt; n_si; // number moving from S to I\n  real&lt;lower=0&gt; n_ir; // number moving from I to R\n  \n  S[1] = S0;\n  I[1] = I0;\n  R[1] = R0;\n  CI[1] = CI0;\n    \n  for (i in 2:(N+1)) {\n    st = S[i-1];\n    it = I[i-1];\n    rt = R[i-1];\n    cit = CI[i-1];\n    for (j in 1:iter_per_day) {\n      n = st + it + rt;\n      n_si = dt * beta * st * it / n;\n      n_ir = dt * gamma * it;\n      st = st - n_si;\n      it = it + n_si - n_ir;\n      rt = rt + n_ir;\n      cit = cit + n_si;\n    }\n    S[i] = st;\n    I[i] = it;\n    R[i] = rt;\n    CI[i] = cit;\n  }\n\n  for (i in 2:(N+1)) {\n    daily_inf[i-1] = CI[i] - CI[i-1];\n  }\n}\n\nmodel {\n\n}\n\ngenerated quantities {\n  array[N] int y_sim;\n  for (i in 1:N) {\n    y_sim[i] = neg_binomial_2_rng(daily_inf[i] + 1e-6, phi);\n    //y_sim[i] = poisson_rng(daily_inf[i] + 1e-6);\n  }\n}\n\"\n\n\n\n\n\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\nmod_data &lt;- stan_model(model_code=stan_code_data, verbose=TRUE)\n\nN=31L \nS0=999\nI0=1\ndt=0.1\n\ndata = list(N=N,\n  S0=S0, I0=I0, R0=0, CI0=0, iter_per_day=round(1/dt),\n  phi=5000, beta=0.3, gamma=0.2, dt=dt)\n\nset.seed(42)\n\nfit = sampling(mod_data, data=data,\n                iter = 200,\n                chains = 1,\n                cores = 1, \n                algorithm = \"Fixed_param\")\n\ndf = as.data.frame(fit)\ny_sim = df[, grepl(\"^y_sim.*\", names(df))]\nplot(1:N, as.numeric(y_sim[1,]))\n\n# saveRDS(y_sim, \"stan_sir_daily_inc_NB.rds\")\n\n\nStan model to estimate parameters\nNote that there are two parameters in the parameters block\n\nstan_code_est &lt;- \"\ndata {\n  int&lt;lower=0&gt; N;\n  int&lt;lower=0&gt; iter_per_day;\n  real dt;\n  int&lt;lower=0&gt; y[N];\n  real&lt;lower=0&gt; S0;\n  real&lt;lower=0&gt; I0;\n  real&lt;lower=0&gt; R0;\n  real&lt;lower=0&gt; CI0;\n  real&lt;lower=0&gt; phi;\n  real&lt;lower=0&gt; r;\n}\n\nparameters {\n  real&lt;lower=0&gt; gamma;\n  real&lt;lower=0&gt; beta;\n}\n\ntransformed parameters {\n  vector&lt;lower=0&gt;[N] daily_inf;\n  vector&lt;lower=0&gt;[N+1] S;\n  vector&lt;lower=0&gt;[N+1] I;\n  vector&lt;lower=0&gt;[N+1] R;\n  vector&lt;lower=0&gt;[N+1] CI;\n  \n  real&lt;lower=0&gt; st; // susceptible at time t\n  real&lt;lower=0&gt; it;\n  real&lt;lower=0&gt; rt;\n  real&lt;lower=0&gt; cit;  // cumulative infections at time t\n  real&lt;lower=0&gt; n; // total population size\n  real&lt;lower=0&gt; n_si; // number moving from S to I\n  real&lt;lower=0&gt; n_ir; // number moving from I to R\n  \n  S[1] = S0;\n  I[1] = I0;\n  R[1] = R0;\n  CI[1] = CI0;\n    \n  for (i in 2:(N+1)) {\n    st = S[i-1];\n    it = I[i-1];\n    rt = R[i-1];\n    cit = CI[i-1];\n    for (j in 1:iter_per_day) {\n      n = st + it + rt;\n      n_si = dt * beta * st * it / n;\n      n_ir = dt * gamma * it;\n      st = st - n_si;\n      it = it + n_si - n_ir;\n      rt = rt + n_ir;\n      cit = cit + n_si;\n    }\n    S[i] = st;\n    I[i] = it;\n    R[i] = rt;\n    CI[i] = cit;\n  }\n\n  for (i in 2:(N+1)) {\n    daily_inf[i-1] = CI[i] - CI[i-1];\n  }\n}\n\n\nmodel {\n  beta ~ exponential(r);\n  gamma ~ exponential(r);\n\n  y ~ neg_binomial_2(daily_inf + 1e-6, phi);\n\n}\"\n\n\nlibrary(rstan)\noptions(mc.cores = parallel::detectCores())\nrstan_options(auto_write = TRUE)\n\nmod_est = stan_model(model_code=stan_code_est, verbose=TRUE)\n\ny_sim = readRDS(\"stan_sir_daily_inc_NB.rds\")\n\ny = as.integer(y_sim[1,])\ndata = list(N=length(y), y=y, S0=S0, I0=I0, R0=0, CI0=0, iter_per_day=round(1/dt), phi=50, r=0.1, dt=dt)\n\nset.seed(42)\n\nfit = sampling(mod_est, data=data,\n                iter = 2000,\n                chains = 4,\n                cores = min(parallel::detectCores(), 4))\n\n# saveRDS(fit, \"stan_sir_daily_inc_NB_fit.rds\")\n\nTrace plot\n\nlibrary(rstan)\nfit = readRDS(\"stan_sir_daily_inc_NB_fit.rds\")\ntraceplot(fit, c(\"beta\",\"gamma\"))\n\n\n\n\n\nPlot the results\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\ndf = as.data.frame(fit)\nd &lt;- df[, c(\"beta\",\"gamma\")]\ndlong &lt;- tidyr::pivot_longer(d, cols=c(\"beta\",\"gamma\"),\n                             names_to=\"param\")        \n# dlong$param &lt;- as.factor(dlong$param)\nlibrary(dplyr)\nggplot(dlong)+ \n  geom_histogram(aes(x=value))+\n  facet_wrap(~param, nrow=1, scales = \"free_x\")+\n  geom_vline(data=filter(dlong, param ==\"beta\"), aes(xintercept=0.3), color=\"firebrick\", linewidth=1.2) +\n  geom_vline(data=filter(dlong, param ==\"gamma\"), aes(xintercept=0.2), color=\"firebrick\", linewidth=1.2)\n\n\n\n# ggsave(\"sir_euler_stan_param.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/Idiosyncrasy_generality/index.html",
    "href": "posts/Idiosyncrasy_generality/index.html",
    "title": "Idiosyncrasies and generalities",
    "section": "",
    "text": "Idiosyncrasies and generalities\nDebates in the population ecology Bjørnstad and Grenfell.\n\nRelative importance of “noise” (small-scale, high-frequency stochastic influences) versus climatic forcing (larger scale, often lower-frequency signals) versus nonlinear interactions between individuals of the same or different species.\nThe impact of intrinsic (i.e., intraspecific) processes, as opposed to extrinsic or community-level interactions\nNested within 2, “dimensionality” of population fluctuations; given that most populations are embedded in rich communities and affected by numerous interspecific interactions, can simple (low-dimensional) models involving one or a few species capture the patterns of fluctuations?\n\n“… To understand any system, we need to appreciate its idiosyncrasies; to encompass broad patterns, we need to extract generalities…”\nViewing infectious disease epidemiology as a subset of ecology can be highly beneficial. By doing so, we can apply all the components that influence the dynamics of animal species to infectious disease dynamics, to varying degrees.\nIn the context of COVID-19, the modeling of confirmed cases has primarily focused on the variation in transmission rates. Often, the variation within reasonable limits in the transmission rate alone is sufficient to produce the daily number of cases. However, there is a potential risk of overlooking other influential factors, which could lead to biased estimates for transmission.\nBy considering infectious disease dynamics from an ecological perspective, we can broaden our understanding and ensure that relevant factors beyond transmission rates are taken into account. This approach can enhance the accuracy of our modeling and contribute to more effective public health interventions in combating diseases like COVID-19."
  },
  {
    "objectID": "posts/Euler-Lotka/index.html",
    "href": "posts/Euler-Lotka/index.html",
    "title": "Euler-Lotka equation",
    "section": "",
    "text": "Euler-Lotka equation\nThe following content was adapted from the Wikipedia article.\nThe Euler-Lotka equation is a fundamental concept in demographic analysis and population ecology, offering critical insights into the age structure and growth rate of populations. At its core, the equation connects the reproductive rates across different ages to the overall population growth rate.\nLet \\(B(t)dt\\) be the number of births during the time interval from \\(t\\) to \\(t+dt\\). Also define the survival function \\(l(a)\\), the fraction of individuals surviving to age \\(a\\). Finally define \\(b(a)\\) to be the birth rate for mothers of age \\(a\\). The product \\(B(t-a) l(a)\\) therefore denotes the number density of individuals born at \\(t-a\\) and still alive at \\(t\\), while \\(B(t-a)l(a)b(a)\\) denotes the number of births in this cohort, which suggest the following Volterra integral equation for \\(B\\): \\[\n\\begin{align}\nB(t) &= \\int_{a=0}^t B(t-a) l(a) b(a)\\\\\n\\end{align}\n\\]\nIf we assume an exponential solution \\(B(t)=Q e^{rt}\\), the equation becomes\n\\[\n\\begin{align}\nQe^{rt} &= \\int_{a=0}^t Qe^{r(t-a)} l(a) b(a)\\\\\n\\end{align}\n\\]\n\\[\n\\begin{align}\n1 = \\int_{a=0}^t e^{-ra} l(a) b(a)\\\\\n\\end{align}\n\\]\nIn R, we can simulate the Euler-Lotka equation to understand population dynamics better. The following code snippet provides a basic framework for this simulation:\n\n# Define age-specific fertility and survival rates\nages &lt;- seq(from = 15, to = 50, by = 1) # Age range\nfertility_rates &lt;- runif(length(ages), min = 0, max = 5) # Random fertility rates\nsurvival_rates &lt;- runif(length(ages), min = 0, max = 1) # Random survival rates\n\n# Euler-Lotka Equation Function\neuler_lotka &lt;- function(r) {\n    sum(survival_rates * fertility_rates * exp(-r * ages)) - 1\n}\n\n# Finding the intrinsic rate of natural increase\nlibrary(rootSolve)\nr_estimated &lt;- uniroot(euler_lotka, lower = 0, upper = 1)$root\n\n# Output the estimated intrinsic rate of natural increase\nprint(paste(\"Estimated r:\", r_estimated))\n\n[1] \"Estimated r: 0.140031822598998\""
  },
  {
    "objectID": "posts/convolution/index.html",
    "href": "posts/convolution/index.html",
    "title": "Convolution",
    "section": "",
    "text": "Convolution\nThe following content was adapted from the post in Grant Sanderson’s YouTube video and the Wikipedia article.\nConvolution is a fundamental concept in mathematics and statistics, playing a crucial role in various applications ranging from signal processing to probability theory. In this blog post, we’ll explore what convolution is, its significance, and how it’s used in mathematics and statistics. Additionally, I’ll include a practical example using R.\nConvolution is a mathematical operation that combines two functions to produce a third function. It’s a way of ‘mixing’ two functions, often used to understand the way a system modifies a signal. In mathematical terms, the convolution of two functions, \\(f\\) and \\(g\\), is defined as:\n\\[\n(f*g)(t) = \\int_0^{\\infty} f(\\tau)g(t-\\tau) \\textrm{d}\\tau\n\\] Let’s consider a simple example of convolution in R. We’ll convolve two functions, a sine wave and a cosine wave, to see how they interact.\n\n# Define the two functions\nf &lt;- function(x) sin(x)\ng &lt;- function(x) cos(x)\n\n# Create a sequence of points\nx &lt;- seq(-pi, pi, length.out = 100)\n\n# Perform the convolution\nconvolved &lt;- convolve(f(x), g(x), type = \"open\")\n\n# Plot the original functions and their convolution\nplot(x, f(x), type='l', col='blue', ylim=c(-50, 50))\nlines(x, g(x), col='red')\nlines(x, convolved[seq(1,199, length.out=100)], col='green')\nlegend(\"topright\", legend=c(\"f(x) = sin(x)\", \"g(x) = cos(x)\", \"Convolved\"), col=c(\"blue\", \"red\", \"green\"), lty=1)\n\n\n\n\nIn compartmental modeling, it can be used to explore the distribution and features of the consecutive compartments. For example, in the SEIR model, the the length of a generation interval is given by the convolution of E and I compartments."
  },
  {
    "objectID": "posts/generation_interval/index.html",
    "href": "posts/generation_interval/index.html",
    "title": "Generation interval, growth rate, reproduction number",
    "section": "",
    "text": "Growth rate, generation interval, and reproduction number\nWallinga and Lipsitch wrote a highly cited paper about the reproduction number. It discusses how to derive reproduction number, \\(R\\), given the growth rate, \\(r\\), and the generation interval, \\(T_c\\).\n\\[\n\\begin{align}\nR &= 1 + r/b\\\\\nR &= (1 + r/b_1)(1 + r/b_2)\\\\\nR &= \\frac{(1 + r/b_1)^x}{\\sum_{i=1}^y(1 + r/b_2)^{-i}}\\\\\n\\end{align}\n\\]\nI can seem to use the third equation to reproduce the answer to the example the authors provided. On page 603, the authors gave the example of Influenza A where the generation interval has a mean of 2.85 days with the standard deviation of 0.93 days. The epidemic growth rate \\(r = 0.2\\). “The \\(R=1.57\\) for the SIR epidemic model, a value of \\(R=1.65\\) for the SEIR epidemic model and a value of \\(R=1.66\\) the more complicated epidemic model with one latent stage and two infectious stages (equation (3.3), with \\(x=1, y=2\\))”\nI tried to reproduce the results.\n\nTc = 2.85\nsigma = 0.93\nr = 0.2\n# for the SIR model\nb = 1/Tc\n(R = 1 + r/b)\n\n[1] 1.57\n\n# for the SEIR model, assume that b1 and b2 are the same\nb1 = b2 = 2/Tc\n(R = (1 + r/b1)*(1 + r/b2))\n\n[1] 1.651225\n\n\nFor the model with \\(x=1, y=2\\), the \\(R\\) is not the same as provided. \\(R\\) can vary depending on how we set \\(b_1\\) and \\(b_2\\), but it is smaller than one.\n\n# for the SEIR model with x=1, y=2 (i.e., two consecutive . assume that b1 and b2 are the same\nx = 1\ny = 2\nb1 = 2/Tc\nb2 = 4/Tc\n\nnumer = function(x) (1 + r/b1)^x\ndenom = function(y) sum(sapply(1:y, function(i) (1+r/b2)^(-i)))\n\n(R = numer(x=x)/denom(y=y))\n\n[1] 0.7828791\n\n\nThe study also refers to Wearing et al. (2005) study and the result is below. Again, it is not \\(R=1.66\\)\n\n# as presented in Wearing et al.(2005)\n\nm = 1\nn = 2\nb1 = 2/Tc\nb2 = 4/Tc\n\nnumer = r*(r/(b1*m)+1)^m\ndenom = b2*(1-(r/(b2*n)+1)^(-n))\n(R = numer / denom)\n\n[1] 1.423909\n\n\nI implemented moment generating function to see if I can reproduce the results this way. One important aspect is that how we should set the rate to get the generation interval we want.\n\nmgf = function(b, r) b / (b + r)\n\n1/mgf(b=1/Tc, r=r)\n\n[1] 1.57\n\nmgf_recursion = function(c, b, r){\n  l = length(b)\n  if (l == 1) {\n    c * mgf(b, r)\n  } \n  else {\n    c[l] * mgf(b[l], r) * mgf_recursion(c=rep(1,length(b[1:(l-1)])), b=b[1:(l-1)], r=r)\n  }\n}\n\nc = c(0,1/2,1/2)\nb = c(3,3,3)/Tc\n\nR0_recursion = function(c,b,r){\n  out &lt;- rep(NA, length(b))\n  for (i in seq_along(b)) {\n    out[i] = mgf_recursion(c[1:i], b[1:i], r)\n  }\n  1/sum(out)\n}\n\nR0_recursion(c=c(1), b=c(1)/Tc, r=0.2)\n\n[1] 1.57\n\nR0_recursion(c=c(0,1), b=c(2,2)/Tc, r=0.2)\n\n[1] 1.651225\n\n# for y=2 (i.e., Gamma distribution with shape=2)\n# use the relationship \n# average time to infection = beta*(1+alpha)/2 where beta and alpha represent scale and shape\n# beta*(1+alpha)/2 = Tc/2\n# rate = b = (1+alpha)/Tc\nR0_recursion(c=c(0,1/2,1/2), b=c(2,3,3)/Tc, r=0.2)\n\n[1] 1.661816"
  },
  {
    "objectID": "posts/generation_interval1/index.html",
    "href": "posts/generation_interval1/index.html",
    "title": "Generation interval",
    "section": "",
    "text": "Generation interval = incubation period + infectious period?\nAlthough not published, I wrote a correspondence to Lancet to commenting the article. In the article, the authors stated that the generation interval is the sum of the incubation period and the infectious period. I argued that this statement holds only for a constant rate of transmission during the exponentially distributed infectious period (i.e., only a very limited case). Then, I showed that the generation interval varies if the infectious period distribution is different from the exponential distribution even with the constant rate of transmission. Following is the whole message. I now think I am glad that it was not published because I realized that Svensson wrote an article in 2007 to show a similar message in a more rigorous and general way. At least the message and arguments I presented were correct.\n\n\nThe main text\nWhen confronted with a novel disease like the coronavirus disease 2019 (COVID-19), the incubation period and the serial interval are two critical epidemiological variables that are measured during the early stages of an outbreak. A transmission model can be constructed based on these variables to project potential scenarios of outbreak expansion or resolution and, importantly, define the basic reproduction number (\\(R_0\\)) for the disease. The recent article by Wu et al., published in The Lancet on January 31 of this year, one month after the first case was reported in Wuhan, China, uses a transmission model parameterized by data available at the time. This study provided important and timely insights into the spread of COVID-19 in China and elsewhere, concluding that, in addition to Wuhan, other major cities in China and well-connected cities outside China were likely to already have sustained localized outbreaks. The model utilized by Wu et al. makes a key statement, which has limited applications and may lead to unjustified assumptions in subsequent model iterations. The dynamic model requires an input on the length of the infectious period, which Wu et al. derived by assuming that the serial interval is equal to the sum of the infectious period and the latent period (on page 4 of the article by Wu et al.). Neither Wu et al. nor the study to which they refer provides a clear rationale for this statement. The serial interval represents the time between the clinical onset of successive cases [3] and it is naturally expressed as the sum of the incubation period and disease age at transmission* 4. Here we treat the incubation period (the time between infection and the onset of symptoms) as the same as latent period (the time between infection and the onset of infectiousness), for simplicity. The disease age at transmission represents the time between the onset of symptoms of a primary case and the time of infection of the associated secondary cases. Therefore, for the mean serial interval to be the same as the sum of mean incubation period and mean infectious period, the mean disease age at transmission must be equal to the mean infectious period. The mean disease age at transmission is indeed as long as the mean of the exponentially-distributed infectious period, which is often used for convenience in building a differential equation-based model (Table 1). However, for other distributions we explored, the mean disease age at transmission is shorter than mean infectious period, and it is only half of the mean infectious period† when the infectious period is constant. The gamma-distributed infectious period can also be easily adopted in a differential-equation based model to represent a more realistic representation of infectious period and in this case, following Wu et al.’s recipe will lead to a shorter serial interval than is intended. Moreover, each of the aforementioned arguments holds when transmission occurs at a constant rate over the infectious period, and violating this assumption of a constant rate (such as when viral load is high during the initial part of the infectious period and tapers off over time 56 will likely induce an even shorter disease age at transmission.\nThe assumption that the serial interval is the sum of incubation (or latent) period and infectious period seems to hold only when the infectious period is exponentially distributed for a constant transmission rate. For a wider application, one has to calculate the disease age at transmission that leads to a correct serial interval for a given distribution of the infectious period. We believe this is an important point and potentially unjustified assumptions in subsequent model iterations may result in providing outbreak responders with inaccurate projections on the characteristics of disease transmission.\nTable 1. Time to infection since symptom onset under infectious period with different distributions.\n\nTable 1. Time to infection since symptom onset under infectious period with different distributions.\n\n\n\n\n\n\n\n\nInfectious period\nTime to transmission (disease age)\n\n\nDistribution\nExpectation\nSample mean‡ [\\(2.5^{th}\\), \\(97.5^{th}\\) percentile]\nExpectation§\n\n\n\n\nExponential\n5\n5\n5\n\n\nGamma(2,5/2)¶\n5\n3.73 [3.75, 3.71]\n3.75\n\n\nUniform(0,10)\n5\n3.34 [3.32, 3.35]\n3.33\n\n\nWeibull(2,5/2)\n5\n3.17 [3.16, 3.19]\n3.18\n\n\nLognormal(2,5/2)\n5\n2.90 [2.89, 2.91]\n2.90\n\n\nConstant at 5\n5\n2.51 [2.49, 2.52]\n2.50\n\n\n\nFootnotes *Taking statistical distribution into account, we can write the serial interval at time \\(t\\) as the convolution of the distributions of disease age, \\(g\\), and incubation period, \\(f\\): \\(s(t)= \\int_0^t g(t-\\tau)f(\\tau)\\textrm{d}\\tau\\). † Interestingly, the Wikipedia article says that the length of a serial interval is equal to the sum of incubation period and the half of the infectious period 7. ‡To simulate disease age, we drew a random infectious period from each distribution and let transmission occur at a constant probability with R0 of 2.2 over the infectious period. We repeated the process 100000 times to calculate a mean and a \\(2.5^{th}\\) and \\(95^{th}\\) percentiles. §The probability density function for the disease age at transmission at time \\(t\\) may be obtained by \\(g(t) = \\frac{P(X&gt;t)}{∫_0^∞ P(X&gt;t)dt} ~\\textrm{for}~ t∈(0,+∞)\\) for infectious period \\(X\\). The expected value can then be calculated as \\(\\int_0^{\\infty} t g(t)\\textrm{d}t\\). ¶Gamma(shape, rate). This distribution can be produced by assuming two successive compartments with the exit rate of 2/5."
  },
  {
    "objectID": "posts/generation_interval1/index.html#table-1.-time-to-infection-since-symptom-onset-under-infectious-period-with-different-distributions.",
    "href": "posts/generation_interval1/index.html#table-1.-time-to-infection-since-symptom-onset-under-infectious-period-with-different-distributions.",
    "title": "Generation interval",
    "section": "Table 1. Time to infection since symptom onset under infectious period with different distributions.",
    "text": "Table 1. Time to infection since symptom onset under infectious period with different distributions."
  },
  {
    "objectID": "posts/generation_interval1/index.html#infectious-period-disease-age-at-transmission",
    "href": "posts/generation_interval1/index.html#infectious-period-disease-age-at-transmission",
    "title": "Generation interval",
    "section": "Infectious period | Disease age at transmission",
    "text": "Infectious period | Disease age at transmission\nDistribution | Mean | Sample mean [95% confidence interval]‡ | Expected value§| Exponential (mean=5) | 5 | 4.98 [4.95, 5.01] | 5.00 Gamma (mean=5,sd¶=3.5)ǁ | 5 | 3.73 [3.75, 3.71] | 3.75 Uniform[0, 10] | 5 | 3.34 |[3.32, 3.35] | 3.33 Weibull (mean=5, sd=2.6)| 5 | 3.17 [3.16, 3.19] | 3.18 Lognormal (mean=5, sd=2)| 5 | 2.90 [2.89, 2.91] | 2.90 Constant at 5 | 5 | 2.51 [2.49, 2.52] | 2.50 ————————————————————— Footnotes *Taking statistical distribution into account, we can write the serial interval at time \\(t\\) as the convolution of the distributions of disease age, \\(g\\), and incubation period, \\(f\\): \\(s(t)= \\int_0^t g(t-\\tau)f(\\tau)\\textrm{d}\\tau\\). † Interestingly, the Wikipedia article says that the length of a serial interval is equal to the sum of incubation period and the half of the infectious period 7. ‡To simulate disease age, we drew a random infectious period from each distribution and let transmission occur at a constant probability with R0 of 2.2 over the infectious period. We repeated the process 100000 times to calculate a mean and a 95% confidence interval. §The probability density function for the disease age at transmission at time t may be obtained by \\(g(t) = \\frac{P(X&gt;t)}{∫_0^∞ P(X&gt;t)dt} ~\\textrm{for}~ t∈(0,+∞)\\) for infectious period X. The expected value can then be calculated as \\(\\int_0^{\\infty} t g(t)\\textrm{d}t\\). ¶sd=standard deviation ǁThis is the same as Gamma(shape=2, rate=2/5) and is produced by assuming two successive compartments with the exit rate of 2/5. ```"
  },
  {
    "objectID": "posts/critical_vacc_threshold/index.html",
    "href": "posts/critical_vacc_threshold/index.html",
    "title": "Critical vaccination threshold",
    "section": "",
    "text": "The following article by Fine provides a great introduction to the critical vaccination threshold.\n\nCritical vaccination threshold\nThe simplest scenario would be to assume that individuals are well-mixed and vaccine recipients are completely protected from infection. For this scenario, the critical threshold for random vaccination, \\(V_c\\), is as follows: \\[\nV_c = 1-1/R_0\n\\] If vaccines are only partially protective and \\(E\\) fraction of the vaccine recipients are completely protected, \\(V_c\\) becomes as follows: \\[\nV_c = \\frac{1-1/R_0}{E}\n\\]\nI used the population World Population Prospects 2022.\n\n\nPlot the results\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nR0 &lt;- seq(1,5,length.out=100)\nVE &lt;- c(0.4, 0.6, 0.8)\nVc &lt;- lapply(VE, function(x) ifelse((1-1/R0)/x &lt;= 1.0, 100*(1-1/R0)/x, NA))\n\ndf &lt;- data.frame(R0=R0, Vc1=Vc[[1]], Vc2=Vc[[2]], Vc3=Vc[[3]])\n\ngg &lt;- ggplot(df, aes(x = R0)) +\n  geom_rect(aes(xmin=1.15, xmax=2.78, ymin=0, ymax=max(df$Vc3)),\n                   fill = \"pink\", alpha=0.01)+\n  geom_line(aes(y=Vc3, linetype=\"80%\")) +\n  geom_line(aes(y=Vc2, linetype=\"60%\")) +\n  geom_line(aes(y=Vc1, linetype=\"40%\")) +\n  scale_linetype_manual(\"Vaccine efficacy\", values=c(\"80%\"=\"solid\",\n                                   \"60%\"=\"dashed\", \n                                   \"40%\"=\"dotted\"))+\n  # labs(y=\"Critical vaccination threshold\", x=expression(R[0])) +\n  labs(y=expression(paste(\"Critical vaccination threshold (%), \", V[C])), \n       x=expression(paste(\"Basic reproduction number, \", R[0]))) +\n  theme(text = element_text(size=16),\n        axis.text = element_text(size=16),\n        legend.text=element_text(size=15), \n        legend.position = \"bottom\")\n\ngg\n\n\n\n# ggsave(\"Vc_R0.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)\n\n\nR0 &lt;- c(1.15, 2.78)\n(Vc &lt;- 1-1/R0)\n\n[1] 0.1304348 0.6402878\n\n\n\n\nIndia simulation\n\nlibrary(data.table)\nlibrary(readxl)\nlibrary(dplyr)\n# d &lt;- read_xlsx(\"C:/Users/jonghoon.kim/Documents/myblog/posts/critical_vacc_threshold/WPP2022_POP_F01_1_POPULATION_SINGLE_AGE_BOTH_SEXES.xlsx\", sheet= \"Medium variant\")\n# d2 &lt;- d[-(1:11),]\n# india &lt;- d2[(d2$...3 == \"India\" & d2$...11 &gt; 2023),]\n# names(india) &lt;- c(1:2, \"country\", 4:10 ,\"year\", paste0(\"age_\", 0:100))\n\n# saveRDS(india, \"C:/Users/jonghoon.kim/Documents/myblog/posts/critical_vacc_threshold/WPP2022_India.rds\")\n\nindia &lt;- readRDS(\"C:/Users/jonghoon.kim/Documents/myblog/posts/critical_vacc_threshold/WPP2022_India.rds\")\nindia$year &lt;- as.numeric(india$year)\n# india[,paste0(\"age_\", 0:100)] &lt;- as.numeric(india[,paste0(\"age_\", 0:100)])\nindia &lt;- india %&gt;% mutate_at(paste0(\"age_\", 0:100), as.numeric)\n\nyrs &lt;- 2024:2100\n# simulate\nVE &lt;- c(0.4, 0.6, 0.8)\nprop_immune &lt;- rep(NA, length(yrs))\n\nfor(i in seq_along(yrs)) {\n  numerator &lt;- sum(india[india$year == yrs[i], paste0(\"age_\", 0:(i-1))])\n  denominator &lt;- sum(india[india$year == yrs[i], paste0(\"age_\", 0:100)])\n  prop_immune[i] &lt;- numerator / denominator\n} \n\nexisting_immune &lt;- 20\npop_immune_ve &lt;- lapply(VE, function(z) 100*z*prop_immune)\n# add the existing immunity, assumed to be 15%\npop_immune_ve_added &lt;- lapply(VE, function(z) 100*z*prop_immune + existing_immune)\n\ndf &lt;- data.frame(years=yrs-2024, \n                 pi1=pop_immune_ve[[1]], \n                 pi2=pop_immune_ve[[2]], \n                 pi3=pop_immune_ve[[3]], \n                 pi4=pop_immune_ve_added[[1]], \n                 pi5=pop_immune_ve_added[[2]], \n                 pi6=pop_immune_ve_added[[3]])\n\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\n\ngg &lt;- ggplot(df, aes(x = years)) +\n  geom_rect(aes(xmin=0, xmax=30, ymin=14, ymax=63),\n                   fill = \"pink\", alpha=0.01)+\n  geom_line(aes(y=pi3, linetype=\"80%\")) +\n  geom_line(aes(y=pi2, linetype=\"60%\")) +\n  geom_line(aes(y=pi1, linetype=\"40%\")) +\n  geom_line(aes(y=pi6, linetype=\"80%\"), color=\"dark green\") +\n  geom_line(aes(y=pi5, linetype=\"60%\"), color=\"dark green\") +\n  geom_line(aes(y=pi4, linetype=\"40%\"), color=\"dark green\") +\n  scale_x_continuous(limits=c(0,30))+\n  scale_linetype_manual(\"Vaccine efficacy\", \n                        values=c(\"80%\"=\"solid\",\n                                 \"60%\"=\"dashed\", \n                                 \"40%\"=\"dotted\"))+\n  # labs(y=\"Critical vaccination threshold\", x=expression(R[0])) +\n  labs(y=\"Population immune (%)\", x=\"Years since vaccination\") +\n  theme(text = element_text(size=16),\n        axis.text = element_text(size=16),\n        legend.text=element_text(size=15), \n        legend.position = \"bottom\")\ngg\n\n\n\n# ggsave(\"immune_time.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/implementing_vaccination_single_dose/index.html",
    "href": "posts/implementing_vaccination_single_dose/index.html",
    "title": "Implmenting vaccination in a static vaccine impact model",
    "section": "",
    "text": "Vaccination: single dose regimen\nMany of the previous implementation about vaccination was just about two-dose regiment and it was only needed to set the two-dose vaccine efficacy was introduced.\nHowever, now the challenge is two doses separately and so population will exist in three separate states: susceptible, one-dose recipients, and two-dose recipients. The intricate aspect is that two-dose recipients will be the definition who exhibit two-dose efficacy rather than the operational definition who could imply those who received two doses. Individuals who receive two doses with enough\nOne critical aspect is that two-dose recipients experience protection level observed in clinical trials, i.e., ~65% for around 3 years after infection.\n\nPE &lt;- c(0.3, 0.6)\nrate_wane &lt;- c(1/2, 1/5) # per year\ncov1 &lt;- c(0.8, rep(0,19), 0.8, rep(0,29)) \ncov2 &lt;- c(0.8, rep(0,19), 0.8, rep(0,29)) \ninterval &lt;- 2*7/365 # 2 weeks\nU &lt;- c(1000, rep(0, 49))\nV1 &lt;- rep(0, 50)\nV2 &lt;- rep(0, 50)\n# vaccines introduced in the first year\nU[1] = U[1] - U[1]*cov1[1]\nV1[1] = V1[1] + U[1]*cov1[1] - V1[1]*cov2[1]\nV2[1] = V2[1] + V1[1]*cov2[1]\n\nfor (i in seq_along(U)[-1]) {\n  U[i] = U[i-1] - U[i-1]*cov1[i] + V1[i-1]*rate_wane[1] + V2[i-1]*rate_wane[2]\n  V1[i] = V1[i-1] - V1[i-1]*rate_wane[1] + U[i-1]*cov1[i]\n  V2[i] = V2[i-1] - V2[i-1]*rate_wane[2] + V1[i-1]*cov2[i]\n}\n\n\n\n\n\\[\n\\begin{align}\n\\mathrm{d}S/\\mathrm{d}t &= -\\beta I S \\\\\n\\mathrm{d}I/\\mathrm{d}t &= \\beta I S - \\gamma I\\\\\n\\end{align}\n\\] The first part is simply to compute \\(dI/dS\\).\n\nve_year = c(0.65,0.59,0.52,0.43,0.32,0.19,0.0)\nve_year_frac = c(0.65,0.59,0.52,0.43,0.32,0.19,0.0)/ve_year[1]\n\nssq = function(par) {\n  # -sum(dexp(ve_year_frac, rate=par, log=TRUE))\n  t=0:5\n  sum((ve_year_frac[1:6] - exp(-par*t))^2)\n}\npar = 0.5\nres = optimize(ssq, lower=0, upper=10)\nt=0:6\nexp(-res$minimum*t)\n\n[1] 1.0000000 0.8421417 0.7092026 0.5972491 0.5029684 0.4235707 0.3567065\n\nve_year_frac\n\n[1] 1.0000000 0.9076923 0.8000000 0.6615385 0.4923077 0.2923077 0.0000000"
  },
  {
    "objectID": "posts/universal_differential_eqn/index.html",
    "href": "posts/universal_differential_eqn/index.html",
    "title": "Universal differential equation",
    "section": "",
    "text": "Universal differential equation\nFollwoing python codes were copied from the GitHub site\n\nlibrary(reticulate)\npy_install(\"tensoflow\")\npy_install(\"typing\")\n\n\n# Author: Mattia Silvestri\n\"\"\"\n    Implementation of the Euler method in Tensorflow and related utility methods.\n\"\"\"\nimport sys\nimport tensorflow as tf\n# import tensorflow_probability as tfp\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, InputLayer\nimport numpy as np\nfrom typing import Tuple, List, Union\nfrom tensorflow.keras import backend as K\nimport matplotlib.pyplot as plt\n\ntf.executing_eagerly()\ntf.config.run_functions_eagerly(True)\n\n###############################################################################\n\n\ndef build_model(input_size: int,\n                output_size: int,\n                hidden: list = [],\n                output_activation: str = 'exponential',\n                name: str = None) -&gt; tf.keras.Model:\n    # Build all layers\n    nn_in = tf.keras.Input(shape=(input_size,))\n    nn_out = nn_in\n    for h in hidden:\n        nn_out = tf.keras.layers.Dense(h, activation='relu')(nn_out)\n    nn_out = tf.keras.layers.Dense(output_size, activation=output_activation)(nn_out)\n    # Build the model\n    model = tf.keras.Model(inputs=nn_in, outputs=nn_out, name=name)\n    return model\n\n##############################################################################\n\n\ndef SIR(y, beta, gamma=1 / 10):\n    \"\"\" function performing SIR simulation \"\"\"\n    S, I, R = y[:, 0:1], y[:, 1:2], y[:, 2:3]\n\n    S_to_I = beta * I * S\n    I_to_R = gamma * I\n    dS = - S_to_I\n    dI = S_to_I - I_to_R\n    dR = I_to_R\n\n    return tf.concat([dS, dI, dR], axis=1)\n\n###############################################################################\n\n\ndef SEIR(y, beta, epsilon=1 / 5, gamma=1 / 10):\n    \"\"\" function performing SIR simulation \"\"\"\n    S, E, I, R = y[:, 0:1], y[:, 1:2], y[:, 2:3], y[:, 3:4]\n\n    S_to_E = beta * I * S\n    E_to_I = epsilon * E\n    I_to_R = gamma * I\n    dS = - E_to_I\n    dE = S_to_E - E_to_I\n    dI = E_to_I - I_to_R\n    dR = I_to_R\n\n    return tf.concat([dS, dE, dI, dR], axis=1)\n\n#############################################################################\n\n\nclass PairedSIR(tf.keras.Model):\n    \"\"\"\n        Method for fitting beta parametrization of a SIR\n        covering a specific time window\n    \"\"\"\n\n    def __init__(self,\n                 steps: int,\n                 gamma: float,\n                 stochastic: bool = False):\n\n        \"\"\"\n        Constructor of the class.\n        :param T int; lenght of historical data (in days)\n        :param steps int; the number of steps of the Euler method.\n        :param gamma float; recovery rate of SIR model\n        :param stochastic bool: flag to indicate stochastic training\n        :param mu float: learning rate for lagrangian smoothing coefficient\n        \"\"\"\n\n        super(PairedSIR, self).__init__()\n\n        self.steps = steps\n        self.gamma = gamma\n        self.stochastic = stochastic\n\n        # Parameters\n        self.initializer = tf.keras.initializers.RandomUniform(minval=0, seed=42)\n        # shape_betas = (self.T, 2) if self.stochastic else (self.T,)\n        self.beta = tf.Variable(self.initializer(shape=(1,), dtype='float32'), trainable=True)\n\n        if self.stochastic:\n            self.std = tf.Variable(self.initializer(shape=(1,), dtype='float32'),\n                                   trainable=True,\n                                   constraint=lambda x: tf.clip_by_value(x, 0, 1.))\n\n        # Trackers\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n\n    def call(self, data, training=False):\n        y0 = data\n        S, I, R = y0[:, 0], y0[:, 1], y0[:, 2]\n        h = 1 / self.steps  # update step euler method\n\n        # if the model is stochastic we sample beta\n        if self.stochastic:\n            # mean, std = self.betas[:,0], self.betas[:,1]\n            epsilon = tf.random.normal((1,), mean=0.5, stddev=0.5)\n            beta = (self.beta + self.std * epsilon)\n        else:\n            beta = self.beta\n\n        # SIR integration\n        for _ in range(self.steps):\n            S_to_I = h * (beta * I * S)\n            I_to_R = h * self.gamma * I\n            S = S - S_to_I\n            I = I + S_to_I - I_to_R\n            R = R + I_to_R\n        # TODO - improve code here, there's a way to avoi\n        # all these append and reshape\n        S = tf.reshape(S, [-1, 1])\n        I = tf.reshape(I, [-1, 1])\n        R = tf.reshape(R, [-1, 1])\n        return tf.concat([S, I, R], 1)\n\n    def train_step(self, data):\n        y0, yt = data\n\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch([self.beta])\n            if self.stochastic:\n                tape.watch([self.std])\n            y_pred = self(y0, training=True)\n            loss = self.compiled_loss(y_pred, yt)\n\n            # Compute Gradient\n        grads = tape.gradient(loss, [self.beta])\n        grads_and_vars = list(zip(grads, [self.beta]))\n        self.optimizer.apply_gradients(grads_and_vars)\n\n        if self.stochastic:\n            grads_std = tape.gradient(loss, [self.std])\n            grads_and_vars_std = list(zip(grads_std, [self.std]))\n            self.optimizer.apply_gradients(grads_and_vars_std)\n\n        self.loss_tracker.update_state(loss)\n\n        return {\"loss\": self.loss_tracker.result()}\n\n    @property\n    def metrics(self):\n        return [self.loss_tracker]\n\n###############################################################################\n\n\nclass UnrolledSIR(tf.keras.Model):\n    \"\"\"\n        Method for fitting beta parametrization of a SIR\n        covering a specific time window\n    \"\"\"\n\n    def __init__(self,\n                 T: int,\n                 steps: int,\n                 gamma: float,\n                 stochastic: bool = False,\n                 mu: float = 1e-1):\n\n        \"\"\"\n        Constructor of the class.\n        :param T int; lenght of historical data (in days)\n        :param steps int; the number of steps of the Euler method.\n        :param gamma float; recovery rate of SIR model\n        :param stochastic bool: flag to indicate stochastic training\n        :param mu float: learning rate for lagrangian smoothing coefficient\n        \"\"\"\n\n        super(UnrolledSIR, self).__init__()\n\n        self.T = T\n        self.steps = steps\n        self.gamma = gamma\n        self.stochastic = stochastic\n        self.mu = mu\n\n        # Parameters\n        self.initializer = tf.keras.initializers.RandomUniform(minval=0.01, maxval=0.04, seed=42)\n        # shape_betas = (self.T, 2) if self.stochastic else (self.T,)\n        self.beta = tf.Variable(self.initializer(shape=(1,), dtype='float32'), trainable=True)\n\n        if self.stochastic:\n            self.std = tf.Variable(self.initializer(shape=(1,), dtype='float32'), trainable=True)\n\n        # Trackers\n        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n\n    def call(self, data, training=False):\n        y0 = data\n        S, I, R = [y0[0]], [y0[1]], [y0[2]]\n        h = 1 / self.steps  # update step euler method\n\n        # if the model is stochastic we sample beta\n        if self.stochastic:\n            # mean, std = self.betas[:,0], self.betas[:,1]\n            epsilon = tf.random.normal(shape=(1,), mean=0.5, stddev=0.5)\n            beta = (self.beta + self.std * epsilon)[0]\n        else:\n            beta = self.beta[-1]\n        for _ in range(self.T - 1):\n            S_t, I_t, R_t = S[-1], I[-1], R[-1]\n\n            # SIR integration\n            for _ in range(self.steps):\n                S_to_I = h * (beta * I_t * S_t)\n                I_to_R = h * self.gamma * I_t\n                S_t = S_t - S_to_I\n                I_t = I_t + S_to_I - I_to_R\n                R_t = R_t + I_to_R\n\n            # TODO - improve code here, there's a way to avoi\n            # all these append and reshape\n            S = tf.concat([S, [S_t]], 0)\n            I = tf.concat([I, [I_t]], 0)\n            R = tf.concat([R, [R_t]], 0)\n        S = tf.reshape(S, [-1, 1])\n        I = tf.reshape(I, [-1, 1])\n        R = tf.reshape(R, [-1, 1])\n\n        return tf.concat([S, I, R], 1)[1:]  # we remove the first examle y0\n\n    def train_step(self, data):\n        y0, yt = data[0], data[1:]\n\n        with tf.GradientTape(persistent=True) as tape:\n            tape.watch([self.beta])\n            if self.stochastic:\n                tape.watch([self.std])\n            y_pred = self(y0, training=True)\n            loss = self.compiled_loss(y_pred, yt)\n\n            # Compute Gradient\n        grads = tape.gradient(loss, [self.beta])\n        grads_and_vars = list(zip(grads, [self.beta]))\n        self.optimizer.apply_gradients(grads_and_vars)\n\n        if self.stochastic:\n            grads_std = tape.gradient(loss, [self.std])\n            grads_and_vars_std = list(zip(grads_std, [self.std]))\n            self.optimizer.apply_gradients(grads_and_vars_std)\n\n        self.loss_tracker.update_state(loss)\n\n        return {\"loss\": self.loss_tracker.result()}\n\n    @property\n    def metrics(self):\n        return [self.loss_tracker]\n\n##############################################################################\n\n\nclass EndtoEndIntervetionMap(tf.keras.Model):\n    \"\"\"\n        Method to approximate a compartmental model given\n        a set of interventions or NPIs\n    \"\"\"\n\n    def __init__(self,\n                 u_ode,\n                 f_ode,\n                 steps: int = 2,\n                 window_size: int = 7,\n                 params: dict = {'gamma': 1 / 10},\n                 stochastic: bool = False,\n                 mu: float = 1e-2):\n        \"\"\"\n        Constructor of the class.\n        :param u_ode; neural net mappoing interventions and beta\n        :para f_ode; compartmental model function\n        :param steps int; the number of steps of the Euler method.\n        :param window_size int; lenght in days for each time step cosidered\n        :param params dict; set of fixed parameters of the compartmental model\n        :param stochastic bool: flag to indicate stochastic training\n        :param mu float: learning rate for lagrangian smoothing coefficient\n        \"\"\"\n\n        super(EndtoEndIntervetionMap, self).__init__()\n\n        self.u_ode = u_ode\n        self.f_ode = f_ode\n        self.steps = steps\n        self.window_size = window_size\n        self.params = params\n        self.stochastic = stochastic\n        self.mu = mu\n\n        # Trackers\n        self.loss_tracker = tf.keras.metrics.Mean()\n        self.val_tracker = tf.keras.metrics.Mean()\n\n    def call(self, data, training=False):\n        y, x = data\n        for layer in self.layers:\n            x = layer(x)\n        # computing beta if stochastic\n        self.betas = x\n        self.x = x\n        if self.stochastic:\n            ## TODO !!! samples negative values\n            self.mean, self.std = x[:, 0], x[:, 1]\n            epsilon = tf.random.normal(shape=(len(self.mean),), mean=.5, stddev=.5)[0]\n            self.betas = tf.reshape(self.mean + self.std * epsilon, [-1, 1])\n        # integration of compartmental model\n        for _ in range(self.window_size * self.steps):\n            y += 1 / self.steps * self.f_ode(y, self.betas, **self.params)\n        return tf.cast(y, dtype=tf.float64)\n\n    def train_step(self, data):\n        # Unpack the data\n        (y0, x), yt = data\n\n        with tf.GradientTape() as tape:\n            # Watch variables\n            y_pred = self((y0, x), training=True)  # Forward pass\n            # Compute the loss value\n            loss = self.compiled_loss(yt[:, 1], y_pred[:, 1])\n\n            # Compute gradients\n        gradients = tape.gradient(loss, self.trainable_variables)\n        # Update weights\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        # Compute losses\n        self.loss_tracker.update_state(loss)\n\n        return {\"loss\": self.loss_tracker.result()}\n\n    def test_step(self, data):\n        (y0, x), yt = data\n\n        y_pred = self((y0, x))\n        loss = self.compiled_loss(yt[:, 1], y_pred[:, 1])\n        self.val_tracker.update_state(loss)\n\n        return {'loss': self.val_tracker.result()}\n\n    def _tensor(self, xs, dtype=tf.float64):\n        return [tf.convert_to_tensor(x, dtype) for x in xs]\n\n    @property\n    def metrics(self):\n        return [self.loss_tracker, self.val_tracker]```"
  },
  {
    "objectID": "posts/counterfactual_stochastic/index.html",
    "href": "posts/counterfactual_stochastic/index.html",
    "title": "Stochastic simulation in counterfactual scenarios",
    "section": "",
    "text": "Counterfactual analysis in epidemiology\nAt the Xth epidemics conference, I came across this . I was aware of the study but … I was also aware of the issue but treated it as natural results arising from stochasticity of the system we are modeling. However, I realized that stochasticity might cause those flutuations but that is not exactly counterfactual.\nstate-based PRNG sequential access u() - state &lt;- advance(state) - number &lt;- extract(state) - return (number/max_number)\nkey-based PRNG random access u(t, i, trans) - key &lt;- t | i | trans - key &lt;- advance(key, seed) - number &lt;- extract(key) - return (number/max_number)\nThe following example was adapted from the post in RPubs.\n\nSimulate the data\nGenerate \\(y_{1:T}\\) as a sequence of noisy observations of a latent variable \\(x_{1:T}\\).\n\\[\n\\begin{align}\n\\mathrm{d}S/\\mathrm{d}t &= -\\beta I S \\\\\n\\mathrm{d}I/\\mathrm{d}t &= \\beta I S - \\gamma I\\\\\n\\end{align}\n\\] The first part is simply to compute \\(dI/dS\\).\n\nfrom sympy import *\n\nR_0, b, g, dIdt, dSdt, S, I = symbols('R_0 b g dIdt dSdt S I')\n\ndSdt = - b*S*I\ndIdt = + b*S*I - g*I\ndSdI = dIdt / dSdt #-(I*S*b - I*g)/(I*S*b)\n\n# b &lt;- R0*g\nsimplify(-(I*S*R_0*g - I*g)/(I*S*R_0*g)) \n\n-1 + 1/(R_0*S)\n\n\n\n# create a data set: x (latent variable) and y (observation)\nset.seed(42) # to make it reproducible  (lots of random numbers follow)\nT &lt;- 50 # number of observations\nx &lt;- rep(NA, T) # latent variable\ny &lt;- rep(NA, T) # observed values\nsx &lt;- 2.2 # standard deviation for x\nsy &lt;- 0.3 # standard deviation for y\nx[1] &lt;- rnorm(1, 0, 1)\ny[1] &lt;- rnorm(1, x[1], sy)\n\nfor (t in seq(2, T)) {\n  x[t] &lt;- rnorm(1, x[t-1], sx)\n  y[t] &lt;- rnorm(1, x[t], sy)\n}\nx_true &lt;- x\nobs &lt;- y\n\n\n\nImplement a particle filter (sequential Monte Carlo)\n\n# particle filter -----------------------------------------------------------\nT &lt;- length(y) # number of observations\nN &lt;- 100 # number of particles\n# to store prior distributions for variables correspond to latent variable x\nx_prior &lt;- matrix(nrow=N, ncol = T) \nx_post &lt;- matrix(nrow=N, ncol = T)  # posterior distributions\nweights &lt;- matrix(nrow=N, ncol = T) # weights used to draw posterior sample\nW &lt;- matrix(nrow =  N, ncol = T) # normalized weights\nA &lt;- matrix(nrow =  N, ncol = T) # indices based on the normalized weights\nx_prior[, 1] &lt;- rnorm(N, 0, sx)# initial X from a normal distribution\n# calculate weights, normal likelihood\nweights[, 1] &lt;- dnorm(obs[1], x_prior[, 1], sy)\nW[, 1] &lt;- weights[, 1]/sum(weights[, 1])# normalise weights\n# indices based on the weighted resampling with replacement \nA[, 1] &lt;- sample(1:N, prob = W[1:N, 1], replace = T) \nx_post[, 1] &lt;- x_prior[A[, 1], 1] # posterior distribution using the indices\n\nfor (t in seq(2, T)) {\n  x_prior[, t] &lt;- rnorm(N, x_post[, t-1], sx) # prior x_{t} based on x_{t-1}\n  weights[, t] &lt;- dnorm(obs[t], x_prior[, t], sy) # calculate weights \n  W[, t] &lt;- weights[, t]/sum(weights[, t]) # normalise weights\n  A[, t] &lt;- sample(1:N, prob = W[1:N, t], replace = T) # indices\n  x_post[, t] &lt;- x_prior[A[, t], t] # posterior samples\n}\n\n\n\n\nSummarize results\nCalculate the mean and 2.5\\(^\\textrm{th}\\) and 97.\\(^\\textrm{th}\\) percentile of the posterior sample as a means to get 95% credible interval.\n\nx_means &lt;- apply(x_post, 2, mean) # posterior mean\nx_quantiles &lt;- apply(x_post, 2, function(x) quantile(x, probs = c(0.025, 0.975))) # 95% credible interval\ndf &lt;- data.frame(t = seq(1, T),\n                 x_mean = x_means,\n                 x_lb = x_quantiles[1, ],\n                 x_ub = x_quantiles[2, ],\n                 x_true = x_true, # latent variables\n                 y = y) # observed values\n\n\nPlot the results\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\nggplot(df, aes(x = t)) +\n  geom_ribbon(aes(ymin = x_lb, ymax = x_ub, fill=\"95% CrI\"), alpha=0.5) +\n  geom_line(aes(y=x_mean, color=\"Posterior mean\")) +\n  geom_line(aes(y=x_true, color=\"True\")) +\n  geom_point(aes(y=x_mean, color=\"Posterior mean\")) +\n  geom_point(aes(y=x_true, color=\"True\")) +\n  labs(y=\"values\", x=\"index\") + \n  scale_colour_manual(\"\", values=c(\"Posterior mean\"=\"firebrick\",\n                                   \"True\"=\"darkgrey\")) +\n  scale_fill_manual(\"\", values=\"firebrick\")+\n  theme(legend.position = \"bottom\")\n\n\n\n# ggsave(\"particle_filter.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/cholera_seir_eqn/index.html",
    "href": "posts/cholera_seir_eqn/index.html",
    "title": "Cholera SEIR equation",
    "section": "",
    "text": "Cholera SEIR equation\n[ \\[\\begin{align}\n\\lambda_p &= \\frac{\\beta(I_1+I_2+ (1-\\psi_1)  (I_1^O+I_2^O)+ (1-\\psi_2) (I_1^T+I_2^T))}{N}\\\\\n\\mathrm{d}S/\\mathrm{d}t &= - (\\lambda_p+\\lambda_w)S + 2 \\omega R_2 \\\\\n\\mathrm{d}E_1/\\mathrm{d}t &= (\\lambda_p+\\lambda_w)S - 2\\epsilon E_1\\\\\n\\mathrm{d}E_2/\\mathrm{d}t &= 2\\epsilon E_1 - 2\\epsilon E_2 \\\\\n\\mathrm{d}I_1/\\mathrm{d}t &= 2 (1-\\theta) \\epsilon E_2 - 2 \\gamma I_1 \\\\\n\\mathrm{d}I_2/\\mathrm{d}t &= 2 \\gamma I_1 - 2 \\gamma I_2 \\\\\n\\mathrm{d}A_1/\\mathrm{d}t &= 2 \\theta \\epsilon E_2 - 2 \\gamma A_1 \\\\\n\\mathrm{d}A_2/\\mathrm{d}t &= 2 \\gamma A_1 - 2 \\gamma A_2 \\\\\n\\mathrm{d}R_1/\\mathrm{d}t &= 2 \\gamma (I_2 + A_2) - 2 \\omega R_1 \\\\\n\\mathrm{d}R_2/\\mathrm{d}t &= 2 \\omega (R_1 - R_2) \\\\\n\n\\mathrm{d}S^O/\\mathrm{d}t &= - (\\lambda_p+\\lambda_w)S^O + 2 \\omega R_2^O + 2 \\omega_V O_2 \\\\\n\\mathrm{d}O_1/\\mathrm{d}t &= + v^O S - 2 \\omega_V O_1 - (\\lambda_p+\\lambda_w)(1-\\chi^O) O_1 \\\\\n\\mathrm{d}O_2/\\mathrm{d}t &= + 2 \\omega_V O_1 - 2 \\omega_V O_2 - (\\lambda_p+\\lambda_w)(1-\\chi^O) O_2 \\\\\n\\mathrm{d}E_1^O/\\mathrm{d}t &= (\\lambda_p+\\lambda_w) ((1-\\chi^O)(O_1+O_2) + S^O) + v^O E_1 - v^T E_1^O - 2\\epsilon E_1^O\\\\\n\\mathrm{d}E_2^O/\\mathrm{d}t &= 2\\epsilon E_1^O - 2\\epsilon E_2^O + v^O E_2 - v^T E_2^O\\\\\n\\mathrm{d}I_1^O/\\mathrm{d}t &= 2 (1-\\theta) \\epsilon E_2^O - 2 \\gamma I_1^O \\\\\n\\mathrm{d}I_2^O/\\mathrm{d}t &= 2 \\gamma I_1^O - 2 \\gamma I_2^O \\\\\n\\mathrm{d}A_1^O/\\mathrm{d}t &= 2 \\theta \\epsilon E_2^O - 2 \\gamma A_1^O + v^O A_1 - v^T A_1^O\\\\\n\\mathrm{d}A_2^O/\\mathrm{d}t &= 2 \\gamma A_1^O - 2 \\gamma A_2^O + v^O A_2 - v^T A_2^O\\\\\n\\mathrm{d}R_1^O/\\mathrm{d}t &= 2 \\gamma (I_2^O + A_2^O) - 2 \\omega R_1^O + v^O R_1 - v^T R_1^O\\\\\n\\mathrm{d}R_2^O/\\mathrm{d}t &= 2 \\omega (R_1^O - R_2^O) + v^O R_2 - v^T R_2^O\\\\\n\n\\mathrm{d}S^T/\\mathrm{d}t &= - (\\lambda_p+\\lambda_w)S^T + 2 \\omega R_2^T + 2 \\omega_V T_2 \\\\\n\\mathrm{d}T_1/\\mathrm{d}t &= + v S^O - 2 \\omega_V T_1 - (\\lambda_p+\\lambda_w)(1-\\chi^T) T_1\\\\\n\\mathrm{d}T_2/\\mathrm{d}t &= + 2 \\omega_V T_1 - 2 \\omega_V T_2 - (\\lambda_p+\\lambda_w)(1-\\chi^T) T_2\\\\\n\\mathrm{d}E_1^T/\\mathrm{d}t &= (\\lambda_p+\\lambda_w)(S^T + (1-\\chi^T)(T_1+T_2)) -\n  2\\epsilon E_1^T + v^T E_1^O\\\\\n\\mathrm{d}E_2^T/\\mathrm{d}t &= 2\\epsilon E_1^T - 2\\epsilon E_2^T + v^T E_2^O\\\\\n\\mathrm{d}I_1^T/\\mathrm{d}t &= 2 (1-\\theta) \\epsilon E_2^T - 2 \\gamma I_1^T\\\\\n\\mathrm{d}I_2^T/\\mathrm{d}t &= 2 \\gamma I_1^T - 2 \\gamma I_2^T \\\\\n\\mathrm{d}A_1^T/\\mathrm{d}t &= 2 \\theta \\epsilon E_2^T - 2 \\gamma A_1^T + v^T A_1^O \\\\\n\\mathrm{d}A_2^T/\\mathrm{d}t &= 2 \\gamma A_1^T - 2 \\gamma A_2^T + v^T A_2^O \\\\\n\\mathrm{d}R_1^T/\\mathrm{d}t &= 2 \\gamma (I_2^T + A_2^T) - 2 \\omega R_1^T + v^T R_1^O \\\\\n\\mathrm{d}R_2^T/\\mathrm{d}t &= 2 \\omega (R_1^T - R_2^T) + v^T R_2^O \\\\\n\n\\end{align}\\] ]"
  },
  {
    "objectID": "posts/universal_differential_eqn_julia/index.html",
    "href": "posts/universal_differential_eqn_julia/index.html",
    "title": "Universal differential equation using Julia",
    "section": "",
    "text": "Universal differential equation (UDE)\nThe UDE refers to an approach to embed the machine learning into differential equations. The resulting UDE has some parts of the equation replaced by universal approximators i.e., neural network (NN). The UDE model approach allows us to approximate a wide, if not infinite, variety of functional relationships. As an example, I will test how well the UDE model approach will approximate a sub-exponential growth model, which is challenging to fit if we use an exponential growth model.\nI am using Julia for the UDE approach as it appeared that the Julia is the most advanced in this regard.\n\n# SciML (Scientific Machine Learning) Tools\nusing OrdinaryDiffEq, SciMLSensitivity\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL\n\n# Standard Libraries\nusing LinearAlgebra, Statistics\n\n# External Libraries\nusing ComponentArrays, Lux, Zygote, Plots, StableRNGs\ngr()\n\nPlots.GRBackend()\n\n\n# Set a random seed for reproducible behaviour\nrng = StableRNG(1111)\n\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000008af)\n\n\n\n\nData generation\nThe SIR model with a sub-exponential growth is used.\n\nfunction sir_subexp!(du, u, p, t)\n    α, β, γ = p \n    du[1] = - β*u[1]*u[2]^α\n    du[2] = + β*u[1]*u[2]^α - γ*u[2]\n    du[3] = + γ*u[2]\nend\n\nsir_subexp! (generic function with 1 method)\n\n\n# Define the experimental parameter\ntspan = (0.0, 20.0);\n# u0 = 5.0f0 * rand(rng, 2)\nu0 = [0.99, 0.01, 0.0];\np_ = [0.8, 0.4, 0.2];\nprob = ODEProblem(sir_subexp!, u0, tspan, p_);\nsolution = solve(prob, Tsit5(), abstol = 1e-12, reltol = 1e-12, saveat = 1.0);\n\n# Add noise in terms of the mean\nX = Array(solution);\nt = solution.t;\n\nxbar = mean(X, dims=2);   \nnoise_magnitude = 5e-2;\nXn = X .+ (noise_magnitude * xbar) .* randn(rng, eltype(X), size(X));\n\nplot(solution, alpha = 0.75, color = :black, label = [\"True Data\" nothing]);\nscatter!(t, transpose(Xn), color = :red, label = [\"Noisy Data\" nothing])\n\n\n\n\n\n\nUDE model\n\n# Let's define our Universal Differential eqution\nrbf(x) = exp.(-(x .^ 2));\n\n# Multilayer FeedForward\nconst U = Lux.Chain(Lux.Dense(3, 5, rbf), Lux.Dense(5, 5, rbf), \nLux.Dense(5, 5, rbf), Lux.Dense(5, 1))\n\nChain(\n    layer_1 = Dense(3 =&gt; 5, rbf),       # 20 parameters\n    layer_2 = Dense(5 =&gt; 5, rbf),       # 30 parameters\n    layer_3 = Dense(5 =&gt; 5, rbf),       # 30 parameters\n    layer_4 = Dense(5 =&gt; 1),            # 6 parameters\n)         # Total: 86 parameters,\n          #        plus 0 states.\n\n# Get the initial parameters and state variables of the model\np, st = Lux.setup(rng, U)\n\n((layer_1 = (weight = Float32[-0.11597705 -0.5499123 0.10071843; -0.20088743 0.5602648 0.2718303; … ; -0.22440201 -0.57859105 0.7904316; -0.4619576 -0.62989676 0.18545352], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_2 = (weight = Float32[-0.043933477 -0.21508422 … 0.55779475 0.5849693; 0.0011237671 0.006483868 … 0.27549765 -0.2874395; … ; 0.5079049 -0.36002874 … 0.41297784 -0.5777891; -0.5179172 -0.60432595 … -0.18625909 0.06577149], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_3 = (weight = Float32[0.21934992 0.20916325 … -0.357856 -0.27426103; 0.59777355 -0.04514681 … 0.22668682 0.73459923; … ; 0.36797842 0.13955377 … 0.28912562 0.20840885; -0.33154675 0.035615936 … 0.011346816 -0.13401343], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_4 = (weight = Float32[-0.49593353 -0.68478346 … -0.4632702 -0.1476636], bias = Float32[0.0;;])), (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()))\n\nconst _st = st\n\n(layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple())\n\n\n# Define the hybrid model\nfunction ude_dynamics!(du, u, p, t, p_true)\n    û = U(u, p, _st)[1] # Network prediction\n    du[1] = dS = - û[1]\n    du[2] = dI = + û[1] - p_true[3]*u[2] \n    du[3] = dR = + p_true[3]*u[2] \nend\n\nude_dynamics! (generic function with 1 method)\n\n\n# Closure with the known parameter\nnn_dynamics!(du, u, p, t) = ude_dynamics!(du, u, p, t, p_)\n\nnn_dynamics! (generic function with 1 method)\n\n# Define the problem\nprob_nn = ODEProblem(nn_dynamics!, Xn[:, 1], tspan, p)\n\nODEProblem with uType Vector{Float64} and tType Float64. In-place: true\ntimespan: (0.0, 20.0)\nu0: 3-element Vector{Float64}:\n 1.0239505612968622\n 0.0034985090690380412\n 0.00031492340046744696\n\n\n# I don't understand the details of the algorithm\n# sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))\n# I just adopted what's provided in the web page: \n# https://docs.sciml.ai/Overview/stable/showcase/missing_physics/\n    \nfunction predict(θ, X = Xn[:, 1], T = t)\n    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)\n    Array(solve(_prob, Tsit5(), saveat = T,\n                abstol = 1e-6, reltol = 1e-6,\n                sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\npredict (generic function with 3 methods)\n\n\nfunction loss(θ)\n    Xhat = predict(θ)\n    mean(abs2, Xn .- Xhat)\nend\n\nloss (generic function with 1 method)\n\n\nlosses = Float64[];\n\ncallback = function (p, l)\n    push!(losses, l)\n    if length(losses) % 50 == 0\n        println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n    end\n    return false\nend\n\n#125 (generic function with 1 method)\n\n\nadtype = Optimization.AutoZygote();\noptf = Optimization.OptimizationFunction((x, p) -&gt; loss(x), adtype);\noptprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p));\n\nmxiter = 2000\n\n2000\n\nres1 = Optimization.solve(optprob, ADAM(), callback = callback, maxiters = mxiter);\nprintln(\"Training loss after $(length(losses)) iterations: $(losses[end])\")\n\nTraining loss after 2001 iterations: 0.0015564208688977953\n\n\n# You can optimize further by using LBFGS\noptprob2 = Optimization.OptimizationProblem(optf, res1.u)\n\nOptimizationProblem. In-place: true\nu0: ComponentVector{Float64}(layer_1 = (weight = [-0.05170182389781148 -0.6044817578829536 0.030221881765418415; -0.3212356181883164 0.6510350091355674 0.44802414727952533; … ; -0.2971002151491799 -0.5198718000300768 0.8494439396527995; -0.5240838499404261 -0.5805885464635301 0.2207764494806622], bias = [0.07034258756546354; -0.1248750018555649; … ; -0.10460911230309018; -0.09231330051973555;;]), layer_2 = (weight = [-0.16444552836939919 -0.3914934840987012 … 0.4595876886814785 0.48596336484236335; 0.061104866379551544 0.09809356182285524 … 0.3325709332030265 -0.23929486434292638; … ; 0.44222617028560024 -0.07489082174986872 … 0.38613240122330794 -0.609809451956525; -0.42554673750885325 -0.3912678259711914 … -0.0705070098211834 0.1835005315863163], bias = [-0.1024237112798211; 0.05522672749980388; … ; -0.06460784045513147; 0.08515934187060795;;]), layer_3 = (weight = [0.30753421402720227 0.29471143354807966 … -0.26966507778103693 -0.19068344291262843; 0.6803575575882911 0.035025314411197044 … 0.3095389929940732 0.8122600471132518; … ; 0.4338542747330682 0.20165221719458867 … 0.3525999208278827 0.27010233878411616; -0.20708092822357524 0.1890743367618981 … 0.16598038476857452 -0.0012012794517766243], bias = [0.0853924513699166; 0.08006728472122455; … ; 0.061942487865868964; 0.1530663635694471;;]), layer_4 = (weight = [-0.4208452558700493 -0.6144846254336126 … -0.40340199958423073 -0.06553529479079494], bias = [0.09174711461758511;;]))\n\nres2 = Optimization.solve(optprob2, Optim.LBFGS(), callback = callback, maxiters = 1000);\nprintln(\"Final training loss after $(length(losses)) iterations: $(losses[end])\")\n\nFinal training loss after 3002 iterations: 0.0003723725121765437\n\n\n# Rename the best candidate\np_trained = res2.u\n\nComponentVector{Float64}(layer_1 = (weight = [0.1316220200349473 -0.6520282050791907 0.008318466489230203; -0.4557052571206415 0.852778797405868 0.6422456105470944; … ; -0.3017967416294747 -0.3539416097805037 0.9435890641280813; -0.6208672577811719 -0.8250782773685028 0.463368746591991], bias = [0.1811403798125198; 0.1295565740621664; … ; 0.14387087901155535; -0.1883283800836187;;]), layer_2 = (weight = [-0.5224273510619183 -0.7851736034397837 … 0.13246567539583712 0.28492163069656595; 0.2121727337798046 0.2580716617374057 … 0.46828242306889367 -0.18639886649813667; … ; 0.31182369287889583 -0.13443324204943075 … 0.18614501798809488 -0.9123175434094871; -0.5023288426177288 -0.4943627237480249 … -0.05984671314326837 0.14167837142544432], bias = [-0.4580492880391406; 0.2156300092628208; … ; -0.18292941364366094; 0.014071394516324979;;]), layer_3 = (weight = [0.17533364576159247 0.2053893580840498 … -0.3080681971901958 -0.3356279391033146; 0.7087531824294593 0.0960694141143031 … 0.3481334453513509 0.8277491605925765; … ; 0.4365583552750147 0.20075642462931304 … 0.37058208065336945 0.2762370994412038; -0.18955475880245964 0.23766428666711154 … 0.23703263569676214 0.011769794045616492], bias = [-0.11836077082556237; 0.07082483083187767; … ; 0.058396858004157386; 0.15828264366434497;;]), layer_4 = (weight = [-0.39882746395912166 -0.6321529793894407 … -0.38424206982191056 -0.18894617437931902], bias = [0.4328709022831265;;]))\n\n\n# Plot the losses\npl_losses = plot(1:mxiter, losses[1:mxiter], yaxis = :log10, xaxis = :log10,\n                 xlabel = \"Iterations\", ylabel = \"Loss\", label = \"ADAM\", color = :blue)\n\n\n\nplot!((mxiter+1):length(losses), losses[(mxiter+1):end], yaxis = :log10, xaxis = :log10,\n      xlabel = \"Iterations\", ylabel = \"Loss\", label = \"BFGS\", color = :red)\n\n\n\n\n\n## Analysis of the trained network\n# Plot the data and the approximation\nts = first(solution.t):(mean(diff(solution.t)) / 2):last(solution.t)\n\n0.0:0.5:20.0\n\nXhat = predict(p_trained, Xn[:, 1], ts)\n\n3×41 Matrix{Float64}:\n 1.02395      1.0072      0.991471    …  0.150335  0.140635  0.13015\n 0.00349851   0.0190933   0.0322405      0.163083  0.156799  0.151863\n 0.000314923  0.00146965  0.00405242     0.714346  0.73033   0.745751\n\n# Trained on noisy data vs real solution\npl_trajectory = plot(ts, transpose(Xhat), xlabel = \"t\", \n                     ylabel = \"S(t), I(t), R(t)\", color = :red,\n                     label = [\"UDE Approximation\" nothing])\n\n\n\nscatter!(solution.t, transpose(Xn), color = :black, label = [\"Measurements\" nothing])\n\n\n\n\n\n# Ideal unknown interactions of the predictor\n# Ybar = [-p_[2] * (Xhat[1, :] .* Xhat[2, :])'; p_[3] * (Xhat[1, :] .* Xhat[2, :])']\n# Ybar = [p_[2] .* Xhat[1,:] .* (Xhat[2,:].^p_[1])]\nYbar = transpose([p_[2] * Xhat[1,i] * (Xhat[2,i].^p_[1]) for i ∈ 1:41, j ∈ 1:1])\n\n1×41 transpose(::Matrix{Float64}) with eltype Float64:\n 0.00444064  0.0169778  0.0254133  …  0.0140944  0.012777  0.0115257\n\n# Neural network guess\nYhat = U(Xhat, p_trained, st)[1]\n\n1×41 Matrix{Float64}:\n 0.0351306  0.0321994  0.0309596  …  0.018876  0.0200484  0.0220433\n\n\npl_reconstruction = plot(ts, transpose(Yhat), xlabel = \"t\", \n                         ylabel = \"U(S,I,R)\", color = :red,\n                         label = [\"UDE Approximation\" nothing]);\n\nplot!(ts, transpose(Ybar), color = :black, label = [\"True Interaction\" nothing])\n\n\n\n\n\n# Plot the error\npl_reconstruction_error = plot(ts, norm.(eachcol(Ybar - Yhat)), yaxis = :log, xlabel = \"t\",\n                               ylabel = \"L2-Error\", label = nothing, color = :red);\npl_missing = plot(pl_reconstruction, pl_reconstruction_error, layout = (2, 1));\n\npl_overall = plot(pl_trajectory, pl_missing)"
  },
  {
    "objectID": "posts/fit_a_straight_line_julia/index.html",
    "href": "posts/fit_a_straight_line_julia/index.html",
    "title": "Fitting a straight line in Julia: Flux machine learning",
    "section": "",
    "text": "Fitting a straight line in Julia\nThis post is my attempt to learn machine learning in Julia. The contents of this page came from the Flux. Flux is a machine learning package written in Julia.\n\n\nCreate training and test data\n\nusing Flux, Distributions, Random, Statistics\n# create the data\n# true parameter values are 4 and 2\nlinear_model(x) = rand(Normal(4x+2,1))[1]\n\nlinear_model (generic function with 1 method)\n\n\nx_train, x_test = hcat(0:5...), hcat(6:10...)\n\n([0 1 … 4 5], [6 7 … 9 10])\n\ny_train, y_test = linear_model.(x_train), linear_model.(x_test)\n\n([1.552234290757838 5.997621972031162 … 17.799006293669162 21.987462504032138], [26.82759499486528 28.113859596390927 … 38.71844108675521 43.89937704938766])\n\n\n\n\nCreate a neural network model with a single layer\n\n# y = σ.(W * x .+ bias) \nmodel = Flux.Dense(1 =&gt; 1) # 2 parameters\n\nDense(1 =&gt; 1)       # 2 parameters\n\n# prediction based on the untrained baseline model \nmodel(x_train)\n\n1×6 Matrix{Float32}:\n 0.0  -0.230685  -0.461369  -0.692054  -0.922739  -1.15342\n\n\n# define the loss function to use it for training\nloss(m, x, y) = mean(abs2.(m(x) .- y))\n\nloss (generic function with 1 method)\n\nloss(model, x_train, y_train)\n\n214.52561035758617\n\n\n\n\nTrain the model\nFlux package has the Flux.train! function for model training. The function requires an optimizer, which can be, for example, created using Descent function.\n\nusing Flux: train!\n\nopt = Descent() # gradient descent algorithm \n\nDescent(0.1)\n\nDescent(0.1)\n\nDescent(0.1)\n\n\ndata = [(x_train, y_train)]\n\n1-element Vector{Tuple{Matrix{Int64}, Matrix{Float64}}}:\n ([0 1 … 4 5], [1.552234290757838 5.997621972031162 … 17.799006293669162 21.987462504032138])\n\n\ntrain!(loss, model, data, opt)\nloss(model, x_train, y_train)\n\n203.61293290763626\n\n\n# check model parameters after going through the data once\nmodel.weight, model.bias\n\n(Float32[8.607251;;], Float32[2.5395057])\n\n\n# iteratively train the model\nfor epoch in 1:200\n    train!(loss, model, data, opt)\nend\n\n\n\nExamine the trained model\n\n# check the loss\nloss(model, x_train, y_train)\n\n0.19602860021782811\n\n# check the model parameters\nmodel.weight, model.bias\n\n(Float32[4.0600896;;], Float32[2.0363088])\n\n# check the model against the test data set\nmodel(x_test)\n\n1×5 Matrix{Float32}:\n 26.3968  30.4569  34.517  38.5771  42.6372\n\ny_test\n\n1×5 Matrix{Float64}:\n 26.8276  28.1139  33.7393  38.7184  43.8994"
  },
  {
    "objectID": "posts/diffeqr/index.html",
    "href": "posts/diffeqr/index.html",
    "title": "diffeqr: R interface to the Julia’s DifferentialEquations.jl",
    "section": "",
    "text": "Julia DifferentialEquations.jl provides an impressive collection of differential equation solvers. The DE solvers available in the package are reliable and a lot faster than what’s avaiable in R. It’s now possible to access the solvers in R thanks to the diffeqr package. The following codes were adapted from the diffeqr GitHub page.\n\ndeSolve package\nI am using the SIR model as an example and for speed comparison, first solving equations using the deSolve package.\n\nsir_deSolve &lt;- function(t, u, p){\n  N &lt;- sum(u)\n  du1 &lt;- - p[1]*u[1]*u[2]/N\n  du2 &lt;- + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 &lt;- + p[2]*u[2]\n    \n  return(list(c(du1, du2, du3))) \n}\n\nLet’s sovel the model using the deSolve::ode function.\n\nlibrary(deSolve)\nu0 &lt;- c(99, 1, 0)\ntspan &lt;- seq(from=0, to=50, by=1)\np &lt;- c(0.4, 0.2)\n\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=sir_deSolve, parms=p))\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(outdf,aes(x=time))+\n  geom_line(aes(y=`1`, color=\"S\")) +\n  geom_line(aes(y=`2`, color=\"I\")) +\n  geom_line(aes(y=`3`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\n\n\ndiffeqr package\nNow let’s use the diffeqr package. Once the de &lt;- diffeqr::diffeq_setup is executed, the functions for DifferentialEquations.jl are available through de$.\n\nlibrary(diffeqr)\nde &lt;- diffeqr::diffeq_setup()\n\ndiffeqr has slightly different convention.\n\nsir &lt;- function(u, p, t){\n  N = sum(u)\n  du1 = - p[1]*u[1]*u[2]/N\n  du2 = + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 = + p[2]*u[2]\n    \n  return(c(du1,du2,du3))\n}\n\nu0 &lt;- c(100, 1, 0.0)\ntspan &lt;- c(0.0, 50.0)\np &lt;- c(0.4, 0.2)\nprob &lt;- de$ODEProblem(sir, u0, tspan, p)\nsol &lt;- de$solve(prob, de$Tsit5(), saveat=1)\n\nmat &lt;- sapply(sol$u,identity)\nudf &lt;- as.data.frame(t(mat))\ntudf &lt;- cbind(data.frame(t=sol$t), udf)\n\ntudflong = tidyr::pivot_longer(tudf, cols=2:4, \n                               names_to=\"var\", \n                               values_to=\"count\")\n\nggplot(tudf,aes(x=t))+\n  geom_line(aes(y=V1, color=\"S\")) +\n  geom_line(aes(y=V2, color=\"I\")) +\n  geom_line(aes(y=V3, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\nThe ODE model can be sped up after compiling usng the just-in-time (JIT) compiler.\n\nprob_jit &lt;- diffeqr::jitoptimize_ode(de, prob)\n# sol &lt;- de$solve(prob_jit, de$Tsit5(), saveat=1);\n\nThe ODE model can be sped up even further by running it on the GPU. The GitHub page is more geared toward the case in which runing the model multiple times over different initial conditions, which is called ensemble solve. This is why the term ensemble is used below. However, we use the same initial conditions as our sole purpose is to run the model multiple times and compare the elapsed time.\n\nprob_func &lt;- function (prob, i, rep){\n  de$remake(prob, u0=u0, p=p)\n}\nprob_ens &lt;- de$EnsembleProblem(prob_jit, prob_func=prob_func, safetycopy=FALSE)\n# sol &lt;- de$solve(prob_ens, de$Tsit5(), de$EnsembleSerial(), trajectories=1, saveat=1);\n# to take the full advantage we need the following.\ndegpu &lt;- diffeqr::diffeqgpu_setup(\"CUDA\")\n# de$solve(prob_ens, degpu$GPUTsit5(), degpu$EnsembleGPUKernel(degpu$CUDABackend()), trajectories=niter, saveat=1);\n\nI stop here but further performance enhancements are possible if your problem can make use of parallel computing.\n\nlibrary(microbenchmark)\nniter &lt;- 1e3\n\nbenchmark = microbenchmark(\n  deSolve = ode(y=u0, times=seq(0,50,by=1), func=sir_deSolve, parms=c(0.4,0.3)), \n  diffeqr = de$solve(prob, de$Tsit5(), saveat=1),\n  diffeqr_jit = de$solve(prob_jit, de$Tsit5(), saveat=1),\n  diffeqr_ens = de$solve(prob_ens, de$Tsit5(), de$EnsembleSerial(), trajectories=1, saveat=1),\n  times=niter\n)\n\nbenchmark\n\nUnit: microseconds\n        expr     min       lq      mean   median       uq       max neval cld\n     deSolve 604.601 660.6515  735.1354 682.3505 707.7515    3790.8  1000   a\n     diffeqr 769.601 799.1505  903.3603 831.7010 871.6505    5971.4  1000   a\n diffeqr_jit  84.501  99.2010  177.8533 112.8010 136.6005   46489.0  1000   a\n diffeqr_ens 185.601 216.7010 2667.6595 242.2505 277.0510 2389143.8  1000   a\n\n# GPU version requires a different framework to test the speed\ntelapsed &lt;- system.time(de$solve(prob_ens, degpu$GPUTsit5(), degpu$EnsembleGPUKernel(degpu$CUDABackend()), trajectories=niter, saveat=1))\n\ndf &lt;- data.frame(routine=benchmark$expr, time=benchmark$time)\nggplot(df) + \n  geom_violin(aes(x=routine,y=time))+\n  scale_y_log10(limits=c(0.01,1e8))+\n  geom_hline(aes(yintercept=telapsed[3]), color=\"firebrick\")+\n  coord_flip()+\n  annotate(\"text\", y=telapsed[3]+5, \n           x=\"diffeqr\",\n           label=\"GPU diffeqr\", color=\"firebrick\")\n\n\n\n# ggsave(\"diffeqr_benchmark.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)"
  },
  {
    "objectID": "posts/oral_cholera_vacc_cov/index.html",
    "href": "posts/oral_cholera_vacc_cov/index.html",
    "title": "Template",
    "section": "",
    "text": "Oral cholera vaccine\nAssumption - Vaccination is implemented in a all-or-nothing fashion - Two-dose efficacy is \\(VE\\) - The first-dose efficacy is \\(ve_1\\)\nWhat the efficacy of the second vaccine dose, \\(ve_2\\), has to be if the want the same number of people is averted,\n\nset.seed(42) # to make it reproducible\nN = 1000\nVE = 0.8 # vaccine efficacy under two dose regimen\nvacc_cov = 0.6 # vaccine coverage\nve1 = 0.4 # efficacy of the first dose\n\nvacc_protected_VE = VE*vacc_cov*N\nvacc_protected_ve1 = ve1*vacc_cov*N\nvacc_protected_ve_ve2 = vacc_protected_ve1 + (VE-ve1)*vacc_cov*N\n\n\n\nImplementing the first and the seocond dose separately\nSuppose that a population size is \\(N\\) and the vaccine coverage, \\(0&lt;=v&lt;=1\\). We introduce a parameter, \\(\\pi\\), to represent the proportion of the first-dose recipients who receive the second dose. If \\(\\pi=1\\), the number of vaccine recipients is \\(Nv\\) and the number of two-dose recipients is also \\(Nv\\) and the number of vaccinees who only received a single dose is zero. More generally, however, if \\(\\pi &lt; 1\\), the number of complete two-dose recipients is \\(Nv\\pi\\) and those who have only received one dose is \\(2Nv(1-\\pi)\\).\n\nset.seed(42) # to make it reproducible\nN = 1000\nve2 = 0.8 # vaccine efficacy under two dose regimen\nve1 = 0.4 # efficacy of the first dose\nvacc_cov = 0.6 # vaccine coverage\npi = 0.95 # proportion of the first-dose recipients who again received the second dose\nvacc_protected_ve2 = ve2*vacc_cov*N\nvacc_protected_ve1 = ve1*vacc_cov*N\nvacc_protected_ve1_ve2 = \n  pi*(vacc_protected_ve1 + (ve2-ve1)*vacc_cov*N) + 2*(1-pi)*ve1*vacc_cov*N\n\nvacc_protected_ve1\n\n[1] 240\n\nvacc_protected_ve1_ve2\n\n[1] 480\n\n\nThese are estimates and algebraic relationship wouldn’t hold. $$ \\[\\begin{align}\nc_2 &= v_1 \\pi \\\\\nc_{1+} &= v_1 \\pi + v_1(1-\\pi) + v_2(1-\\pi v_1/v_2)\n\n\\end{align}\\] $$ \\(c_2\\) and \\(c_{1+}\\) represent coverage for complete two-dose regiment and at least one dose, respectively. \\(\\pi\\) represents the proportion of the vaccinees who received the first dose and went on to receive the second dose.\nEstimates from Pezzolli et al. (2020)\n\ncov_first = 90.3 # v_1\ncov_second = 88.2# v_2\ncov_two &lt;- 69.9 #c_2\ncov_one_plus &lt;- 84.6 #c_1+\n\nBoundary conditions for the pi for the vr1plus must not be bigger than 1\n\n# create a function that calculates \nocv_round_cov_calc = function(vc1=0.95, vc2=0.95, vr1plus=NULL, vr2=NULL, pi=0.7){\n  \n  if (pi &lt; (vc1+vc2-1)/vc1) {\n    stop(paste0(\"pi must be larger than (vc1+vc2-1)/vc1, \", (vc1+vc2-1)/vc1))\n  } else if (pi &gt; (vc2/vc1)) {\n    stop(paste0(\"pi must be smaller than vc2/vc1, \", vc2/vc1))\n  }\n  vr2 = vc1*pi  \n  # vr1plus = vc1*pi + vc1*(1-pi) + vc2*(1-pi*vc1/vc2)\n  vr1plus = vr2 + vc1*(1-pi) + vc2*(1-pi*vc1/vc2) \n  vr1 = vr1plus - vr2\n  # TODO there are several limiting conditions\n  # vaccine coverage is \n  return (list(vr1plus=vr1plus, vr2=vr2, vr1=vr1))\n}\nocv_round_cov_calc(vc1=0.56, vc2=0.46, vr1plus=NULL, vr2=NULL, pi=0.80)\n\n$vr1plus\n[1] 0.572\n\n$vr2\n[1] 0.448\n\n$vr1\n[1] 0.124\n\n\nWhen the first dose is distributed, apply the When the second dose is distributed, identify the number of two-dose and one-dose recipients and compare that number with the previous round\n\n\nSummarize results"
  },
  {
    "objectID": "posts/sir_cpp_julia_odin/index.html",
    "href": "posts/sir_cpp_julia_odin/index.html",
    "title": "SIR model benchmarks: deSolve, odin, and diffeqr",
    "section": "",
    "text": "deSolve package\n\nsir_deSolve &lt;- function(t, u, p){\n  N &lt;- sum(u)\n  du1 &lt;- - p[1]*u[1]*u[2]/N\n  du2 &lt;- + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 &lt;- + p[2]*u[2]\n    \n  return(list(c(du1, du2, du3))) \n}\n\nlibrary(deSolve)\nu0 &lt;- c(0.99, 0.01, 0)\ntspan &lt;- seq(from=0, to=50)\np &lt;- c(0.4, 0.2)\n\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=sir_deSolve, parms=p))\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(outdf,aes(x=time))+\n  geom_line(aes(y=`1`, color=\"S\")) +\n  geom_line(aes(y=`2`, color=\"I\")) +\n  geom_line(aes(y=`3`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\n\n\nManual C++\nEuler method was implemented\n\nsir_cpp &lt;- '\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nList sir_cpp(List params) {\n  double tau = params[\"tau\"]; // time step size\n  double ndays = params[\"ndays\"]; // number of days for output\n  int nsubsteps = ceil(1/tau);\n  \n  NumericVector S(ndays+1);\n  NumericVector I(ndays+1);\n  NumericVector R(ndays+1); \n  NumericVector time(ndays+1);\n  \n  S(0) = params[\"susceptible\"];\n  I(0) = params[\"infectious\"];\n  R(0) = params[\"recovered\"];\n  \n  double b = params[\"b\"]; // transmission rate per unit of time\n  double g = params[\"g\"]; // recovery rate\n  \n  for (int day = 0; day &lt; ndays; day++) {\n    double St = S[day];\n    double It = I[day];\n    double Rt = R[day];\n    \n    for (int substep = 0; substep &lt; nsubsteps; substep++){\n      double N = St + It + Rt;\n      double foi = b * It / N;\n\n      double StoI = St * foi * tau;\n      double ItoR = It * g * tau;\n\n      double dS = - StoI;\n      double dI = + StoI - ItoR;\n      double dR = + ItoR;\n\n      St = St + dS;\n      It = It + dI;\n      Rt = Rt + dR;\n    }\n    // Update next timestep\n    S[day + 1] = St;\n    I[day + 1] = It;\n    R[day + 1] = Rt;\n    time[day + 1] = day+1;\n  }\n\n  DataFrame result = DataFrame::create(\n    Named(\"time\") = time,\n    Named(\"S\") = S,\n    Named(\"I\") = I,\n    Named(\"R\") = R);\n\n  return result;\n}'\n\n\nRcpp::cppFunction(code=sir_cpp)\n\nparams &lt;- list()\nparams &lt;- within(params, {\n  tau &lt;- 0.1 # in days\n  ndays &lt;- 50\n\n  b &lt;- 0.4\n  g &lt;- 0.2\n  \n  susceptible &lt;- 0.99\n  infectious &lt;- 0.01\n  recovered &lt;- 0.0\n})\n\nout_cpp &lt;- sir_cpp(params)\n\nggplot(out_cpp, aes(x=time))+\n  geom_line(aes(y=`S`, color=\"S\")) +\n  geom_line(aes(y=`I`, color=\"I\")) +\n  geom_line(aes(y=`R`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\n\n\nodin package\n\nlibrary(odin)\nsir_odin &lt;- odin::odin({\n  ## Derivatives\n  deriv(S) &lt;- -b*S*I/(S+I+R)\n  deriv(I) &lt;- b*S*I/(S+I+R)-g*I\n  deriv(R) &lt;- g*I\n\n  ## Initial conditions\n  initial(S) &lt;- 0.99\n  initial(I) &lt;- 0.01\n  initial(R) &lt;- 0.00\n\n  ## parameters\n  b &lt;- user(0.4)\n  g &lt;- user(0.2)\n})\n\n── R CMD INSTALL ───────────────────────────────────────────────────────────────\n* installing *source* package 'odin3fe5a09e' ...\n** using staged installation\n** libs\nusing C compiler: 'gcc.exe (GCC) 12.2.0'\ngcc  -I\"C:/PROGRA~1/R/R-43~1.1/include\" -DNDEBUG     -I\"C:/rtools43/x86_64-w64-mingw32.static.posix/include\"     -O2 -Wall -gdwarf-2 -mfpmath=sse -msse2 -mstackrealign  -UNDEBUG -Wall -pedantic -g -O0 -c odin.c -o odin.o\nodin.c: In function 'odin_metadata':\nodin.c:106:18: warning: unused variable 'internal' [-Wunused-variable]\n  106 |   odin_internal *internal = odin_get_internal(internal_p, 1);\n      |                  ^~~~~~~~\ngcc  -I\"C:/PROGRA~1/R/R-43~1.1/include\" -DNDEBUG     -I\"C:/rtools43/x86_64-w64-mingw32.static.posix/include\"     -O2 -Wall -gdwarf-2 -mfpmath=sse -msse2 -mstackrealign  -UNDEBUG -Wall -pedantic -g -O0 -c registration.c -o registration.o\ngcc -shared -static-libgcc -o odin3fe5a09e.dll tmp.def odin.o registration.o -LC:/rtools43/x86_64-w64-mingw32.static.posix/lib/x64 -LC:/rtools43/x86_64-w64-mingw32.static.posix/lib -LC:/PROGRA~1/R/R-43~1.1/bin/x64 -lR\ninstalling to C:/Users/jonghoon.kim/AppData/Local/Temp/Rtmp8m1it9/devtools_install_5f082305170c/00LOCK-file5f0873d653c3/00new/odin3fe5a09e/libs/x64\n* DONE (odin3fe5a09e)\n\nsir_mod &lt;- sir_odin$new(b=0.4, g=0.2)\ntspan &lt;- seq(from=0, to=50)\nout_odin &lt;- as.data.frame(sir_mod$run(tspan))\n\nggplot(out_odin, aes(x=t))+\n  geom_line(aes(y=`S`, color=\"S\")) +\n  geom_line(aes(y=`I`, color=\"I\")) +\n  geom_line(aes(y=`R`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\n\n\ndiffeqr package\n\nlibrary(diffeqr)\nde &lt;- diffeqr::diffeq_setup()\n\nsir_julia &lt;- function(u, p, t){\n  N = sum(u)\n  du1 = - p[1]*u[1]*u[2]/N\n  du2 = + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 = + p[2]*u[2]\n    \n  return(c(du1,du2,du3))\n}\n\nu0 &lt;- c(0.99, 0.01, 0.0)\ntspan &lt;- c(0.0, 50.0)\np &lt;- c(0.4, 0.2)\nprob &lt;- de$ODEProblem(sir_julia, u0, tspan, p)\nprob_jit &lt;- diffeqr::jitoptimize_ode(de, prob)\n\nsol &lt;- de$solve(prob_jit, de$Tsit5(), saveat=1)\n\nmat &lt;- sapply(sol$u, identity)\nudf &lt;- as.data.frame(t(mat))\ntudf &lt;- cbind(data.frame(t=sol$t), udf)\n\nggplot(tudf, aes(x=t))+\n  geom_line(aes(y=V1, color=\"S\")) +\n  geom_line(aes(y=V2, color=\"I\")) +\n  geom_line(aes(y=V3, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n# ggsave(\"diffeqr_sir.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)  \n\n\n\nBenchmarks\n\nlibrary(microbenchmark)\n\nbenchmark = microbenchmark(\n  deSolve = ode(y=u0, times=tspan, func=sir_deSolve, parms=p),\n  cpp = sir_cpp(params),\n  odin = sir_mod$run(tspan),\n  julia = de$solve(prob_jit, de$Tsit5(), saveat=1),\n  times = 1000\n)\n\nbenchmark\n\nUnit: microseconds\n    expr   min     lq     mean median      uq    max neval  cld\n deSolve 630.3 720.30 925.5176  754.2 1035.75 6149.5  1000 a   \n     cpp 278.6 367.75 498.5170  414.2  598.30 5520.1  1000  b  \n    odin 220.6 326.55 448.8347  399.6  513.60 4218.0  1000   c \n   julia 116.8 191.90 255.9409  231.6  293.70  831.2  1000    d"
  },
  {
    "objectID": "posts/python_neural_network_from_scratch/index.html",
    "href": "posts/python_neural_network_from_scratch/index.html",
    "title": "Neural Network from Scracth in Python",
    "section": "",
    "text": "Biased introduction to neural network\nAt the outset, let me tell you that I don’t intend to write a comprehensive introduction, which I don’t think I can even if I want to, to the neural network (NN). I am a theoretical epidemiologist who manages to have a minimal knoweldge on the theory and use a small set of skills in mathematics, statistics, and computer programming. I use those tools to better understand the epidemiology of the infectious disease and potential impact of the intervention programs such as vaccination.\nSo this is going to be a biased introduction to the fundamental concepts and practical computational skillsof NN to get me started in using and learning the NN. Although I am more familiar with R &gt;&gt; Java &gt;&gt; C++, Julia, I am using python, which I believe happens to be a better language for exploring NN or machine learning in general.\nNN is a collection of neuron, which refers to a simple linear function. The NN involves nonlinear transformation of the output from each neuron and eventually can approximate in theory any functions.\nAnd the core image\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_func(x,y, title):\n    # helper function to plot activation functions\n    plt.plot(x, y)\n    plt.title(title)\n    plt.xlabel('x')\n    plt.ylabel('activation(x)')\n    plt.grid(True)\n    plt.show()\n\nx = np.linspace(-10, 10, 100)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nplot_func(x,sigmoid(x),\"Sigmoid\")"
  },
  {
    "objectID": "posts/sir_deSolve_odin_diffeqr/index.html",
    "href": "posts/sir_deSolve_odin_diffeqr/index.html",
    "title": "SIR model benchmarks: deSolve, odin, and diffeqr",
    "section": "",
    "text": "deSolve package\n\nsir_deSolve &lt;- function(t, u, p){\n  N &lt;- sum(u)\n  du1 &lt;- - p[1]*u[1]*u[2]/N\n  du2 &lt;- + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 &lt;- + p[2]*u[2]\n    \n  return(list(c(du1,du2,du3))) \n}\n\nlibrary(deSolve)\nu0 &lt;- c(0.99, 0.01, 0)\ntspan &lt;- seq(from=0, to=50)\np &lt;- c(0.4, 0.2)\n\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=sir_deSolve, parms=p))\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(outdf,aes(x=time))+\n  geom_line(aes(y=`1`, color=\"S\")) +\n  geom_line(aes(y=`2`, color=\"I\")) +\n  geom_line(aes(y=`3`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\n\n\nManual C++\nEuler method was implemented\n\nsir_cpp &lt;- '\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nList sir_cpp(List params) {\n  double tau = params[\"tau\"]; // time step size\n  double ndays = params[\"ndays\"]; // number of days for output\n  int nsubsteps = ceil(1/tau);\n  \n  NumericVector S(ndays+1);\n  NumericVector I(ndays+1);\n  NumericVector R(ndays+1); \n  NumericVector time(ndays+1);\n  \n  S(0) = params[\"susceptible\"];\n  I(0) = params[\"infectious\"];\n  R(0) = params[\"recovered\"];\n  \n  double b = params[\"b\"]; // transmission rate per unit of time\n  double g = params[\"g\"]; // recovery rate\n  \n  for (int day = 0; day &lt; ndays; day++) {\n    double St = S[day];\n    double It = I[day];\n    double Rt = R[day];\n    \n    for (int substep = 0; substep &lt; nsubsteps; substep++){\n      double N = St + It + Rt;\n      double foi = b * It / N;\n\n      double StoI = St * foi * tau;\n      double ItoR = It * g * tau;\n\n      double dS = - StoI;\n      double dI = + StoI - ItoR;\n      double dR = + ItoR;\n\n      St = St + dS;\n      It = It + dI;\n      Rt = Rt + dR;\n    }\n    // Update next timestep\n    S[day + 1] = St;\n    I[day + 1] = It;\n    R[day + 1] = Rt;\n    time[day + 1] = day+1;\n  }\n\n  DataFrame result = DataFrame::create(\n    Named(\"time\") = time,\n    Named(\"S\") = S,\n    Named(\"I\") = I,\n    Named(\"R\") = R);\n\n  return result;\n}'\n\n\nRcpp::cppFunction(code=sir_cpp)\n\nparams &lt;- list()\nparams &lt;- within(params, {\n  tau &lt;- 0.1 # in days\n  ndays &lt;- 50\n\n  b &lt;- 0.4\n  g &lt;- 0.2\n  \n  susceptible &lt;- 0.99\n  infectious &lt;- 0.01\n  recovered &lt;- 0.0\n})\n\nout_cpp &lt;- sir_cpp(params)\n\nggplot(out_cpp, aes(x=time))+\n  geom_line(aes(y=`S`, color=\"S\")) +\n  geom_line(aes(y=`I`, color=\"I\")) +\n  geom_line(aes(y=`R`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\n\n\nodin package\n\nlibrary(odin)\nsir_odin &lt;- odin::odin({\n  ## Derivatives\n  deriv(S) &lt;- -b*S*I/(S+I+R)\n  deriv(I) &lt;- b*S*I/(S+I+R)-g*I\n  deriv(R) &lt;- g*I\n\n  ## Initial conditions\n  initial(S) &lt;- 0.99\n  initial(I) &lt;- 0.01\n  initial(R) &lt;- 0.00\n\n  ## parameters\n  b &lt;- user(0.4)\n  g &lt;- user(0.2)\n})\n\n── R CMD INSTALL ───────────────────────────────────────────────────────────────\n* installing *source* package 'odin3fe5a09e' ...\n** using staged installation\n** libs\nusing C compiler: 'gcc.exe (GCC) 12.2.0'\ngcc  -I\"C:/PROGRA~1/R/R-43~1.1/include\" -DNDEBUG     -I\"C:/rtools43/x86_64-w64-mingw32.static.posix/include\"     -O2 -Wall -gdwarf-2 -mfpmath=sse -msse2 -mstackrealign  -UNDEBUG -Wall -pedantic -g -O0 -c odin.c -o odin.o\nodin.c: In function 'odin_metadata':\nodin.c:106:18: warning: unused variable 'internal' [-Wunused-variable]\n  106 |   odin_internal *internal = odin_get_internal(internal_p, 1);\n      |                  ^~~~~~~~\ngcc  -I\"C:/PROGRA~1/R/R-43~1.1/include\" -DNDEBUG     -I\"C:/rtools43/x86_64-w64-mingw32.static.posix/include\"     -O2 -Wall -gdwarf-2 -mfpmath=sse -msse2 -mstackrealign  -UNDEBUG -Wall -pedantic -g -O0 -c registration.c -o registration.o\ngcc -shared -static-libgcc -o odin3fe5a09e.dll tmp.def odin.o registration.o -LC:/rtools43/x86_64-w64-mingw32.static.posix/lib/x64 -LC:/rtools43/x86_64-w64-mingw32.static.posix/lib -LC:/PROGRA~1/R/R-43~1.1/bin/x64 -lR\ninstalling to C:/Users/jonghoon.kim/AppData/Local/Temp/RtmpoVtDRD/devtools_install_60e45cff70a/00LOCK-file60e443e27d60/00new/odin3fe5a09e/libs/x64\n* DONE (odin3fe5a09e)\n\nsir_mod &lt;- sir_odin$new(b=0.4, g=0.2)\ntspan &lt;- seq(from=0, to=50)\nout_odin &lt;- as.data.frame(sir_mod$run(tspan))\n\nggplot(out_odin, aes(x=t))+\n  geom_line(aes(y=`S`, color=\"S\")) +\n  geom_line(aes(y=`I`, color=\"I\")) +\n  geom_line(aes(y=`R`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n\n\n\ndiffeqr package\n\nlibrary(diffeqr)\nde &lt;- diffeqr::diffeq_setup()\n\nsir_julia &lt;- function(u, p, t){\n  N = sum(u)\n  du1 = - p[1]*u[1]*u[2]/N\n  du2 = + p[1]*u[1]*u[2]/N - p[2]*u[2]\n  du3 = + p[2]*u[2]\n    \n  return(c(du1,du2,du3))\n}\n\nu0 &lt;- c(0.99, 0.01, 0.0)\ntspan &lt;- c(0.0, 50.0)\np &lt;- c(0.4, 0.2)\nprob &lt;- de$ODEProblem(sir_julia, u0, tspan, p)\nprob_jit &lt;- diffeqr::jitoptimize_ode(de, prob)\n\nsol &lt;- de$solve(prob_jit, de$Tsit5(), saveat=1)\n\nmat &lt;- sapply(sol$u, identity)\nudf &lt;- as.data.frame(t(mat))\ntudf &lt;- cbind(data.frame(t=sol$t), udf)\n\nggplot(tudf, aes(x=t))+\n  geom_line(aes(y=V1, color=\"S\")) +\n  geom_line(aes(y=V2, color=\"I\")) +\n  geom_line(aes(y=V3, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n# ggsave(\"diffeqr_sir.png\", gg, units=\"in\", width=3.4*2, height=2.7*2)  \n\n\n\nBenchmarks\nThe diffeqr is the most efficient tool for running the SIR model in its ODE form.\nAs a side note, the test gave an unfair advantage to the sir_cpp due to the use of the Euler method with 0.1 day step size. In fact, deSolve::ode outperformed sir_cpp when it was used with \"method=\"euler\". Therefore there is no need to write manually CPP models if an ODE solver is what you need. The situation might differ when implementing a stochastic model, which will be discussed in a later post.\n\nlibrary(microbenchmark)\n\nbenchmark = microbenchmark(\n  deSolve = ode(y=u0, times=tspan, func=sir_deSolve, parms=p),\n  cpp = sir_cpp(params),\n  odin = sir_mod$run(tspan),\n  julia = de$solve(prob_jit, de$Tsit5(), saveat=1),\n  times = 1000\n)\n\nbenchmark\n\nUnit: microseconds\n    expr   min     lq     mean median     uq     max neval cld\n deSolve 401.2 428.35 465.7012 442.35 458.05  2894.6  1000 a  \n     cpp 169.8 206.90 239.4003 234.40 251.50  2599.0  1000  b \n    odin 127.6 184.90 227.0216 208.90 253.00  4004.9  1000  b \n   julia  71.9 109.60 148.7960 130.65 144.00 15164.4  1000   c"
  },
  {
    "objectID": "baking/typhoid_outbreak_timeseries/index.html",
    "href": "baking/typhoid_outbreak_timeseries/index.html",
    "title": "Cholera",
    "section": "",
    "text": "library(readxl)\nd &lt;- read_xlsx(\"Typhoid_outbreak_TS_datasheet_SciData_0103_jhk.xlsx\", sheet=\"typhoid_TS_summary\")\nlibrary(dplyr)\nd |&gt; group_by(country) |&gt; summarize(count=n())\n\n# A tibble: 19 × 2\n   country           count\n   &lt;chr&gt;             &lt;int&gt;\n 1 China                 2\n 2 Cote d'Ivoire         1\n 3 DRC                   2\n 4 India                 4\n 5 Jordan                1\n 6 Malawi–Mozambique     1\n 7 Mexico                1\n 8 Myanmar               1\n 9 Nepal                 1\n10 Pakistan              2\n11 South Africa          2\n12 Thailand              3\n13 Tunisia               1\n14 Turkey                1\n15 USA                   1\n16 Uganda                4\n17 Vietnam               1\n18 Zambia                1\n19 Zimbabwe             11\n\nd |&gt; group_by(d$WHO_region) |&gt; summarize(count=n())\n\n# A tibble: 6 × 2\n  `d$WHO_region` count\n  &lt;chr&gt;          &lt;int&gt;\n1 AFR               22\n2 AMR                2\n3 EMR                4\n4 EUR                1\n5 SEAR              10\n6 WPR                2\n\nd2 &lt;- read_xlsx(\"fit_comparison_new.xlsx\", sheet=\"Sheet1\")\nnrow(d2)\n\n[1] 521\n\nsum(d2$`R0...8` &gt; 15) \n\n[1] 172\n\nfits_n0 &lt;- readRDS(\"fits_n0_1_521_20231013.rds\")\n# saveRDS(fits_n0, \"fits_n0_1_521_20231013.rds\")\nnfevals &lt;- sapply(fits_n0, function(z) z$fit$optim$nfeval)\nbestvals &lt;- sapply(fits_n0, function(z) z$fit$optim$bestval)\n\nids &lt;- which(!is.infinite(bestvals))\nlength(ids)\n\n[1] 430\n\nsummary(nfevals[ids])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 160400  160400  160400  160400  160400  160400 \n\nsummary(bestvals[ids])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  16.31   56.38  109.62  167.07  206.51 1347.93 \n\narbitrary_cutoff &lt;- 200\nsum(bestvals &lt; arbitrary_cutoff)\n\n[1] 317"
  },
  {
    "objectID": "baking/python_neural_network_from_scratch/index.html",
    "href": "baking/python_neural_network_from_scratch/index.html",
    "title": "Neural Network from Scracth in Python",
    "section": "",
    "text": "Biased introduction to neural network\nAt the outset, let me tell you that I don’t intend to write a comprehensive introduction, which I don’t think I can even if I want to, to the neural network (NN). I am a theoretical epidemiologist who manages to have a minimal knoweldge on the theory and use a small set of skills in mathematics, statistics, and computer programming. I use those tools to better understand the epidemiology of the infectious disease and potential impact of the intervention programs such as vaccination.\nSo this is going to be a biased introduction to the fundamental concepts and practical computational skillsof NN to get me started in using and learning the NN. Although I am more familiar with R &gt;&gt; Java &gt;&gt; C++, Julia, I am using python, which I believe happens to be a better language for exploring NN or machine learning in general.\nNN is a collection of neuron, which refers to a simple linear function. The NN involves nonlinear transformation of the output from each neuron and eventually can approximate in theory any functions.\nAnd the core image\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef plot_func(x,y, title):\n    # helper function to plot activation functions\n    plt.plot(x, y)\n    plt.title(title)\n    plt.xlabel('x')\n    plt.ylabel('activation(x)')\n    plt.grid(True)\n    plt.show()\n\nx = np.linspace(-10, 10, 100)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nplot_func(x,sigmoid(x),\"Sigmoid\")"
  },
  {
    "objectID": "baking/lux_julia/index.html",
    "href": "baking/lux_julia/index.html",
    "title": "Julia Lux",
    "section": "",
    "text": "Universal differential equation (UDE)\n\n# SciML (Scientific Machine Learning) Tools\nusing OrdinaryDiffEq, SciMLSensitivity\nusing Optimization, OptimizationOptimisers, OptimizationOptimJL\n\n# Standard Libraries\nusing LinearAlgebra, Statistics\n\n# External Libraries\nusing ComponentArrays, Lux, Zygote, Plots, StableRNGs\ngr()\n\nPlots.GRBackend()\n\n\n# Set a random seed for reproducible behaviour\nrng = StableRNG(1111)\n\nStableRNGs.LehmerRNG(state=0x000000000000000000000000000008af)\n\n\n\n\nData generation\nThe SIR model with a sub-exponential growth is used.\n\nfunction sir_subexp!(du, u, p, t)\n    α, β, γ = p \n    du[1] = - β*u[1]*u[2]^α\n    du[2] = + β*u[1]*u[2]^α - γ*u[2]\n    du[3] = + γ*u[2]\nend\n\nsir_subexp! (generic function with 1 method)\n\n\n# Define the experimental parameter\ntspan = (0.0, 20.0);\n# u0 = 5.0f0 * rand(rng, 2)\nu0 = [0.99, 0.01, 0.0];\np_ = [0.8, 0.4, 0.2];\nprob = ODEProblem(sir_subexp!, u0, tspan, p_);\nsolution = solve(prob, Tsit5(), abstol = 1e-12, reltol = 1e-12, saveat = 1.0);\n\n# Add noise in terms of the mean\nX = Array(solution);\nt = solution.t;\n\nxbar = mean(X, dims=2);   \nnoise_magnitude = 5e-2;\nXn = X .+ (noise_magnitude * xbar) .* randn(rng, eltype(X), size(X));\n\nplot(solution, alpha = 0.75, color = :black, label = [\"True Data\" nothing]);\nscatter!(t, transpose(Xn), color = :red, label = [\"Noisy Data\" nothing])\n\n\n\n\n\n\nUDE model\n\n# Let's define our Universal Differential eqution\nrbf(x) = exp.(-(x .^ 2));\n\n# Multilayer FeedForward\nconst U = Lux.Chain(Lux.Dense(3, 5, rbf), Lux.Dense(5, 5, rbf), \nLux.Dense(5, 5, rbf), Lux.Dense(5, 1))\n\nChain(\n    layer_1 = Dense(3 =&gt; 5, rbf),       # 20 parameters\n    layer_2 = Dense(5 =&gt; 5, rbf),       # 30 parameters\n    layer_3 = Dense(5 =&gt; 5, rbf),       # 30 parameters\n    layer_4 = Dense(5 =&gt; 1),            # 6 parameters\n)         # Total: 86 parameters,\n          #        plus 0 states.\n\n# Get the initial parameters and state variables of the model\np, st = Lux.setup(rng, U)\n\n((layer_1 = (weight = Float32[-0.11597705 -0.5499123 0.10071843; -0.20088743 0.5602648 0.2718303; … ; -0.22440201 -0.57859105 0.7904316; -0.4619576 -0.62989676 0.18545352], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_2 = (weight = Float32[-0.043933477 -0.21508422 … 0.55779475 0.5849693; 0.0011237671 0.006483868 … 0.27549765 -0.2874395; … ; 0.5079049 -0.36002874 … 0.41297784 -0.5777891; -0.5179172 -0.60432595 … -0.18625909 0.06577149], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_3 = (weight = Float32[0.21934992 0.20916325 … -0.357856 -0.27426103; 0.59777355 -0.04514681 … 0.22668682 0.73459923; … ; 0.36797842 0.13955377 … 0.28912562 0.20840885; -0.33154675 0.035615936 … 0.011346816 -0.13401343], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_4 = (weight = Float32[-0.49593353 -0.68478346 … -0.4632702 -0.1476636], bias = Float32[0.0;;])), (layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple()))\n\nconst _st = st\n\n(layer_1 = NamedTuple(), layer_2 = NamedTuple(), layer_3 = NamedTuple(), layer_4 = NamedTuple())\n\n\n# Define the hybrid model\nfunction ude_dynamics!(du, u, p, t, p_true)\n    û = U(u, p, _st)[1] # Network prediction\n    du[1] = dS = - û[1]\n    du[2] = dI = + û[1] - p_true[3]*u[2] \n    du[3] = dR = + p_true[3]*u[2] \nend\n\nude_dynamics! (generic function with 1 method)\n\n\n# Closure with the known parameter\nnn_dynamics!(du, u, p, t) = ude_dynamics!(du, u, p, t, p_)\n\nnn_dynamics! (generic function with 1 method)\n\n# Define the problem\nprob_nn = ODEProblem(nn_dynamics!, Xn[:, 1], tspan, p)\n\nODEProblem with uType Vector{Float64} and tType Float64. In-place: true\ntimespan: (0.0, 20.0)\nu0: 3-element Vector{Float64}:\n 1.0239505612968622\n 0.0034985090690380412\n 0.00031492340046744696\n\n\n# I don't understand the details of the algorithm\n# sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))\n# I just adopted what's provided in the web page: \n# https://docs.sciml.ai/Overview/stable/showcase/missing_physics/\n    \nfunction predict(θ, X = Xn[:, 1], T = t)\n    _prob = remake(prob_nn, u0 = X, tspan = (T[1], T[end]), p = θ)\n    Array(solve(_prob, Tsit5(), saveat = T,\n                abstol = 1e-6, reltol = 1e-6,\n                sensealg=QuadratureAdjoint(autojacvec=ReverseDiffVJP(true))))\nend\n\npredict (generic function with 3 methods)\n\n\nfunction loss(θ)\n    Xhat = predict(θ)\n    mean(abs2, Xn .- Xhat)\nend\n\nloss (generic function with 1 method)\n\n\nlosses = Float64[];\n\ncallback = function (p, l)\n    push!(losses, l)\n    if length(losses) % 50 == 0\n        println(\"Current loss after $(length(losses)) iterations: $(losses[end])\")\n    end\n    return false\nend\n\n#125 (generic function with 1 method)\n\n\nadtype = Optimization.AutoZygote();\noptf = Optimization.OptimizationFunction((x, p) -&gt; loss(x), adtype);\noptprob = Optimization.OptimizationProblem(optf, ComponentVector{Float64}(p));\n\nmxiter = 2000\n\n2000\n\nres1 = Optimization.solve(optprob, ADAM(), callback = callback, maxiters = mxiter);\nprintln(\"Training loss after $(length(losses)) iterations: $(losses[end])\")\n\nTraining loss after 2001 iterations: 0.0015564208688977953\n\n\n# You can optimize further by using LBFGS\noptprob2 = Optimization.OptimizationProblem(optf, res1.u)\n\nOptimizationProblem. In-place: true\nu0: ComponentVector{Float64}(layer_1 = (weight = [-0.05170182389781148 -0.6044817578829536 0.030221881765418415; -0.3212356181883164 0.6510350091355674 0.44802414727952533; … ; -0.2971002151491799 -0.5198718000300768 0.8494439396527995; -0.5240838499404261 -0.5805885464635301 0.2207764494806622], bias = [0.07034258756546354; -0.1248750018555649; … ; -0.10460911230309018; -0.09231330051973555;;]), layer_2 = (weight = [-0.16444552836939919 -0.3914934840987012 … 0.4595876886814785 0.48596336484236335; 0.061104866379551544 0.09809356182285524 … 0.3325709332030265 -0.23929486434292638; … ; 0.44222617028560024 -0.07489082174986872 … 0.38613240122330794 -0.609809451956525; -0.42554673750885325 -0.3912678259711914 … -0.0705070098211834 0.1835005315863163], bias = [-0.1024237112798211; 0.05522672749980388; … ; -0.06460784045513147; 0.08515934187060795;;]), layer_3 = (weight = [0.30753421402720227 0.29471143354807966 … -0.26966507778103693 -0.19068344291262843; 0.6803575575882911 0.035025314411197044 … 0.3095389929940732 0.8122600471132518; … ; 0.4338542747330682 0.20165221719458867 … 0.3525999208278827 0.27010233878411616; -0.20708092822357524 0.1890743367618981 … 0.16598038476857452 -0.0012012794517766243], bias = [0.0853924513699166; 0.08006728472122455; … ; 0.061942487865868964; 0.1530663635694471;;]), layer_4 = (weight = [-0.4208452558700493 -0.6144846254336126 … -0.40340199958423073 -0.06553529479079494], bias = [0.09174711461758511;;]))\n\nres2 = Optimization.solve(optprob2, Optim.LBFGS(), callback = callback, maxiters = 1000);\nprintln(\"Final training loss after $(length(losses)) iterations: $(losses[end])\")\n\nFinal training loss after 3002 iterations: 0.0003723725121765437\n\n\n# Rename the best candidate\np_trained = res2.u\n\nComponentVector{Float64}(layer_1 = (weight = [0.1316220200349473 -0.6520282050791907 0.008318466489230203; -0.4557052571206415 0.852778797405868 0.6422456105470944; … ; -0.3017967416294747 -0.3539416097805037 0.9435890641280813; -0.6208672577811719 -0.8250782773685028 0.463368746591991], bias = [0.1811403798125198; 0.1295565740621664; … ; 0.14387087901155535; -0.1883283800836187;;]), layer_2 = (weight = [-0.5224273510619183 -0.7851736034397837 … 0.13246567539583712 0.28492163069656595; 0.2121727337798046 0.2580716617374057 … 0.46828242306889367 -0.18639886649813667; … ; 0.31182369287889583 -0.13443324204943075 … 0.18614501798809488 -0.9123175434094871; -0.5023288426177288 -0.4943627237480249 … -0.05984671314326837 0.14167837142544432], bias = [-0.4580492880391406; 0.2156300092628208; … ; -0.18292941364366094; 0.014071394516324979;;]), layer_3 = (weight = [0.17533364576159247 0.2053893580840498 … -0.3080681971901958 -0.3356279391033146; 0.7087531824294593 0.0960694141143031 … 0.3481334453513509 0.8277491605925765; … ; 0.4365583552750147 0.20075642462931304 … 0.37058208065336945 0.2762370994412038; -0.18955475880245964 0.23766428666711154 … 0.23703263569676214 0.011769794045616492], bias = [-0.11836077082556237; 0.07082483083187767; … ; 0.058396858004157386; 0.15828264366434497;;]), layer_4 = (weight = [-0.39882746395912166 -0.6321529793894407 … -0.38424206982191056 -0.18894617437931902], bias = [0.4328709022831265;;]))\n\n\n# Plot the losses\npl_losses = plot(1:mxiter, losses[1:mxiter], yaxis = :log10, xaxis = :log10,\n                 xlabel = \"Iterations\", ylabel = \"Loss\", label = \"ADAM\", color = :blue)\n\n\n\nplot!((mxiter+1):length(losses), losses[(mxiter+1):end], yaxis = :log10, xaxis = :log10,\n      xlabel = \"Iterations\", ylabel = \"Loss\", label = \"BFGS\", color = :red)\n\n\n\n\n\n## Analysis of the trained network\n# Plot the data and the approximation\nts = first(solution.t):(mean(diff(solution.t)) / 2):last(solution.t)\n\n0.0:0.5:20.0\n\nXhat = predict(p_trained, Xn[:, 1], ts)\n\n3×41 Matrix{Float64}:\n 1.02395      1.0072      0.991471    …  0.150335  0.140635  0.13015\n 0.00349851   0.0190933   0.0322405      0.163083  0.156799  0.151863\n 0.000314923  0.00146965  0.00405242     0.714346  0.73033   0.745751\n\n# Trained on noisy data vs real solution\npl_trajectory = plot(ts, transpose(Xhat), xlabel = \"t\", \n                     ylabel = \"S(t), I(t), R(t)\", color = :red,\n                     label = [\"UDE Approximation\" nothing])\n\n\n\nscatter!(solution.t, transpose(Xn), color = :black, label = [\"Measurements\" nothing])\n\n\n\n\n\n# Ideal unknown interactions of the predictor\n# Ybar = [-p_[2] * (Xhat[1, :] .* Xhat[2, :])'; p_[3] * (Xhat[1, :] .* Xhat[2, :])']\n# Ybar = [p_[2] .* Xhat[1,:] .* (Xhat[2,:].^p_[1])]\nYbar = transpose([p_[2] * Xhat[1,i] * (Xhat[2,i].^p_[1]) for i ∈ 1:41, j ∈ 1:1])\n\n1×41 transpose(::Matrix{Float64}) with eltype Float64:\n 0.00444064  0.0169778  0.0254133  …  0.0140944  0.012777  0.0115257\n\n# Neural network guess\nYhat = U(Xhat, p_trained, st)[1]\n\n1×41 Matrix{Float64}:\n 0.0351306  0.0321994  0.0309596  …  0.018876  0.0200484  0.0220433\n\n\npl_reconstruction = plot(ts, transpose(Yhat), xlabel = \"t\", \n                         ylabel = \"U(S,I,R)\", color = :red,\n                         label = [\"UDE Approximation\" nothing]);\n\nplot!(ts, transpose(Ybar), color = :black, label = [\"True Interaction\" nothing])\n\n\n\n\n\n# Plot the error\npl_reconstruction_error = plot(ts, norm.(eachcol(Ybar - Yhat)), yaxis = :log, xlabel = \"t\",\n                               ylabel = \"L2-Error\", label = nothing, color = :red);\npl_missing = plot(pl_reconstruction, pl_reconstruction_error, layout = (2, 1));\n\npl_overall = plot(pl_trajectory, pl_missing)"
  },
  {
    "objectID": "baking/python_tensorflow_basics/index.html",
    "href": "baking/python_tensorflow_basics/index.html",
    "title": "Python",
    "section": "",
    "text": "import tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\n\nTensorFlow version: 2.10.1\n\n\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n\n\n\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10)\n])\n\n\npredictions = model(x_train[:1]).numpy()\npredictions\n\narray([[-1.0659655e+00, -5.6349498e-01, -9.2068411e-02, -9.8071992e-05,\n         5.1046234e-01,  2.9953119e-01,  1.5084814e-01, -5.8150619e-01,\n        -2.9266942e-01, -8.9257973e-01]], dtype=float32)\n\n# want to see as \ntf.nn.softmax(predictions).numpy()\n\narray([[0.03950126, 0.06528768, 0.10460903, 0.11468626, 0.19109307,\n        0.15475287, 0.13337255, 0.06412229, 0.08559517, 0.04697984]],\n      dtype=float32)\n\n\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\nloss_fn(y_train[:1], predictions).numpy()\n\n1.8659258\n\n\n\nmodel.compile(optimizer='adam',\n              loss=loss_fn,\n              metrics=['accuracy'])\n\n\nmodel.fit(x_train, y_train, epochs=5)\n\nEpoch 1/5\n\n   1/1875 [..............................] - ETA: 16:46 - loss: 2.4121 - accuracy: 0.0312\n  18/1875 [..............................] - ETA: 5s - loss: 1.9974 - accuracy: 0.3559   \n  35/1875 [..............................] - ETA: 5s - loss: 1.6133 - accuracy: 0.5232\n  48/1875 [..............................] - ETA: 6s - loss: 1.4181 - accuracy: 0.5898\n  61/1875 [..............................] - ETA: 6s - loss: 1.2882 - accuracy: 0.6235\n  76/1875 [&gt;.............................] - ETA: 6s - loss: 1.1568 - accuracy: 0.6653\n  93/1875 [&gt;.............................] - ETA: 6s - loss: 1.0512 - accuracy: 0.6972\n 110/1875 [&gt;.............................] - ETA: 5s - loss: 0.9665 - accuracy: 0.7193\n 127/1875 [=&gt;............................] - ETA: 5s - loss: 0.8994 - accuracy: 0.7377\n 144/1875 [=&gt;............................] - ETA: 5s - loss: 0.8495 - accuracy: 0.7530\n 161/1875 [=&gt;............................] - ETA: 5s - loss: 0.8058 - accuracy: 0.7659\n 178/1875 [=&gt;............................] - ETA: 5s - loss: 0.7704 - accuracy: 0.7758\n 196/1875 [==&gt;...........................] - ETA: 5s - loss: 0.7353 - accuracy: 0.7865\n 205/1875 [==&gt;...........................] - ETA: 5s - loss: 0.7188 - accuracy: 0.7915\n 215/1875 [==&gt;...........................] - ETA: 5s - loss: 0.7039 - accuracy: 0.7961\n 233/1875 [==&gt;...........................] - ETA: 5s - loss: 0.6762 - accuracy: 0.8036\n 247/1875 [==&gt;...........................] - ETA: 5s - loss: 0.6584 - accuracy: 0.8091\n 265/1875 [===&gt;..........................] - ETA: 5s - loss: 0.6410 - accuracy: 0.8138\n 280/1875 [===&gt;..........................] - ETA: 5s - loss: 0.6237 - accuracy: 0.8191\n 291/1875 [===&gt;..........................] - ETA: 5s - loss: 0.6144 - accuracy: 0.8214\n 300/1875 [===&gt;..........................] - ETA: 5s - loss: 0.6070 - accuracy: 0.8234\n 310/1875 [===&gt;..........................] - ETA: 5s - loss: 0.5993 - accuracy: 0.8249\n 324/1875 [====&gt;.........................] - ETA: 5s - loss: 0.5897 - accuracy: 0.8276\n 334/1875 [====&gt;.........................] - ETA: 5s - loss: 0.5825 - accuracy: 0.8298\n 349/1875 [====&gt;.........................] - ETA: 5s - loss: 0.5722 - accuracy: 0.8326\n 364/1875 [====&gt;.........................] - ETA: 5s - loss: 0.5621 - accuracy: 0.8350\n 378/1875 [=====&gt;........................] - ETA: 5s - loss: 0.5544 - accuracy: 0.8371\n 390/1875 [=====&gt;........................] - ETA: 5s - loss: 0.5493 - accuracy: 0.8385\n 407/1875 [=====&gt;........................] - ETA: 5s - loss: 0.5428 - accuracy: 0.8408\n 425/1875 [=====&gt;........................] - ETA: 5s - loss: 0.5352 - accuracy: 0.8428\n 442/1875 [======&gt;.......................] - ETA: 5s - loss: 0.5293 - accuracy: 0.8449\n 460/1875 [======&gt;.......................] - ETA: 4s - loss: 0.5214 - accuracy: 0.8467\n 476/1875 [======&gt;.......................] - ETA: 4s - loss: 0.5147 - accuracy: 0.8487\n 492/1875 [======&gt;.......................] - ETA: 4s - loss: 0.5070 - accuracy: 0.8511\n 507/1875 [=======&gt;......................] - ETA: 4s - loss: 0.5023 - accuracy: 0.8527\n 523/1875 [=======&gt;......................] - ETA: 4s - loss: 0.4967 - accuracy: 0.8545\n 537/1875 [=======&gt;......................] - ETA: 4s - loss: 0.4917 - accuracy: 0.8557\n 551/1875 [=======&gt;......................] - ETA: 4s - loss: 0.4876 - accuracy: 0.8571\n 565/1875 [========&gt;.....................] - ETA: 4s - loss: 0.4820 - accuracy: 0.8585\n 579/1875 [========&gt;.....................] - ETA: 4s - loss: 0.4785 - accuracy: 0.8598\n 594/1875 [========&gt;.....................] - ETA: 4s - loss: 0.4730 - accuracy: 0.8615\n 609/1875 [========&gt;.....................] - ETA: 4s - loss: 0.4682 - accuracy: 0.8630\n 626/1875 [=========&gt;....................] - ETA: 4s - loss: 0.4616 - accuracy: 0.8648\n 640/1875 [=========&gt;....................] - ETA: 4s - loss: 0.4562 - accuracy: 0.8665\n 656/1875 [=========&gt;....................] - ETA: 4s - loss: 0.4515 - accuracy: 0.8679\n 673/1875 [=========&gt;....................] - ETA: 4s - loss: 0.4464 - accuracy: 0.8695\n 687/1875 [=========&gt;....................] - ETA: 4s - loss: 0.4427 - accuracy: 0.8706\n 697/1875 [==========&gt;...................] - ETA: 4s - loss: 0.4398 - accuracy: 0.8713\n 716/1875 [==========&gt;...................] - ETA: 4s - loss: 0.4357 - accuracy: 0.8724\n 738/1875 [==========&gt;...................] - ETA: 3s - loss: 0.4294 - accuracy: 0.8744\n 754/1875 [===========&gt;..................] - ETA: 3s - loss: 0.4242 - accuracy: 0.8761\n 772/1875 [===========&gt;..................] - ETA: 3s - loss: 0.4202 - accuracy: 0.8772\n 782/1875 [===========&gt;..................] - ETA: 3s - loss: 0.4170 - accuracy: 0.8783\n 793/1875 [===========&gt;..................] - ETA: 3s - loss: 0.4150 - accuracy: 0.8789\n 810/1875 [===========&gt;..................] - ETA: 3s - loss: 0.4118 - accuracy: 0.8798\n 832/1875 [============&gt;.................] - ETA: 3s - loss: 0.4079 - accuracy: 0.8807\n 850/1875 [============&gt;.................] - ETA: 3s - loss: 0.4060 - accuracy: 0.8812\n 868/1875 [============&gt;.................] - ETA: 3s - loss: 0.4042 - accuracy: 0.8816\n 887/1875 [=============&gt;................] - ETA: 3s - loss: 0.4007 - accuracy: 0.8824\n 907/1875 [=============&gt;................] - ETA: 3s - loss: 0.3968 - accuracy: 0.8835\n 922/1875 [=============&gt;................] - ETA: 3s - loss: 0.3943 - accuracy: 0.8843\n 932/1875 [=============&gt;................] - ETA: 3s - loss: 0.3928 - accuracy: 0.8848\n 941/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3908 - accuracy: 0.8854\n 950/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3896 - accuracy: 0.8858\n 961/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3875 - accuracy: 0.8864\n 970/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3855 - accuracy: 0.8869\n 982/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3838 - accuracy: 0.8874\n 995/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3821 - accuracy: 0.8882\n1008/1875 [===============&gt;..............] - ETA: 3s - loss: 0.3802 - accuracy: 0.8888\n1024/1875 [===============&gt;..............] - ETA: 2s - loss: 0.3780 - accuracy: 0.8896\n1038/1875 [===============&gt;..............] - ETA: 2s - loss: 0.3761 - accuracy: 0.8901\n1050/1875 [===============&gt;..............] - ETA: 2s - loss: 0.3737 - accuracy: 0.8908\n1063/1875 [================&gt;.............] - ETA: 2s - loss: 0.3715 - accuracy: 0.8914\n1075/1875 [================&gt;.............] - ETA: 2s - loss: 0.3696 - accuracy: 0.8921\n1093/1875 [================&gt;.............] - ETA: 2s - loss: 0.3666 - accuracy: 0.8929\n1111/1875 [================&gt;.............] - ETA: 2s - loss: 0.3644 - accuracy: 0.8935\n1130/1875 [=================&gt;............] - ETA: 2s - loss: 0.3616 - accuracy: 0.8942\n1147/1875 [=================&gt;............] - ETA: 2s - loss: 0.3593 - accuracy: 0.8949\n1164/1875 [=================&gt;............] - ETA: 2s - loss: 0.3575 - accuracy: 0.8954\n1178/1875 [=================&gt;............] - ETA: 2s - loss: 0.3550 - accuracy: 0.8962\n1188/1875 [==================&gt;...........] - ETA: 2s - loss: 0.3539 - accuracy: 0.8965\n1202/1875 [==================&gt;...........] - ETA: 2s - loss: 0.3518 - accuracy: 0.8972\n1214/1875 [==================&gt;...........] - ETA: 2s - loss: 0.3506 - accuracy: 0.8975\n1227/1875 [==================&gt;...........] - ETA: 2s - loss: 0.3488 - accuracy: 0.8980\n1239/1875 [==================&gt;...........] - ETA: 2s - loss: 0.3473 - accuracy: 0.8985\n1250/1875 [===================&gt;..........] - ETA: 2s - loss: 0.3461 - accuracy: 0.8989\n1265/1875 [===================&gt;..........] - ETA: 2s - loss: 0.3451 - accuracy: 0.8994\n1280/1875 [===================&gt;..........] - ETA: 2s - loss: 0.3435 - accuracy: 0.8999\n1294/1875 [===================&gt;..........] - ETA: 2s - loss: 0.3421 - accuracy: 0.9003\n1305/1875 [===================&gt;..........] - ETA: 2s - loss: 0.3415 - accuracy: 0.9005\n1319/1875 [====================&gt;.........] - ETA: 1s - loss: 0.3402 - accuracy: 0.9009\n1334/1875 [====================&gt;.........] - ETA: 1s - loss: 0.3391 - accuracy: 0.9012\n1348/1875 [====================&gt;.........] - ETA: 1s - loss: 0.3374 - accuracy: 0.9017\n1359/1875 [====================&gt;.........] - ETA: 1s - loss: 0.3364 - accuracy: 0.9021\n1369/1875 [====================&gt;.........] - ETA: 1s - loss: 0.3357 - accuracy: 0.9022\n1379/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3346 - accuracy: 0.9025\n1389/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3336 - accuracy: 0.9028\n1400/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3328 - accuracy: 0.9029\n1410/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3321 - accuracy: 0.9031\n1420/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3310 - accuracy: 0.9034\n1431/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3299 - accuracy: 0.9038\n1444/1875 [======================&gt;.......] - ETA: 1s - loss: 0.3285 - accuracy: 0.9042\n1458/1875 [======================&gt;.......] - ETA: 1s - loss: 0.3270 - accuracy: 0.9046\n1472/1875 [======================&gt;.......] - ETA: 1s - loss: 0.3255 - accuracy: 0.9050\n1483/1875 [======================&gt;.......] - ETA: 1s - loss: 0.3244 - accuracy: 0.9054\n1492/1875 [======================&gt;.......] - ETA: 1s - loss: 0.3235 - accuracy: 0.9057\n1501/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3227 - accuracy: 0.9059\n1513/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3219 - accuracy: 0.9061\n1526/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3214 - accuracy: 0.9062\n1536/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3207 - accuracy: 0.9065\n1546/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3202 - accuracy: 0.9066\n1556/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3193 - accuracy: 0.9069\n1568/1875 [========================&gt;.....] - ETA: 1s - loss: 0.3181 - accuracy: 0.9072\n1580/1875 [========================&gt;.....] - ETA: 1s - loss: 0.3170 - accuracy: 0.9074\n1592/1875 [========================&gt;.....] - ETA: 1s - loss: 0.3163 - accuracy: 0.9076\n1604/1875 [========================&gt;.....] - ETA: 1s - loss: 0.3157 - accuracy: 0.9077\n1615/1875 [========================&gt;.....] - ETA: 0s - loss: 0.3147 - accuracy: 0.9080\n1624/1875 [========================&gt;.....] - ETA: 0s - loss: 0.3142 - accuracy: 0.9082\n1634/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3132 - accuracy: 0.9085\n1646/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3123 - accuracy: 0.9087\n1655/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3116 - accuracy: 0.9089\n1667/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3107 - accuracy: 0.9092\n1679/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3095 - accuracy: 0.9096\n1690/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3086 - accuracy: 0.9098\n1701/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3078 - accuracy: 0.9100\n1712/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3067 - accuracy: 0.9103\n1723/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3057 - accuracy: 0.9105\n1734/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3046 - accuracy: 0.9108\n1745/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3037 - accuracy: 0.9110\n1756/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3031 - accuracy: 0.9112\n1767/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3023 - accuracy: 0.9114\n1777/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3018 - accuracy: 0.9116\n1788/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3011 - accuracy: 0.9119\n1799/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3001 - accuracy: 0.9122\n1810/1875 [===========================&gt;..] - ETA: 0s - loss: 0.2995 - accuracy: 0.9125\n1821/1875 [============================&gt;.] - ETA: 0s - loss: 0.2988 - accuracy: 0.9127\n1833/1875 [============================&gt;.] - ETA: 0s - loss: 0.2977 - accuracy: 0.9130\n1843/1875 [============================&gt;.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9132\n1854/1875 [============================&gt;.] - ETA: 0s - loss: 0.2962 - accuracy: 0.9134\n1865/1875 [============================&gt;.] - ETA: 0s - loss: 0.2957 - accuracy: 0.9135\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.2951 - accuracy: 0.9137\nEpoch 2/5\n\n   1/1875 [..............................] - ETA: 7s - loss: 0.0998 - accuracy: 1.0000\n  12/1875 [..............................] - ETA: 8s - loss: 0.1320 - accuracy: 0.9583\n  24/1875 [..............................] - ETA: 8s - loss: 0.1455 - accuracy: 0.9583\n  34/1875 [..............................] - ETA: 8s - loss: 0.1534 - accuracy: 0.9577\n  44/1875 [..............................] - ETA: 9s - loss: 0.1484 - accuracy: 0.9567\n  54/1875 [..............................] - ETA: 9s - loss: 0.1582 - accuracy: 0.9554\n  65/1875 [&gt;.............................] - ETA: 9s - loss: 0.1585 - accuracy: 0.9514\n  74/1875 [&gt;.............................] - ETA: 9s - loss: 0.1649 - accuracy: 0.9502\n  86/1875 [&gt;.............................] - ETA: 8s - loss: 0.1564 - accuracy: 0.9517\n  97/1875 [&gt;.............................] - ETA: 8s - loss: 0.1564 - accuracy: 0.9536\n 108/1875 [&gt;.............................] - ETA: 8s - loss: 0.1565 - accuracy: 0.9540\n 118/1875 [&gt;.............................] - ETA: 8s - loss: 0.1583 - accuracy: 0.9526\n 126/1875 [=&gt;............................] - ETA: 8s - loss: 0.1587 - accuracy: 0.9524\n 134/1875 [=&gt;............................] - ETA: 9s - loss: 0.1573 - accuracy: 0.9524\n 145/1875 [=&gt;............................] - ETA: 8s - loss: 0.1566 - accuracy: 0.9524\n 155/1875 [=&gt;............................] - ETA: 8s - loss: 0.1598 - accuracy: 0.9518\n 163/1875 [=&gt;............................] - ETA: 9s - loss: 0.1588 - accuracy: 0.9526\n 171/1875 [=&gt;............................] - ETA: 9s - loss: 0.1570 - accuracy: 0.9529\n 180/1875 [=&gt;............................] - ETA: 9s - loss: 0.1567 - accuracy: 0.9526\n 188/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1561 - accuracy: 0.9530\n 196/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1573 - accuracy: 0.9520\n 204/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1591 - accuracy: 0.9516\n 212/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1571 - accuracy: 0.9527\n 222/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1591 - accuracy: 0.9520\n 230/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1596 - accuracy: 0.9524\n 238/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1602 - accuracy: 0.9522\n 247/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1631 - accuracy: 0.9515\n 255/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1619 - accuracy: 0.9520\n 264/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1593 - accuracy: 0.9529\n 275/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1594 - accuracy: 0.9528\n 284/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1594 - accuracy: 0.9529\n 292/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1602 - accuracy: 0.9525\n 301/1875 [===&gt;..........................] - ETA: 8s - loss: 0.1595 - accuracy: 0.9532\n 310/1875 [===&gt;..........................] - ETA: 8s - loss: 0.1573 - accuracy: 0.9536\n 321/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1578 - accuracy: 0.9535\n 332/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1571 - accuracy: 0.9540\n 341/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1561 - accuracy: 0.9542\n 351/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1553 - accuracy: 0.9546\n 361/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1554 - accuracy: 0.9547\n 371/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1561 - accuracy: 0.9543\n 381/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1570 - accuracy: 0.9539\n 390/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1569 - accuracy: 0.9538\n 400/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1566 - accuracy: 0.9537\n 410/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1554 - accuracy: 0.9540\n 420/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1544 - accuracy: 0.9542\n 429/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1531 - accuracy: 0.9545\n 440/1875 [======&gt;.......................] - ETA: 8s - loss: 0.1546 - accuracy: 0.9541\n 449/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1549 - accuracy: 0.9541\n 456/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1553 - accuracy: 0.9540\n 463/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1544 - accuracy: 0.9544\n 471/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1539 - accuracy: 0.9546\n 481/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1540 - accuracy: 0.9543\n 492/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1541 - accuracy: 0.9544\n 501/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1536 - accuracy: 0.9546\n 512/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1526 - accuracy: 0.9551\n 522/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1535 - accuracy: 0.9549\n 530/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1534 - accuracy: 0.9549\n 538/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1527 - accuracy: 0.9553\n 547/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1535 - accuracy: 0.9550\n 557/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1537 - accuracy: 0.9548\n 568/1875 [========&gt;.....................] - ETA: 7s - loss: 0.1533 - accuracy: 0.9548\n 578/1875 [========&gt;.....................] - ETA: 7s - loss: 0.1531 - accuracy: 0.9548\n 589/1875 [========&gt;.....................] - ETA: 7s - loss: 0.1531 - accuracy: 0.9547\n 600/1875 [========&gt;.....................] - ETA: 7s - loss: 0.1541 - accuracy: 0.9542\n 610/1875 [========&gt;.....................] - ETA: 7s - loss: 0.1533 - accuracy: 0.9545\n 621/1875 [========&gt;.....................] - ETA: 6s - loss: 0.1537 - accuracy: 0.9543\n 631/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1532 - accuracy: 0.9545\n 642/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1528 - accuracy: 0.9546\n 653/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1525 - accuracy: 0.9547\n 664/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1534 - accuracy: 0.9543\n 674/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1534 - accuracy: 0.9542\n 684/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1537 - accuracy: 0.9541\n 692/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1537 - accuracy: 0.9540\n 702/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1542 - accuracy: 0.9539\n 712/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1542 - accuracy: 0.9540\n 722/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1543 - accuracy: 0.9540\n 733/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1542 - accuracy: 0.9540\n 744/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1542 - accuracy: 0.9539\n 754/1875 [===========&gt;..................] - ETA: 6s - loss: 0.1535 - accuracy: 0.9542\n 765/1875 [===========&gt;..................] - ETA: 6s - loss: 0.1534 - accuracy: 0.9542\n 774/1875 [===========&gt;..................] - ETA: 6s - loss: 0.1532 - accuracy: 0.9543\n 782/1875 [===========&gt;..................] - ETA: 6s - loss: 0.1529 - accuracy: 0.9543\n 792/1875 [===========&gt;..................] - ETA: 5s - loss: 0.1526 - accuracy: 0.9543\n 801/1875 [===========&gt;..................] - ETA: 5s - loss: 0.1528 - accuracy: 0.9541\n 809/1875 [===========&gt;..................] - ETA: 5s - loss: 0.1521 - accuracy: 0.9544\n 817/1875 [============&gt;.................] - ETA: 5s - loss: 0.1516 - accuracy: 0.9546\n 825/1875 [============&gt;.................] - ETA: 5s - loss: 0.1509 - accuracy: 0.9548\n 833/1875 [============&gt;.................] - ETA: 5s - loss: 0.1506 - accuracy: 0.9548\n 842/1875 [============&gt;.................] - ETA: 5s - loss: 0.1506 - accuracy: 0.9547\n 850/1875 [============&gt;.................] - ETA: 5s - loss: 0.1503 - accuracy: 0.9549\n 858/1875 [============&gt;.................] - ETA: 5s - loss: 0.1503 - accuracy: 0.9548\n 866/1875 [============&gt;.................] - ETA: 5s - loss: 0.1502 - accuracy: 0.9549\n 874/1875 [============&gt;.................] - ETA: 5s - loss: 0.1504 - accuracy: 0.9548\n 882/1875 [=============&gt;................] - ETA: 5s - loss: 0.1501 - accuracy: 0.9548\n 889/1875 [=============&gt;................] - ETA: 5s - loss: 0.1505 - accuracy: 0.9545\n 896/1875 [=============&gt;................] - ETA: 5s - loss: 0.1508 - accuracy: 0.9545\n 903/1875 [=============&gt;................] - ETA: 5s - loss: 0.1515 - accuracy: 0.9543\n 910/1875 [=============&gt;................] - ETA: 5s - loss: 0.1513 - accuracy: 0.9545\n 917/1875 [=============&gt;................] - ETA: 5s - loss: 0.1510 - accuracy: 0.9545\n 924/1875 [=============&gt;................] - ETA: 5s - loss: 0.1506 - accuracy: 0.9545\n 931/1875 [=============&gt;................] - ETA: 5s - loss: 0.1506 - accuracy: 0.9546\n 939/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1503 - accuracy: 0.9545\n 947/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1502 - accuracy: 0.9545\n 957/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1503 - accuracy: 0.9545\n 967/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1499 - accuracy: 0.9547\n 976/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1502 - accuracy: 0.9544\n 986/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1498 - accuracy: 0.9545\n 997/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1498 - accuracy: 0.9545\n1008/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1492 - accuracy: 0.9549\n1019/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1493 - accuracy: 0.9549\n1029/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1489 - accuracy: 0.9550\n1039/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1487 - accuracy: 0.9551\n1048/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1483 - accuracy: 0.9552\n1058/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1478 - accuracy: 0.9554\n1069/1875 [================&gt;.............] - ETA: 4s - loss: 0.1473 - accuracy: 0.9555\n1079/1875 [================&gt;.............] - ETA: 4s - loss: 0.1471 - accuracy: 0.9556\n1087/1875 [================&gt;.............] - ETA: 4s - loss: 0.1467 - accuracy: 0.9557\n1095/1875 [================&gt;.............] - ETA: 4s - loss: 0.1462 - accuracy: 0.9559\n1103/1875 [================&gt;.............] - ETA: 4s - loss: 0.1465 - accuracy: 0.9557\n1111/1875 [================&gt;.............] - ETA: 4s - loss: 0.1460 - accuracy: 0.9559\n1122/1875 [================&gt;.............] - ETA: 4s - loss: 0.1460 - accuracy: 0.9559\n1132/1875 [=================&gt;............] - ETA: 4s - loss: 0.1459 - accuracy: 0.9560\n1140/1875 [=================&gt;............] - ETA: 4s - loss: 0.1461 - accuracy: 0.9559\n1147/1875 [=================&gt;............] - ETA: 4s - loss: 0.1463 - accuracy: 0.9559\n1154/1875 [=================&gt;............] - ETA: 4s - loss: 0.1462 - accuracy: 0.9558\n1161/1875 [=================&gt;............] - ETA: 4s - loss: 0.1464 - accuracy: 0.9558\n1168/1875 [=================&gt;............] - ETA: 4s - loss: 0.1463 - accuracy: 0.9559\n1176/1875 [=================&gt;............] - ETA: 4s - loss: 0.1464 - accuracy: 0.9558\n1184/1875 [=================&gt;............] - ETA: 3s - loss: 0.1461 - accuracy: 0.9559\n1191/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1458 - accuracy: 0.9560\n1199/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1458 - accuracy: 0.9559\n1206/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1457 - accuracy: 0.9560\n1214/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1457 - accuracy: 0.9561\n1221/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1457 - accuracy: 0.9561\n1229/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1455 - accuracy: 0.9562\n1237/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1455 - accuracy: 0.9561\n1245/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1456 - accuracy: 0.9560\n1253/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1451 - accuracy: 0.9561\n1260/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1450 - accuracy: 0.9561\n1268/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1454 - accuracy: 0.9560\n1277/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1455 - accuracy: 0.9559\n1288/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1452 - accuracy: 0.9560\n1299/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1451 - accuracy: 0.9560\n1309/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1446 - accuracy: 0.9561\n1319/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1445 - accuracy: 0.9561\n1329/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1444 - accuracy: 0.9562\n1339/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1442 - accuracy: 0.9562\n1350/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1445 - accuracy: 0.9562\n1361/1875 [====================&gt;.........] - ETA: 2s - loss: 0.1443 - accuracy: 0.9563\n1370/1875 [====================&gt;.........] - ETA: 2s - loss: 0.1443 - accuracy: 0.9563\n1380/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1444 - accuracy: 0.9562\n1391/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1445 - accuracy: 0.9562\n1400/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1446 - accuracy: 0.9561\n1408/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1445 - accuracy: 0.9560\n1417/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1442 - accuracy: 0.9561\n1428/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1436 - accuracy: 0.9563\n1438/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1440 - accuracy: 0.9562\n1449/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1437 - accuracy: 0.9563\n1460/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1432 - accuracy: 0.9564\n1470/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1429 - accuracy: 0.9565\n1478/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1429 - accuracy: 0.9565\n1486/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1430 - accuracy: 0.9565\n1494/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1429 - accuracy: 0.9566\n1502/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1429 - accuracy: 0.9566\n1510/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1428 - accuracy: 0.9567\n1518/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1429 - accuracy: 0.9567\n1526/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1425 - accuracy: 0.9568\n1533/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1425 - accuracy: 0.9568\n1541/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1424 - accuracy: 0.9569\n1548/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1424 - accuracy: 0.9569\n1555/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1423 - accuracy: 0.9569\n1562/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1421 - accuracy: 0.9569\n1569/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1420 - accuracy: 0.9570\n1576/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1422 - accuracy: 0.9570\n1583/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1423 - accuracy: 0.9569\n1590/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1420 - accuracy: 0.9570\n1598/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1421 - accuracy: 0.9570\n1605/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1420 - accuracy: 0.9571\n1613/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1419 - accuracy: 0.9571\n1624/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1422 - accuracy: 0.9570\n1635/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1420 - accuracy: 0.9571\n1645/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1421 - accuracy: 0.9570\n1656/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1423 - accuracy: 0.9570\n1667/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1422 - accuracy: 0.9570\n1678/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1420 - accuracy: 0.9571\n1689/1875 [==========================&gt;...] - ETA: 1s - loss: 0.1418 - accuracy: 0.9572\n1700/1875 [==========================&gt;...] - ETA: 1s - loss: 0.1418 - accuracy: 0.9572\n1709/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1417 - accuracy: 0.9572\n1718/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1416 - accuracy: 0.9572\n1729/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1419 - accuracy: 0.9572\n1739/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1417 - accuracy: 0.9573\n1750/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1414 - accuracy: 0.9573\n1761/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1415 - accuracy: 0.9573\n1771/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1413 - accuracy: 0.9574\n1781/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1414 - accuracy: 0.9574\n1791/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1413 - accuracy: 0.9575\n1802/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1412 - accuracy: 0.9575\n1812/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1411 - accuracy: 0.9575\n1820/1875 [============================&gt;.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9576\n1828/1875 [============================&gt;.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9576\n1835/1875 [============================&gt;.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9576\n1843/1875 [============================&gt;.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9577\n1851/1875 [============================&gt;.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9577\n1859/1875 [============================&gt;.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9577\n1866/1875 [============================&gt;.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9577\n1873/1875 [============================&gt;.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9577\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.1409 - accuracy: 0.9577\nEpoch 3/5\n\n   1/1875 [..............................] - ETA: 14s - loss: 0.1379 - accuracy: 0.9375\n   8/1875 [..............................] - ETA: 13s - loss: 0.1125 - accuracy: 0.9648\n  16/1875 [..............................] - ETA: 13s - loss: 0.1006 - accuracy: 0.9727\n  24/1875 [..............................] - ETA: 13s - loss: 0.1249 - accuracy: 0.9688\n  32/1875 [..............................] - ETA: 12s - loss: 0.1097 - accuracy: 0.9717\n  40/1875 [..............................] - ETA: 12s - loss: 0.1074 - accuracy: 0.9703\n  48/1875 [..............................] - ETA: 12s - loss: 0.1207 - accuracy: 0.9668\n  56/1875 [..............................] - ETA: 12s - loss: 0.1182 - accuracy: 0.9660\n  63/1875 [&gt;.............................] - ETA: 12s - loss: 0.1149 - accuracy: 0.9673\n  71/1875 [&gt;.............................] - ETA: 12s - loss: 0.1115 - accuracy: 0.9679\n  79/1875 [&gt;.............................] - ETA: 12s - loss: 0.1111 - accuracy: 0.9668\n  87/1875 [&gt;.............................] - ETA: 12s - loss: 0.1143 - accuracy: 0.9644\n  97/1875 [&gt;.............................] - ETA: 12s - loss: 0.1111 - accuracy: 0.9659\n 108/1875 [&gt;.............................] - ETA: 11s - loss: 0.1131 - accuracy: 0.9661\n 118/1875 [&gt;.............................] - ETA: 11s - loss: 0.1117 - accuracy: 0.9674\n 128/1875 [=&gt;............................] - ETA: 11s - loss: 0.1131 - accuracy: 0.9666\n 137/1875 [=&gt;............................] - ETA: 11s - loss: 0.1136 - accuracy: 0.9665\n 146/1875 [=&gt;............................] - ETA: 10s - loss: 0.1146 - accuracy: 0.9664\n 157/1875 [=&gt;............................] - ETA: 10s - loss: 0.1133 - accuracy: 0.9670\n 167/1875 [=&gt;............................] - ETA: 10s - loss: 0.1137 - accuracy: 0.9669\n 178/1875 [=&gt;............................] - ETA: 10s - loss: 0.1136 - accuracy: 0.9666\n 189/1875 [==&gt;...........................] - ETA: 10s - loss: 0.1135 - accuracy: 0.9664\n 199/1875 [==&gt;...........................] - ETA: 10s - loss: 0.1123 - accuracy: 0.9666\n 210/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1145 - accuracy: 0.9655 \n 221/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1148 - accuracy: 0.9652\n 231/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1140 - accuracy: 0.9655\n 240/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1148 - accuracy: 0.9652\n 248/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1148 - accuracy: 0.9655\n 256/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1147 - accuracy: 0.9653\n 266/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1129 - accuracy: 0.9663\n 275/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1111 - accuracy: 0.9666\n 283/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1101 - accuracy: 0.9669\n 291/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1122 - accuracy: 0.9664\n 298/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1121 - accuracy: 0.9664\n 305/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1117 - accuracy: 0.9666\n 312/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1122 - accuracy: 0.9662\n 318/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1118 - accuracy: 0.9664\n 325/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1133 - accuracy: 0.9659\n 333/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1143 - accuracy: 0.9657\n 340/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1128 - accuracy: 0.9662\n 347/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1129 - accuracy: 0.9661\n 355/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1132 - accuracy: 0.9659\n 362/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1129 - accuracy: 0.9662\n 369/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1140 - accuracy: 0.9658\n 376/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1137 - accuracy: 0.9659\n 384/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1138 - accuracy: 0.9658\n 391/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1142 - accuracy: 0.9657\n 399/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1139 - accuracy: 0.9658\n 406/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1131 - accuracy: 0.9661\n 413/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1147 - accuracy: 0.9660\n 420/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1141 - accuracy: 0.9662\n 428/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1143 - accuracy: 0.9663\n 436/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1147 - accuracy: 0.9662\n 444/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1148 - accuracy: 0.9661\n 452/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1148 - accuracy: 0.9662\n 460/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1139 - accuracy: 0.9666\n 467/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1143 - accuracy: 0.9663\n 474/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1138 - accuracy: 0.9664\n 481/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1144 - accuracy: 0.9665\n 488/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1142 - accuracy: 0.9666\n 496/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1137 - accuracy: 0.9667\n 504/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1146 - accuracy: 0.9665\n 512/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1158 - accuracy: 0.9661\n 520/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1153 - accuracy: 0.9663\n 528/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1144 - accuracy: 0.9665\n 536/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1141 - accuracy: 0.9666\n 544/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1146 - accuracy: 0.9665\n 552/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1150 - accuracy: 0.9663\n 560/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1147 - accuracy: 0.9664\n 568/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1145 - accuracy: 0.9664\n 576/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1148 - accuracy: 0.9663\n 584/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1143 - accuracy: 0.9664\n 592/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1141 - accuracy: 0.9663\n 600/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1135 - accuracy: 0.9665\n 608/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1132 - accuracy: 0.9665\n 616/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1130 - accuracy: 0.9666\n 624/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1123 - accuracy: 0.9667\n 632/1875 [=========&gt;....................] - ETA: 8s - loss: 0.1121 - accuracy: 0.9668\n 640/1875 [=========&gt;....................] - ETA: 8s - loss: 0.1124 - accuracy: 0.9665\n 648/1875 [=========&gt;....................] - ETA: 8s - loss: 0.1125 - accuracy: 0.9665\n 656/1875 [=========&gt;....................] - ETA: 7s - loss: 0.1126 - accuracy: 0.9665\n 664/1875 [=========&gt;....................] - ETA: 7s - loss: 0.1124 - accuracy: 0.9666\n 672/1875 [=========&gt;....................] - ETA: 7s - loss: 0.1120 - accuracy: 0.9667\n 680/1875 [=========&gt;....................] - ETA: 7s - loss: 0.1114 - accuracy: 0.9668\n 688/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1116 - accuracy: 0.9665\n 696/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1119 - accuracy: 0.9665\n 704/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1115 - accuracy: 0.9666\n 712/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1117 - accuracy: 0.9666\n 719/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1114 - accuracy: 0.9666\n 727/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1113 - accuracy: 0.9666\n 735/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1113 - accuracy: 0.9667\n 743/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1112 - accuracy: 0.9667\n 751/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1113 - accuracy: 0.9666\n 758/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1110 - accuracy: 0.9667\n 765/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1112 - accuracy: 0.9667\n 773/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1115 - accuracy: 0.9667\n 781/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1114 - accuracy: 0.9667\n 788/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1115 - accuracy: 0.9666\n 796/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1118 - accuracy: 0.9667\n 804/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1117 - accuracy: 0.9667\n 811/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1115 - accuracy: 0.9667\n 819/1875 [============&gt;.................] - ETA: 7s - loss: 0.1112 - accuracy: 0.9668\n 827/1875 [============&gt;.................] - ETA: 6s - loss: 0.1110 - accuracy: 0.9669\n 835/1875 [============&gt;.................] - ETA: 6s - loss: 0.1105 - accuracy: 0.9670\n 842/1875 [============&gt;.................] - ETA: 6s - loss: 0.1106 - accuracy: 0.9669\n 849/1875 [============&gt;.................] - ETA: 6s - loss: 0.1106 - accuracy: 0.9669\n 856/1875 [============&gt;.................] - ETA: 6s - loss: 0.1102 - accuracy: 0.9671\n 866/1875 [============&gt;.................] - ETA: 6s - loss: 0.1101 - accuracy: 0.9671\n 877/1875 [=============&gt;................] - ETA: 6s - loss: 0.1104 - accuracy: 0.9670\n 888/1875 [=============&gt;................] - ETA: 6s - loss: 0.1101 - accuracy: 0.9672\n 898/1875 [=============&gt;................] - ETA: 6s - loss: 0.1097 - accuracy: 0.9674\n 907/1875 [=============&gt;................] - ETA: 6s - loss: 0.1094 - accuracy: 0.9675\n 918/1875 [=============&gt;................] - ETA: 6s - loss: 0.1096 - accuracy: 0.9675\n 928/1875 [=============&gt;................] - ETA: 6s - loss: 0.1105 - accuracy: 0.9672\n 938/1875 [==============&gt;...............] - ETA: 6s - loss: 0.1105 - accuracy: 0.9672\n 949/1875 [==============&gt;...............] - ETA: 6s - loss: 0.1104 - accuracy: 0.9672\n 960/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1103 - accuracy: 0.9672\n 971/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1103 - accuracy: 0.9672\n 982/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1101 - accuracy: 0.9673\n 992/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1102 - accuracy: 0.9672\n1000/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1100 - accuracy: 0.9673\n1009/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1097 - accuracy: 0.9674\n1019/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1094 - accuracy: 0.9674\n1027/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1094 - accuracy: 0.9674\n1036/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1091 - accuracy: 0.9674\n1046/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1090 - accuracy: 0.9674\n1057/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1086 - accuracy: 0.9676\n1066/1875 [================&gt;.............] - ETA: 5s - loss: 0.1090 - accuracy: 0.9675\n1074/1875 [================&gt;.............] - ETA: 5s - loss: 0.1086 - accuracy: 0.9677\n1084/1875 [================&gt;.............] - ETA: 5s - loss: 0.1082 - accuracy: 0.9678\n1092/1875 [================&gt;.............] - ETA: 4s - loss: 0.1084 - accuracy: 0.9677\n1101/1875 [================&gt;.............] - ETA: 4s - loss: 0.1084 - accuracy: 0.9677\n1112/1875 [================&gt;.............] - ETA: 4s - loss: 0.1082 - accuracy: 0.9678\n1123/1875 [================&gt;.............] - ETA: 4s - loss: 0.1080 - accuracy: 0.9679\n1132/1875 [=================&gt;............] - ETA: 4s - loss: 0.1080 - accuracy: 0.9679\n1141/1875 [=================&gt;............] - ETA: 4s - loss: 0.1080 - accuracy: 0.9678\n1151/1875 [=================&gt;............] - ETA: 4s - loss: 0.1082 - accuracy: 0.9678\n1162/1875 [=================&gt;............] - ETA: 4s - loss: 0.1086 - accuracy: 0.9677\n1171/1875 [=================&gt;............] - ETA: 4s - loss: 0.1086 - accuracy: 0.9677\n1181/1875 [=================&gt;............] - ETA: 4s - loss: 0.1084 - accuracy: 0.9678\n1191/1875 [==================&gt;...........] - ETA: 4s - loss: 0.1085 - accuracy: 0.9677\n1202/1875 [==================&gt;...........] - ETA: 4s - loss: 0.1081 - accuracy: 0.9678\n1212/1875 [==================&gt;...........] - ETA: 4s - loss: 0.1082 - accuracy: 0.9677\n1221/1875 [==================&gt;...........] - ETA: 4s - loss: 0.1080 - accuracy: 0.9679\n1230/1875 [==================&gt;...........] - ETA: 4s - loss: 0.1079 - accuracy: 0.9679\n1241/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1078 - accuracy: 0.9679\n1252/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1079 - accuracy: 0.9679\n1263/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1075 - accuracy: 0.9681\n1274/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1075 - accuracy: 0.9680\n1283/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1074 - accuracy: 0.9681\n1293/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1076 - accuracy: 0.9680\n1302/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1072 - accuracy: 0.9681\n1313/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1070 - accuracy: 0.9681\n1324/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1068 - accuracy: 0.9681\n1335/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1068 - accuracy: 0.9680\n1346/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1069 - accuracy: 0.9680\n1357/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1070 - accuracy: 0.9680\n1367/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1072 - accuracy: 0.9679\n1375/1875 [=====================&gt;........] - ETA: 3s - loss: 0.1071 - accuracy: 0.9679\n1383/1875 [=====================&gt;........] - ETA: 3s - loss: 0.1069 - accuracy: 0.9679\n1391/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1068 - accuracy: 0.9679\n1400/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1074 - accuracy: 0.9679\n1409/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1074 - accuracy: 0.9679\n1418/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1075 - accuracy: 0.9679\n1428/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1073 - accuracy: 0.9679\n1438/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1074 - accuracy: 0.9678\n1449/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1074 - accuracy: 0.9679\n1459/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1073 - accuracy: 0.9680\n1470/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1072 - accuracy: 0.9680\n1478/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1073 - accuracy: 0.9680\n1486/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1073 - accuracy: 0.9680\n1494/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1073 - accuracy: 0.9680\n1503/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1074 - accuracy: 0.9679\n1512/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1072 - accuracy: 0.9679\n1520/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1070 - accuracy: 0.9680\n1529/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1070 - accuracy: 0.9680\n1537/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1070 - accuracy: 0.9680\n1545/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1072 - accuracy: 0.9679\n1555/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1073 - accuracy: 0.9679\n1565/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1074 - accuracy: 0.9679\n1576/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1077 - accuracy: 0.9678\n1585/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1080 - accuracy: 0.9677\n1593/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1078 - accuracy: 0.9678\n1602/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1078 - accuracy: 0.9678\n1613/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1078 - accuracy: 0.9678\n1624/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1078 - accuracy: 0.9679\n1635/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1076 - accuracy: 0.9679\n1646/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1073 - accuracy: 0.9680\n1657/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1074 - accuracy: 0.9680\n1667/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1071 - accuracy: 0.9680\n1678/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1070 - accuracy: 0.9680\n1688/1875 [==========================&gt;...] - ETA: 1s - loss: 0.1069 - accuracy: 0.9681\n1699/1875 [==========================&gt;...] - ETA: 1s - loss: 0.1067 - accuracy: 0.9681\n1709/1875 [==========================&gt;...] - ETA: 1s - loss: 0.1066 - accuracy: 0.9682\n1719/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1062 - accuracy: 0.9684\n1728/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1061 - accuracy: 0.9684\n1737/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1062 - accuracy: 0.9684\n1746/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1063 - accuracy: 0.9684\n1757/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1060 - accuracy: 0.9684\n1768/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1058 - accuracy: 0.9685\n1779/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1055 - accuracy: 0.9686\n1790/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1057 - accuracy: 0.9685\n1801/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1058 - accuracy: 0.9685\n1812/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1057 - accuracy: 0.9685\n1823/1875 [============================&gt;.] - ETA: 0s - loss: 0.1060 - accuracy: 0.9685\n1833/1875 [============================&gt;.] - ETA: 0s - loss: 0.1060 - accuracy: 0.9684\n1843/1875 [============================&gt;.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9685\n1853/1875 [============================&gt;.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9684\n1864/1875 [============================&gt;.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9685\n1875/1875 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9685\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.1057 - accuracy: 0.9685\nEpoch 4/5\n\n   1/1875 [..............................] - ETA: 9s - loss: 0.0532 - accuracy: 1.0000\n  11/1875 [..............................] - ETA: 9s - loss: 0.0714 - accuracy: 0.9773\n  21/1875 [..............................] - ETA: 10s - loss: 0.0671 - accuracy: 0.9792\n  30/1875 [..............................] - ETA: 10s - loss: 0.0674 - accuracy: 0.9781\n  41/1875 [..............................] - ETA: 9s - loss: 0.0827 - accuracy: 0.9748 \n  51/1875 [..............................] - ETA: 9s - loss: 0.0809 - accuracy: 0.9749\n  62/1875 [..............................] - ETA: 9s - loss: 0.0859 - accuracy: 0.9723\n  72/1875 [&gt;.............................] - ETA: 9s - loss: 0.0846 - accuracy: 0.9727\n  81/1875 [&gt;.............................] - ETA: 9s - loss: 0.0824 - accuracy: 0.9730\n  91/1875 [&gt;.............................] - ETA: 9s - loss: 0.0806 - accuracy: 0.9736\n 102/1875 [&gt;.............................] - ETA: 9s - loss: 0.0818 - accuracy: 0.9721\n 112/1875 [&gt;.............................] - ETA: 9s - loss: 0.0821 - accuracy: 0.9715\n 123/1875 [&gt;.............................] - ETA: 9s - loss: 0.0825 - accuracy: 0.9721\n 133/1875 [=&gt;............................] - ETA: 9s - loss: 0.0802 - accuracy: 0.9730\n 144/1875 [=&gt;............................] - ETA: 8s - loss: 0.0831 - accuracy: 0.9729\n 154/1875 [=&gt;............................] - ETA: 8s - loss: 0.0835 - accuracy: 0.9734\n 165/1875 [=&gt;............................] - ETA: 8s - loss: 0.0820 - accuracy: 0.9741\n 176/1875 [=&gt;............................] - ETA: 8s - loss: 0.0810 - accuracy: 0.9743\n 187/1875 [=&gt;............................] - ETA: 8s - loss: 0.0833 - accuracy: 0.9731\n 197/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0821 - accuracy: 0.9735\n 207/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0830 - accuracy: 0.9733\n 218/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0817 - accuracy: 0.9738\n 229/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0806 - accuracy: 0.9741\n 239/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0802 - accuracy: 0.9742\n 247/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0808 - accuracy: 0.9736\n 257/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0822 - accuracy: 0.9731\n 268/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0825 - accuracy: 0.9731\n 279/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0816 - accuracy: 0.9731\n 289/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0819 - accuracy: 0.9734\n 300/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0816 - accuracy: 0.9735\n 310/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0816 - accuracy: 0.9737\n 319/1875 [====&gt;.........................] - ETA: 8s - loss: 0.0822 - accuracy: 0.9737\n 329/1875 [====&gt;.........................] - ETA: 7s - loss: 0.0809 - accuracy: 0.9743\n 340/1875 [====&gt;.........................] - ETA: 7s - loss: 0.0819 - accuracy: 0.9740\n 350/1875 [====&gt;.........................] - ETA: 7s - loss: 0.0812 - accuracy: 0.9743\n 361/1875 [====&gt;.........................] - ETA: 7s - loss: 0.0807 - accuracy: 0.9745\n 371/1875 [====&gt;.........................] - ETA: 7s - loss: 0.0805 - accuracy: 0.9746\n 382/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0811 - accuracy: 0.9741\n 393/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0814 - accuracy: 0.9740\n 402/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0811 - accuracy: 0.9742\n 413/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0819 - accuracy: 0.9741\n 424/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0821 - accuracy: 0.9740\n 435/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0822 - accuracy: 0.9743\n 445/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0821 - accuracy: 0.9742\n 456/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0817 - accuracy: 0.9741\n 467/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0833 - accuracy: 0.9739\n 478/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0840 - accuracy: 0.9737\n 488/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0846 - accuracy: 0.9736\n 496/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0862 - accuracy: 0.9730\n 503/1875 [=======&gt;......................] - ETA: 7s - loss: 0.0857 - accuracy: 0.9732\n 511/1875 [=======&gt;......................] - ETA: 7s - loss: 0.0859 - accuracy: 0.9733\n 519/1875 [=======&gt;......................] - ETA: 7s - loss: 0.0857 - accuracy: 0.9734\n 529/1875 [=======&gt;......................] - ETA: 7s - loss: 0.0849 - accuracy: 0.9737\n 539/1875 [=======&gt;......................] - ETA: 6s - loss: 0.0848 - accuracy: 0.9737\n 547/1875 [=======&gt;......................] - ETA: 6s - loss: 0.0845 - accuracy: 0.9739\n 558/1875 [=======&gt;......................] - ETA: 6s - loss: 0.0843 - accuracy: 0.9738\n 568/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0843 - accuracy: 0.9738\n 579/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0836 - accuracy: 0.9740\n 589/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0845 - accuracy: 0.9738\n 600/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0846 - accuracy: 0.9737\n 611/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0844 - accuracy: 0.9739\n 620/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0843 - accuracy: 0.9738\n 631/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0843 - accuracy: 0.9737\n 641/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0839 - accuracy: 0.9738\n 652/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0835 - accuracy: 0.9740\n 663/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0837 - accuracy: 0.9740\n 673/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0830 - accuracy: 0.9743\n 683/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0827 - accuracy: 0.9743\n 694/1875 [==========&gt;...................] - ETA: 6s - loss: 0.0827 - accuracy: 0.9743\n 704/1875 [==========&gt;...................] - ETA: 6s - loss: 0.0825 - accuracy: 0.9744\n 714/1875 [==========&gt;...................] - ETA: 6s - loss: 0.0821 - accuracy: 0.9745\n 725/1875 [==========&gt;...................] - ETA: 5s - loss: 0.0821 - accuracy: 0.9745\n 735/1875 [==========&gt;...................] - ETA: 5s - loss: 0.0831 - accuracy: 0.9741\n 745/1875 [==========&gt;...................] - ETA: 5s - loss: 0.0838 - accuracy: 0.9740\n 755/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0842 - accuracy: 0.9738\n 765/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0844 - accuracy: 0.9737\n 776/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0839 - accuracy: 0.9739\n 787/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0842 - accuracy: 0.9738\n 797/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0842 - accuracy: 0.9738\n 808/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0839 - accuracy: 0.9740\n 818/1875 [============&gt;.................] - ETA: 5s - loss: 0.0840 - accuracy: 0.9739\n 827/1875 [============&gt;.................] - ETA: 5s - loss: 0.0839 - accuracy: 0.9739\n 837/1875 [============&gt;.................] - ETA: 5s - loss: 0.0845 - accuracy: 0.9737\n 847/1875 [============&gt;.................] - ETA: 5s - loss: 0.0845 - accuracy: 0.9736\n 857/1875 [============&gt;.................] - ETA: 5s - loss: 0.0844 - accuracy: 0.9737\n 867/1875 [============&gt;.................] - ETA: 5s - loss: 0.0841 - accuracy: 0.9738\n 878/1875 [=============&gt;................] - ETA: 5s - loss: 0.0842 - accuracy: 0.9737\n 889/1875 [=============&gt;................] - ETA: 5s - loss: 0.0843 - accuracy: 0.9738\n 900/1875 [=============&gt;................] - ETA: 5s - loss: 0.0845 - accuracy: 0.9739\n 910/1875 [=============&gt;................] - ETA: 5s - loss: 0.0846 - accuracy: 0.9738\n 920/1875 [=============&gt;................] - ETA: 4s - loss: 0.0849 - accuracy: 0.9738\n 931/1875 [=============&gt;................] - ETA: 4s - loss: 0.0849 - accuracy: 0.9738\n 942/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0847 - accuracy: 0.9739\n 952/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0845 - accuracy: 0.9740\n 963/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0845 - accuracy: 0.9739\n 973/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0842 - accuracy: 0.9740\n 981/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0843 - accuracy: 0.9740\n 989/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0844 - accuracy: 0.9741\n 997/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0842 - accuracy: 0.9741\n1005/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0845 - accuracy: 0.9741\n1015/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0847 - accuracy: 0.9740\n1023/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0850 - accuracy: 0.9740\n1034/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0850 - accuracy: 0.9739\n1045/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0851 - accuracy: 0.9738\n1055/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0852 - accuracy: 0.9739\n1065/1875 [================&gt;.............] - ETA: 4s - loss: 0.0851 - accuracy: 0.9739\n1076/1875 [================&gt;.............] - ETA: 4s - loss: 0.0848 - accuracy: 0.9740\n1085/1875 [================&gt;.............] - ETA: 4s - loss: 0.0848 - accuracy: 0.9741\n1093/1875 [================&gt;.............] - ETA: 4s - loss: 0.0849 - accuracy: 0.9741\n1101/1875 [================&gt;.............] - ETA: 4s - loss: 0.0847 - accuracy: 0.9742\n1109/1875 [================&gt;.............] - ETA: 4s - loss: 0.0849 - accuracy: 0.9741\n1117/1875 [================&gt;.............] - ETA: 3s - loss: 0.0850 - accuracy: 0.9740\n1125/1875 [=================&gt;............] - ETA: 3s - loss: 0.0850 - accuracy: 0.9740\n1136/1875 [=================&gt;............] - ETA: 3s - loss: 0.0853 - accuracy: 0.9738\n1146/1875 [=================&gt;............] - ETA: 3s - loss: 0.0854 - accuracy: 0.9738\n1154/1875 [=================&gt;............] - ETA: 3s - loss: 0.0852 - accuracy: 0.9739\n1164/1875 [=================&gt;............] - ETA: 3s - loss: 0.0852 - accuracy: 0.9738\n1175/1875 [=================&gt;............] - ETA: 3s - loss: 0.0849 - accuracy: 0.9739\n1186/1875 [=================&gt;............] - ETA: 3s - loss: 0.0850 - accuracy: 0.9738\n1197/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0854 - accuracy: 0.9737\n1207/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0852 - accuracy: 0.9737\n1217/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0852 - accuracy: 0.9738\n1226/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0854 - accuracy: 0.9737\n1234/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0854 - accuracy: 0.9737\n1243/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0851 - accuracy: 0.9738\n1253/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0852 - accuracy: 0.9737\n1264/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0850 - accuracy: 0.9737\n1274/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0849 - accuracy: 0.9738\n1284/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0852 - accuracy: 0.9736\n1294/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0855 - accuracy: 0.9735\n1305/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0855 - accuracy: 0.9735\n1316/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0855 - accuracy: 0.9734\n1327/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0854 - accuracy: 0.9735\n1335/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0856 - accuracy: 0.9733\n1343/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0854 - accuracy: 0.9735\n1351/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0853 - accuracy: 0.9734\n1359/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1367/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1376/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0851 - accuracy: 0.9735\n1386/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1397/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1408/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0848 - accuracy: 0.9736\n1418/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0849 - accuracy: 0.9736\n1429/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0854 - accuracy: 0.9734\n1440/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0854 - accuracy: 0.9734\n1451/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0854 - accuracy: 0.9734\n1461/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1472/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1482/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0855 - accuracy: 0.9734\n1492/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0855 - accuracy: 0.9734\n1502/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0853 - accuracy: 0.9734\n1512/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0854 - accuracy: 0.9734\n1522/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0857 - accuracy: 0.9733\n1531/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0858 - accuracy: 0.9733\n1541/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0860 - accuracy: 0.9733\n1552/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0858 - accuracy: 0.9734\n1563/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0857 - accuracy: 0.9734\n1574/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0859 - accuracy: 0.9733\n1584/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0856 - accuracy: 0.9734\n1595/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0859 - accuracy: 0.9734\n1606/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0858 - accuracy: 0.9733\n1617/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0859 - accuracy: 0.9733\n1628/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0859 - accuracy: 0.9734\n1639/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0860 - accuracy: 0.9732\n1649/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0860 - accuracy: 0.9733\n1660/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0860 - accuracy: 0.9732\n1670/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0860 - accuracy: 0.9732\n1681/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0861 - accuracy: 0.9732\n1691/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9732\n1701/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0861 - accuracy: 0.9732\n1710/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9732\n1719/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9732\n1727/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9731\n1735/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9732\n1743/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0859 - accuracy: 0.9732\n1753/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0861 - accuracy: 0.9732\n1763/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0862 - accuracy: 0.9731\n1770/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0864 - accuracy: 0.9731\n1780/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0867 - accuracy: 0.9730\n1790/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0866 - accuracy: 0.9730\n1798/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0867 - accuracy: 0.9730\n1806/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0866 - accuracy: 0.9731\n1814/1875 [============================&gt;.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9731\n1824/1875 [============================&gt;.] - ETA: 0s - loss: 0.0863 - accuracy: 0.9731\n1834/1875 [============================&gt;.] - ETA: 0s - loss: 0.0862 - accuracy: 0.9731\n1843/1875 [============================&gt;.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9731\n1851/1875 [============================&gt;.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9730\n1860/1875 [============================&gt;.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9730\n1871/1875 [============================&gt;.] - ETA: 0s - loss: 0.0863 - accuracy: 0.9731\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0864 - accuracy: 0.9730\nEpoch 5/5\n\n   1/1875 [..............................] - ETA: 9s - loss: 0.0046 - accuracy: 1.0000\n  11/1875 [..............................] - ETA: 10s - loss: 0.0449 - accuracy: 0.9801\n  19/1875 [..............................] - ETA: 10s - loss: 0.0526 - accuracy: 0.9819\n  29/1875 [..............................] - ETA: 10s - loss: 0.0576 - accuracy: 0.9817\n  39/1875 [..............................] - ETA: 10s - loss: 0.0668 - accuracy: 0.9808\n  49/1875 [..............................] - ETA: 9s - loss: 0.0603 - accuracy: 0.9821 \n  59/1875 [..............................] - ETA: 9s - loss: 0.0597 - accuracy: 0.9815\n  70/1875 [&gt;.............................] - ETA: 9s - loss: 0.0666 - accuracy: 0.9799\n  81/1875 [&gt;.............................] - ETA: 9s - loss: 0.0703 - accuracy: 0.9788\n  92/1875 [&gt;.............................] - ETA: 9s - loss: 0.0730 - accuracy: 0.9789\n 103/1875 [&gt;.............................] - ETA: 9s - loss: 0.0706 - accuracy: 0.9803\n 113/1875 [&gt;.............................] - ETA: 9s - loss: 0.0713 - accuracy: 0.9804\n 121/1875 [&gt;.............................] - ETA: 9s - loss: 0.0724 - accuracy: 0.9796\n 130/1875 [=&gt;............................] - ETA: 9s - loss: 0.0721 - accuracy: 0.9796\n 141/1875 [=&gt;............................] - ETA: 9s - loss: 0.0714 - accuracy: 0.9798\n 150/1875 [=&gt;............................] - ETA: 9s - loss: 0.0737 - accuracy: 0.9794\n 158/1875 [=&gt;............................] - ETA: 9s - loss: 0.0746 - accuracy: 0.9782\n 166/1875 [=&gt;............................] - ETA: 9s - loss: 0.0731 - accuracy: 0.9787\n 173/1875 [=&gt;............................] - ETA: 9s - loss: 0.0747 - accuracy: 0.9787\n 181/1875 [=&gt;............................] - ETA: 9s - loss: 0.0744 - accuracy: 0.9789\n 189/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0737 - accuracy: 0.9790\n 197/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0746 - accuracy: 0.9786\n 204/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0741 - accuracy: 0.9786\n 212/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0740 - accuracy: 0.9783\n 219/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0737 - accuracy: 0.9785\n 227/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0728 - accuracy: 0.9785\n 235/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0718 - accuracy: 0.9789\n 242/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0711 - accuracy: 0.9791\n 249/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0711 - accuracy: 0.9788\n 256/1875 [===&gt;..........................] - ETA: 9s - loss: 0.0729 - accuracy: 0.9785\n 263/1875 [===&gt;..........................] - ETA: 9s - loss: 0.0725 - accuracy: 0.9785\n 270/1875 [===&gt;..........................] - ETA: 10s - loss: 0.0723 - accuracy: 0.9786\n 277/1875 [===&gt;..........................] - ETA: 10s - loss: 0.0726 - accuracy: 0.9787\n 284/1875 [===&gt;..........................] - ETA: 10s - loss: 0.0725 - accuracy: 0.9785\n 292/1875 [===&gt;..........................] - ETA: 10s - loss: 0.0765 - accuracy: 0.9775\n 300/1875 [===&gt;..........................] - ETA: 9s - loss: 0.0780 - accuracy: 0.9770 \n 307/1875 [===&gt;..........................] - ETA: 9s - loss: 0.0785 - accuracy: 0.9765\n 315/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0788 - accuracy: 0.9764\n 322/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0781 - accuracy: 0.9764\n 329/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0778 - accuracy: 0.9765\n 336/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0786 - accuracy: 0.9762\n 343/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0785 - accuracy: 0.9762\n 350/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0782 - accuracy: 0.9763\n 357/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0786 - accuracy: 0.9765\n 364/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0785 - accuracy: 0.9764\n 372/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0780 - accuracy: 0.9766\n 379/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0774 - accuracy: 0.9769\n 386/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0771 - accuracy: 0.9768\n 394/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0764 - accuracy: 0.9770\n 402/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0756 - accuracy: 0.9771\n 409/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0756 - accuracy: 0.9772\n 417/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0762 - accuracy: 0.9771\n 424/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0762 - accuracy: 0.9770\n 432/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0764 - accuracy: 0.9769\n 440/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0759 - accuracy: 0.9771\n 448/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0754 - accuracy: 0.9772\n 455/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0747 - accuracy: 0.9774\n 463/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0745 - accuracy: 0.9773\n 471/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0744 - accuracy: 0.9771\n 479/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0742 - accuracy: 0.9772\n 487/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0746 - accuracy: 0.9770\n 494/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0746 - accuracy: 0.9768\n 502/1875 [=======&gt;......................] - ETA: 9s - loss: 0.0745 - accuracy: 0.9769\n 510/1875 [=======&gt;......................] - ETA: 9s - loss: 0.0752 - accuracy: 0.9768\n 518/1875 [=======&gt;......................] - ETA: 9s - loss: 0.0748 - accuracy: 0.9770\n 526/1875 [=======&gt;......................] - ETA: 9s - loss: 0.0752 - accuracy: 0.9769\n 535/1875 [=======&gt;......................] - ETA: 8s - loss: 0.0750 - accuracy: 0.9770\n 543/1875 [=======&gt;......................] - ETA: 8s - loss: 0.0749 - accuracy: 0.9770\n 551/1875 [=======&gt;......................] - ETA: 8s - loss: 0.0751 - accuracy: 0.9769\n 559/1875 [=======&gt;......................] - ETA: 8s - loss: 0.0751 - accuracy: 0.9769\n 567/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0756 - accuracy: 0.9766\n 575/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0757 - accuracy: 0.9766\n 583/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0760 - accuracy: 0.9765\n 591/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0757 - accuracy: 0.9766\n 599/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0756 - accuracy: 0.9765\n 607/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0758 - accuracy: 0.9765\n 615/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0766 - accuracy: 0.9762\n 623/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0765 - accuracy: 0.9762\n 631/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0764 - accuracy: 0.9763\n 639/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0764 - accuracy: 0.9762\n 647/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0764 - accuracy: 0.9762\n 655/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0769 - accuracy: 0.9761\n 663/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0767 - accuracy: 0.9762\n 671/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0765 - accuracy: 0.9763\n 679/1875 [=========&gt;....................] - ETA: 7s - loss: 0.0762 - accuracy: 0.9764\n 687/1875 [=========&gt;....................] - ETA: 7s - loss: 0.0760 - accuracy: 0.9765\n 695/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0756 - accuracy: 0.9766\n 703/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0752 - accuracy: 0.9768\n 711/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0748 - accuracy: 0.9769\n 719/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0745 - accuracy: 0.9770\n 727/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0742 - accuracy: 0.9770\n 735/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0737 - accuracy: 0.9772\n 743/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0740 - accuracy: 0.9771\n 751/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0736 - accuracy: 0.9771\n 759/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0739 - accuracy: 0.9770\n 768/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0744 - accuracy: 0.9769\n 776/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0742 - accuracy: 0.9770\n 784/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0744 - accuracy: 0.9770\n 792/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0744 - accuracy: 0.9769\n 800/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0745 - accuracy: 0.9770\n 808/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0742 - accuracy: 0.9771\n 816/1875 [============&gt;.................] - ETA: 7s - loss: 0.0745 - accuracy: 0.9769\n 824/1875 [============&gt;.................] - ETA: 7s - loss: 0.0741 - accuracy: 0.9771\n 832/1875 [============&gt;.................] - ETA: 6s - loss: 0.0744 - accuracy: 0.9770\n 840/1875 [============&gt;.................] - ETA: 6s - loss: 0.0742 - accuracy: 0.9770\n 848/1875 [============&gt;.................] - ETA: 6s - loss: 0.0738 - accuracy: 0.9770\n 856/1875 [============&gt;.................] - ETA: 6s - loss: 0.0735 - accuracy: 0.9771\n 865/1875 [============&gt;.................] - ETA: 6s - loss: 0.0733 - accuracy: 0.9771\n 874/1875 [============&gt;.................] - ETA: 6s - loss: 0.0734 - accuracy: 0.9771\n 882/1875 [=============&gt;................] - ETA: 6s - loss: 0.0735 - accuracy: 0.9770\n 890/1875 [=============&gt;................] - ETA: 6s - loss: 0.0739 - accuracy: 0.9770\n 898/1875 [=============&gt;................] - ETA: 6s - loss: 0.0742 - accuracy: 0.9770\n 907/1875 [=============&gt;................] - ETA: 6s - loss: 0.0742 - accuracy: 0.9770\n 915/1875 [=============&gt;................] - ETA: 6s - loss: 0.0740 - accuracy: 0.9770\n 923/1875 [=============&gt;................] - ETA: 6s - loss: 0.0740 - accuracy: 0.9769\n 931/1875 [=============&gt;................] - ETA: 6s - loss: 0.0738 - accuracy: 0.9769\n 939/1875 [==============&gt;...............] - ETA: 6s - loss: 0.0735 - accuracy: 0.9770\n 948/1875 [==============&gt;...............] - ETA: 6s - loss: 0.0735 - accuracy: 0.9771\n 957/1875 [==============&gt;...............] - ETA: 6s - loss: 0.0735 - accuracy: 0.9770\n 966/1875 [==============&gt;...............] - ETA: 6s - loss: 0.0735 - accuracy: 0.9770\n 974/1875 [==============&gt;...............] - ETA: 5s - loss: 0.0734 - accuracy: 0.9769\n 982/1875 [==============&gt;...............] - ETA: 5s - loss: 0.0733 - accuracy: 0.9768\n 990/1875 [==============&gt;...............] - ETA: 5s - loss: 0.0731 - accuracy: 0.9769\n 998/1875 [==============&gt;...............] - ETA: 5s - loss: 0.0730 - accuracy: 0.9769\n1007/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0732 - accuracy: 0.9768\n1015/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0733 - accuracy: 0.9768\n1023/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0734 - accuracy: 0.9767\n1031/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0734 - accuracy: 0.9767\n1039/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0738 - accuracy: 0.9767\n1047/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0738 - accuracy: 0.9767\n1055/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0741 - accuracy: 0.9766\n1063/1875 [================&gt;.............] - ETA: 5s - loss: 0.0739 - accuracy: 0.9766\n1071/1875 [================&gt;.............] - ETA: 5s - loss: 0.0742 - accuracy: 0.9766\n1079/1875 [================&gt;.............] - ETA: 5s - loss: 0.0741 - accuracy: 0.9766\n1087/1875 [================&gt;.............] - ETA: 5s - loss: 0.0739 - accuracy: 0.9767\n1095/1875 [================&gt;.............] - ETA: 5s - loss: 0.0737 - accuracy: 0.9768\n1103/1875 [================&gt;.............] - ETA: 5s - loss: 0.0740 - accuracy: 0.9768\n1111/1875 [================&gt;.............] - ETA: 5s - loss: 0.0742 - accuracy: 0.9767\n1119/1875 [================&gt;.............] - ETA: 5s - loss: 0.0740 - accuracy: 0.9767\n1127/1875 [=================&gt;............] - ETA: 4s - loss: 0.0738 - accuracy: 0.9768\n1135/1875 [=================&gt;............] - ETA: 4s - loss: 0.0737 - accuracy: 0.9768\n1143/1875 [=================&gt;............] - ETA: 4s - loss: 0.0735 - accuracy: 0.9768\n1151/1875 [=================&gt;............] - ETA: 4s - loss: 0.0735 - accuracy: 0.9768\n1159/1875 [=================&gt;............] - ETA: 4s - loss: 0.0732 - accuracy: 0.9769\n1168/1875 [=================&gt;............] - ETA: 4s - loss: 0.0734 - accuracy: 0.9769\n1176/1875 [=================&gt;............] - ETA: 4s - loss: 0.0737 - accuracy: 0.9768\n1184/1875 [=================&gt;............] - ETA: 4s - loss: 0.0737 - accuracy: 0.9768\n1193/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0735 - accuracy: 0.9769\n1202/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0737 - accuracy: 0.9768\n1213/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0738 - accuracy: 0.9768\n1223/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0736 - accuracy: 0.9769\n1231/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0735 - accuracy: 0.9770\n1241/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0734 - accuracy: 0.9770\n1251/1875 [===================&gt;..........] - ETA: 4s - loss: 0.0733 - accuracy: 0.9771\n1259/1875 [===================&gt;..........] - ETA: 4s - loss: 0.0735 - accuracy: 0.9770\n1267/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0734 - accuracy: 0.9770\n1277/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0735 - accuracy: 0.9769\n1288/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0739 - accuracy: 0.9769\n1299/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0741 - accuracy: 0.9769\n1310/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0743 - accuracy: 0.9768\n1320/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0743 - accuracy: 0.9769\n1327/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0741 - accuracy: 0.9769\n1337/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0741 - accuracy: 0.9769\n1348/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0740 - accuracy: 0.9770\n1358/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0739 - accuracy: 0.9770\n1369/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0737 - accuracy: 0.9771\n1379/1875 [=====================&gt;........] - ETA: 3s - loss: 0.0735 - accuracy: 0.9771\n1389/1875 [=====================&gt;........] - ETA: 3s - loss: 0.0734 - accuracy: 0.9772\n1397/1875 [=====================&gt;........] - ETA: 3s - loss: 0.0732 - accuracy: 0.9772\n1405/1875 [=====================&gt;........] - ETA: 3s - loss: 0.0731 - accuracy: 0.9773\n1413/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0729 - accuracy: 0.9773\n1421/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0730 - accuracy: 0.9773\n1432/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0731 - accuracy: 0.9773\n1443/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0731 - accuracy: 0.9772\n1451/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0733 - accuracy: 0.9771\n1458/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0734 - accuracy: 0.9771\n1466/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0733 - accuracy: 0.9771\n1477/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0733 - accuracy: 0.9770\n1487/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0735 - accuracy: 0.9770\n1497/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0736 - accuracy: 0.9769\n1508/1875 [=======================&gt;......] - ETA: 2s - loss: 0.0736 - accuracy: 0.9770\n1518/1875 [=======================&gt;......] - ETA: 2s - loss: 0.0734 - accuracy: 0.9771\n1529/1875 [=======================&gt;......] - ETA: 2s - loss: 0.0737 - accuracy: 0.9770\n1540/1875 [=======================&gt;......] - ETA: 2s - loss: 0.0737 - accuracy: 0.9770\n1551/1875 [=======================&gt;......] - ETA: 2s - loss: 0.0735 - accuracy: 0.9770\n1561/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0737 - accuracy: 0.9769\n1571/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0738 - accuracy: 0.9769\n1579/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0738 - accuracy: 0.9769\n1587/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9769\n1595/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9769\n1603/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0739 - accuracy: 0.9769\n1611/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0739 - accuracy: 0.9770\n1619/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9770\n1627/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9770\n1635/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0741 - accuracy: 0.9770\n1643/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9770\n1651/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0742 - accuracy: 0.9770\n1659/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0741 - accuracy: 0.9770\n1667/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0741 - accuracy: 0.9770\n1675/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9771\n1682/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0741 - accuracy: 0.9771\n1690/1875 [==========================&gt;...] - ETA: 1s - loss: 0.0741 - accuracy: 0.9770\n1698/1875 [==========================&gt;...] - ETA: 1s - loss: 0.0742 - accuracy: 0.9770\n1706/1875 [==========================&gt;...] - ETA: 1s - loss: 0.0742 - accuracy: 0.9770\n1714/1875 [==========================&gt;...] - ETA: 1s - loss: 0.0740 - accuracy: 0.9770\n1722/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0739 - accuracy: 0.9771\n1730/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0738 - accuracy: 0.9771\n1738/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0738 - accuracy: 0.9771\n1747/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1755/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1763/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1771/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1779/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0738 - accuracy: 0.9771\n1787/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1795/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1803/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0736 - accuracy: 0.9771\n1811/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1819/1875 [============================&gt;.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9770\n1827/1875 [============================&gt;.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9771\n1834/1875 [============================&gt;.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9771\n1843/1875 [============================&gt;.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1851/1875 [============================&gt;.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9772\n1859/1875 [============================&gt;.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9772\n1867/1875 [============================&gt;.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9772\n1875/1875 [==============================] - 12s 6ms/step - loss: 0.0733 - accuracy: 0.9772\n&lt;keras.callbacks.History object at 0x000002D3DAF4BBB0&gt;\n\n\n\nmodel.evaluate(x_test,  y_test, verbose=2)\n\n313/313 - 1s - loss: 0.0761 - accuracy: 0.9764 - 1s/epoch - 4ms/step\n[0.07606449723243713, 0.9764000177383423]\n\nprobability_model = tf.keras.Sequential([\n  model,\n  tf.keras.layers.Softmax()\n])\n\nprobability_model(x_test[:5])\n\n&lt;tf.Tensor: shape=(5, 10), dtype=float32, numpy=\narray([[1.3065116e-07, 1.1906588e-08, 8.6254568e-06, 1.9500349e-04,\n        3.9280154e-13, 2.7274782e-08, 8.9514090e-14, 9.9979573e-01,\n        1.6844812e-07, 2.5713874e-07],\n       [3.4573787e-08, 3.8334861e-06, 9.9999619e-01, 4.0571624e-08,\n        1.1119099e-15, 2.6929008e-09, 4.4391992e-09, 6.3775271e-13,\n        8.0284677e-09, 4.7792949e-15],\n       [3.5627131e-08, 9.9984312e-01, 3.6839916e-05, 1.0188468e-06,\n        8.2455826e-06, 2.3786862e-07, 2.3271245e-06, 1.0226570e-04,\n        5.7826060e-06, 2.2025354e-08],\n       [9.9988711e-01, 8.9482723e-09, 1.3988179e-05, 2.6024620e-07,\n        1.2380229e-06, 4.9973755e-06, 5.2229767e-05, 2.6953589e-05,\n        3.2521751e-07, 1.2755126e-05],\n       [2.7323288e-06, 2.6271124e-07, 2.4348870e-05, 2.1980078e-08,\n        9.9810565e-01, 4.4357641e-08, 2.3158240e-07, 4.0300849e-05,\n        6.7471746e-08, 1.8263297e-03]], dtype=float32)&gt;"
  },
  {
    "objectID": "baking/python_tensorflow_basics/index.html#a-simple-python-mahcine-learning",
    "href": "baking/python_tensorflow_basics/index.html#a-simple-python-mahcine-learning",
    "title": "Python",
    "section": "",
    "text": "import tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)\n\nTensorFlow version: 2.10.1\n\n\nmnist = tf.keras.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n\n\n\n\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10)\n])\n\n\npredictions = model(x_train[:1]).numpy()\npredictions\n\narray([[-1.0659655e+00, -5.6349498e-01, -9.2068411e-02, -9.8071992e-05,\n         5.1046234e-01,  2.9953119e-01,  1.5084814e-01, -5.8150619e-01,\n        -2.9266942e-01, -8.9257973e-01]], dtype=float32)\n\n# want to see as \ntf.nn.softmax(predictions).numpy()\n\narray([[0.03950126, 0.06528768, 0.10460903, 0.11468626, 0.19109307,\n        0.15475287, 0.13337255, 0.06412229, 0.08559517, 0.04697984]],\n      dtype=float32)\n\n\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\nloss_fn(y_train[:1], predictions).numpy()\n\n1.8659258\n\n\n\nmodel.compile(optimizer='adam',\n              loss=loss_fn,\n              metrics=['accuracy'])\n\n\nmodel.fit(x_train, y_train, epochs=5)\n\nEpoch 1/5\n\n   1/1875 [..............................] - ETA: 16:46 - loss: 2.4121 - accuracy: 0.0312\n  18/1875 [..............................] - ETA: 5s - loss: 1.9974 - accuracy: 0.3559   \n  35/1875 [..............................] - ETA: 5s - loss: 1.6133 - accuracy: 0.5232\n  48/1875 [..............................] - ETA: 6s - loss: 1.4181 - accuracy: 0.5898\n  61/1875 [..............................] - ETA: 6s - loss: 1.2882 - accuracy: 0.6235\n  76/1875 [&gt;.............................] - ETA: 6s - loss: 1.1568 - accuracy: 0.6653\n  93/1875 [&gt;.............................] - ETA: 6s - loss: 1.0512 - accuracy: 0.6972\n 110/1875 [&gt;.............................] - ETA: 5s - loss: 0.9665 - accuracy: 0.7193\n 127/1875 [=&gt;............................] - ETA: 5s - loss: 0.8994 - accuracy: 0.7377\n 144/1875 [=&gt;............................] - ETA: 5s - loss: 0.8495 - accuracy: 0.7530\n 161/1875 [=&gt;............................] - ETA: 5s - loss: 0.8058 - accuracy: 0.7659\n 178/1875 [=&gt;............................] - ETA: 5s - loss: 0.7704 - accuracy: 0.7758\n 196/1875 [==&gt;...........................] - ETA: 5s - loss: 0.7353 - accuracy: 0.7865\n 205/1875 [==&gt;...........................] - ETA: 5s - loss: 0.7188 - accuracy: 0.7915\n 215/1875 [==&gt;...........................] - ETA: 5s - loss: 0.7039 - accuracy: 0.7961\n 233/1875 [==&gt;...........................] - ETA: 5s - loss: 0.6762 - accuracy: 0.8036\n 247/1875 [==&gt;...........................] - ETA: 5s - loss: 0.6584 - accuracy: 0.8091\n 265/1875 [===&gt;..........................] - ETA: 5s - loss: 0.6410 - accuracy: 0.8138\n 280/1875 [===&gt;..........................] - ETA: 5s - loss: 0.6237 - accuracy: 0.8191\n 291/1875 [===&gt;..........................] - ETA: 5s - loss: 0.6144 - accuracy: 0.8214\n 300/1875 [===&gt;..........................] - ETA: 5s - loss: 0.6070 - accuracy: 0.8234\n 310/1875 [===&gt;..........................] - ETA: 5s - loss: 0.5993 - accuracy: 0.8249\n 324/1875 [====&gt;.........................] - ETA: 5s - loss: 0.5897 - accuracy: 0.8276\n 334/1875 [====&gt;.........................] - ETA: 5s - loss: 0.5825 - accuracy: 0.8298\n 349/1875 [====&gt;.........................] - ETA: 5s - loss: 0.5722 - accuracy: 0.8326\n 364/1875 [====&gt;.........................] - ETA: 5s - loss: 0.5621 - accuracy: 0.8350\n 378/1875 [=====&gt;........................] - ETA: 5s - loss: 0.5544 - accuracy: 0.8371\n 390/1875 [=====&gt;........................] - ETA: 5s - loss: 0.5493 - accuracy: 0.8385\n 407/1875 [=====&gt;........................] - ETA: 5s - loss: 0.5428 - accuracy: 0.8408\n 425/1875 [=====&gt;........................] - ETA: 5s - loss: 0.5352 - accuracy: 0.8428\n 442/1875 [======&gt;.......................] - ETA: 5s - loss: 0.5293 - accuracy: 0.8449\n 460/1875 [======&gt;.......................] - ETA: 4s - loss: 0.5214 - accuracy: 0.8467\n 476/1875 [======&gt;.......................] - ETA: 4s - loss: 0.5147 - accuracy: 0.8487\n 492/1875 [======&gt;.......................] - ETA: 4s - loss: 0.5070 - accuracy: 0.8511\n 507/1875 [=======&gt;......................] - ETA: 4s - loss: 0.5023 - accuracy: 0.8527\n 523/1875 [=======&gt;......................] - ETA: 4s - loss: 0.4967 - accuracy: 0.8545\n 537/1875 [=======&gt;......................] - ETA: 4s - loss: 0.4917 - accuracy: 0.8557\n 551/1875 [=======&gt;......................] - ETA: 4s - loss: 0.4876 - accuracy: 0.8571\n 565/1875 [========&gt;.....................] - ETA: 4s - loss: 0.4820 - accuracy: 0.8585\n 579/1875 [========&gt;.....................] - ETA: 4s - loss: 0.4785 - accuracy: 0.8598\n 594/1875 [========&gt;.....................] - ETA: 4s - loss: 0.4730 - accuracy: 0.8615\n 609/1875 [========&gt;.....................] - ETA: 4s - loss: 0.4682 - accuracy: 0.8630\n 626/1875 [=========&gt;....................] - ETA: 4s - loss: 0.4616 - accuracy: 0.8648\n 640/1875 [=========&gt;....................] - ETA: 4s - loss: 0.4562 - accuracy: 0.8665\n 656/1875 [=========&gt;....................] - ETA: 4s - loss: 0.4515 - accuracy: 0.8679\n 673/1875 [=========&gt;....................] - ETA: 4s - loss: 0.4464 - accuracy: 0.8695\n 687/1875 [=========&gt;....................] - ETA: 4s - loss: 0.4427 - accuracy: 0.8706\n 697/1875 [==========&gt;...................] - ETA: 4s - loss: 0.4398 - accuracy: 0.8713\n 716/1875 [==========&gt;...................] - ETA: 4s - loss: 0.4357 - accuracy: 0.8724\n 738/1875 [==========&gt;...................] - ETA: 3s - loss: 0.4294 - accuracy: 0.8744\n 754/1875 [===========&gt;..................] - ETA: 3s - loss: 0.4242 - accuracy: 0.8761\n 772/1875 [===========&gt;..................] - ETA: 3s - loss: 0.4202 - accuracy: 0.8772\n 782/1875 [===========&gt;..................] - ETA: 3s - loss: 0.4170 - accuracy: 0.8783\n 793/1875 [===========&gt;..................] - ETA: 3s - loss: 0.4150 - accuracy: 0.8789\n 810/1875 [===========&gt;..................] - ETA: 3s - loss: 0.4118 - accuracy: 0.8798\n 832/1875 [============&gt;.................] - ETA: 3s - loss: 0.4079 - accuracy: 0.8807\n 850/1875 [============&gt;.................] - ETA: 3s - loss: 0.4060 - accuracy: 0.8812\n 868/1875 [============&gt;.................] - ETA: 3s - loss: 0.4042 - accuracy: 0.8816\n 887/1875 [=============&gt;................] - ETA: 3s - loss: 0.4007 - accuracy: 0.8824\n 907/1875 [=============&gt;................] - ETA: 3s - loss: 0.3968 - accuracy: 0.8835\n 922/1875 [=============&gt;................] - ETA: 3s - loss: 0.3943 - accuracy: 0.8843\n 932/1875 [=============&gt;................] - ETA: 3s - loss: 0.3928 - accuracy: 0.8848\n 941/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3908 - accuracy: 0.8854\n 950/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3896 - accuracy: 0.8858\n 961/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3875 - accuracy: 0.8864\n 970/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3855 - accuracy: 0.8869\n 982/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3838 - accuracy: 0.8874\n 995/1875 [==============&gt;...............] - ETA: 3s - loss: 0.3821 - accuracy: 0.8882\n1008/1875 [===============&gt;..............] - ETA: 3s - loss: 0.3802 - accuracy: 0.8888\n1024/1875 [===============&gt;..............] - ETA: 2s - loss: 0.3780 - accuracy: 0.8896\n1038/1875 [===============&gt;..............] - ETA: 2s - loss: 0.3761 - accuracy: 0.8901\n1050/1875 [===============&gt;..............] - ETA: 2s - loss: 0.3737 - accuracy: 0.8908\n1063/1875 [================&gt;.............] - ETA: 2s - loss: 0.3715 - accuracy: 0.8914\n1075/1875 [================&gt;.............] - ETA: 2s - loss: 0.3696 - accuracy: 0.8921\n1093/1875 [================&gt;.............] - ETA: 2s - loss: 0.3666 - accuracy: 0.8929\n1111/1875 [================&gt;.............] - ETA: 2s - loss: 0.3644 - accuracy: 0.8935\n1130/1875 [=================&gt;............] - ETA: 2s - loss: 0.3616 - accuracy: 0.8942\n1147/1875 [=================&gt;............] - ETA: 2s - loss: 0.3593 - accuracy: 0.8949\n1164/1875 [=================&gt;............] - ETA: 2s - loss: 0.3575 - accuracy: 0.8954\n1178/1875 [=================&gt;............] - ETA: 2s - loss: 0.3550 - accuracy: 0.8962\n1188/1875 [==================&gt;...........] - ETA: 2s - loss: 0.3539 - accuracy: 0.8965\n1202/1875 [==================&gt;...........] - ETA: 2s - loss: 0.3518 - accuracy: 0.8972\n1214/1875 [==================&gt;...........] - ETA: 2s - loss: 0.3506 - accuracy: 0.8975\n1227/1875 [==================&gt;...........] - ETA: 2s - loss: 0.3488 - accuracy: 0.8980\n1239/1875 [==================&gt;...........] - ETA: 2s - loss: 0.3473 - accuracy: 0.8985\n1250/1875 [===================&gt;..........] - ETA: 2s - loss: 0.3461 - accuracy: 0.8989\n1265/1875 [===================&gt;..........] - ETA: 2s - loss: 0.3451 - accuracy: 0.8994\n1280/1875 [===================&gt;..........] - ETA: 2s - loss: 0.3435 - accuracy: 0.8999\n1294/1875 [===================&gt;..........] - ETA: 2s - loss: 0.3421 - accuracy: 0.9003\n1305/1875 [===================&gt;..........] - ETA: 2s - loss: 0.3415 - accuracy: 0.9005\n1319/1875 [====================&gt;.........] - ETA: 1s - loss: 0.3402 - accuracy: 0.9009\n1334/1875 [====================&gt;.........] - ETA: 1s - loss: 0.3391 - accuracy: 0.9012\n1348/1875 [====================&gt;.........] - ETA: 1s - loss: 0.3374 - accuracy: 0.9017\n1359/1875 [====================&gt;.........] - ETA: 1s - loss: 0.3364 - accuracy: 0.9021\n1369/1875 [====================&gt;.........] - ETA: 1s - loss: 0.3357 - accuracy: 0.9022\n1379/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3346 - accuracy: 0.9025\n1389/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3336 - accuracy: 0.9028\n1400/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3328 - accuracy: 0.9029\n1410/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3321 - accuracy: 0.9031\n1420/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3310 - accuracy: 0.9034\n1431/1875 [=====================&gt;........] - ETA: 1s - loss: 0.3299 - accuracy: 0.9038\n1444/1875 [======================&gt;.......] - ETA: 1s - loss: 0.3285 - accuracy: 0.9042\n1458/1875 [======================&gt;.......] - ETA: 1s - loss: 0.3270 - accuracy: 0.9046\n1472/1875 [======================&gt;.......] - ETA: 1s - loss: 0.3255 - accuracy: 0.9050\n1483/1875 [======================&gt;.......] - ETA: 1s - loss: 0.3244 - accuracy: 0.9054\n1492/1875 [======================&gt;.......] - ETA: 1s - loss: 0.3235 - accuracy: 0.9057\n1501/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3227 - accuracy: 0.9059\n1513/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3219 - accuracy: 0.9061\n1526/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3214 - accuracy: 0.9062\n1536/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3207 - accuracy: 0.9065\n1546/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3202 - accuracy: 0.9066\n1556/1875 [=======================&gt;......] - ETA: 1s - loss: 0.3193 - accuracy: 0.9069\n1568/1875 [========================&gt;.....] - ETA: 1s - loss: 0.3181 - accuracy: 0.9072\n1580/1875 [========================&gt;.....] - ETA: 1s - loss: 0.3170 - accuracy: 0.9074\n1592/1875 [========================&gt;.....] - ETA: 1s - loss: 0.3163 - accuracy: 0.9076\n1604/1875 [========================&gt;.....] - ETA: 1s - loss: 0.3157 - accuracy: 0.9077\n1615/1875 [========================&gt;.....] - ETA: 0s - loss: 0.3147 - accuracy: 0.9080\n1624/1875 [========================&gt;.....] - ETA: 0s - loss: 0.3142 - accuracy: 0.9082\n1634/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3132 - accuracy: 0.9085\n1646/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3123 - accuracy: 0.9087\n1655/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3116 - accuracy: 0.9089\n1667/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3107 - accuracy: 0.9092\n1679/1875 [=========================&gt;....] - ETA: 0s - loss: 0.3095 - accuracy: 0.9096\n1690/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3086 - accuracy: 0.9098\n1701/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3078 - accuracy: 0.9100\n1712/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3067 - accuracy: 0.9103\n1723/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3057 - accuracy: 0.9105\n1734/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3046 - accuracy: 0.9108\n1745/1875 [==========================&gt;...] - ETA: 0s - loss: 0.3037 - accuracy: 0.9110\n1756/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3031 - accuracy: 0.9112\n1767/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3023 - accuracy: 0.9114\n1777/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3018 - accuracy: 0.9116\n1788/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3011 - accuracy: 0.9119\n1799/1875 [===========================&gt;..] - ETA: 0s - loss: 0.3001 - accuracy: 0.9122\n1810/1875 [===========================&gt;..] - ETA: 0s - loss: 0.2995 - accuracy: 0.9125\n1821/1875 [============================&gt;.] - ETA: 0s - loss: 0.2988 - accuracy: 0.9127\n1833/1875 [============================&gt;.] - ETA: 0s - loss: 0.2977 - accuracy: 0.9130\n1843/1875 [============================&gt;.] - ETA: 0s - loss: 0.2969 - accuracy: 0.9132\n1854/1875 [============================&gt;.] - ETA: 0s - loss: 0.2962 - accuracy: 0.9134\n1865/1875 [============================&gt;.] - ETA: 0s - loss: 0.2957 - accuracy: 0.9135\n1875/1875 [==============================] - 8s 4ms/step - loss: 0.2951 - accuracy: 0.9137\nEpoch 2/5\n\n   1/1875 [..............................] - ETA: 7s - loss: 0.0998 - accuracy: 1.0000\n  12/1875 [..............................] - ETA: 8s - loss: 0.1320 - accuracy: 0.9583\n  24/1875 [..............................] - ETA: 8s - loss: 0.1455 - accuracy: 0.9583\n  34/1875 [..............................] - ETA: 8s - loss: 0.1534 - accuracy: 0.9577\n  44/1875 [..............................] - ETA: 9s - loss: 0.1484 - accuracy: 0.9567\n  54/1875 [..............................] - ETA: 9s - loss: 0.1582 - accuracy: 0.9554\n  65/1875 [&gt;.............................] - ETA: 9s - loss: 0.1585 - accuracy: 0.9514\n  74/1875 [&gt;.............................] - ETA: 9s - loss: 0.1649 - accuracy: 0.9502\n  86/1875 [&gt;.............................] - ETA: 8s - loss: 0.1564 - accuracy: 0.9517\n  97/1875 [&gt;.............................] - ETA: 8s - loss: 0.1564 - accuracy: 0.9536\n 108/1875 [&gt;.............................] - ETA: 8s - loss: 0.1565 - accuracy: 0.9540\n 118/1875 [&gt;.............................] - ETA: 8s - loss: 0.1583 - accuracy: 0.9526\n 126/1875 [=&gt;............................] - ETA: 8s - loss: 0.1587 - accuracy: 0.9524\n 134/1875 [=&gt;............................] - ETA: 9s - loss: 0.1573 - accuracy: 0.9524\n 145/1875 [=&gt;............................] - ETA: 8s - loss: 0.1566 - accuracy: 0.9524\n 155/1875 [=&gt;............................] - ETA: 8s - loss: 0.1598 - accuracy: 0.9518\n 163/1875 [=&gt;............................] - ETA: 9s - loss: 0.1588 - accuracy: 0.9526\n 171/1875 [=&gt;............................] - ETA: 9s - loss: 0.1570 - accuracy: 0.9529\n 180/1875 [=&gt;............................] - ETA: 9s - loss: 0.1567 - accuracy: 0.9526\n 188/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1561 - accuracy: 0.9530\n 196/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1573 - accuracy: 0.9520\n 204/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1591 - accuracy: 0.9516\n 212/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1571 - accuracy: 0.9527\n 222/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1591 - accuracy: 0.9520\n 230/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1596 - accuracy: 0.9524\n 238/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1602 - accuracy: 0.9522\n 247/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1631 - accuracy: 0.9515\n 255/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1619 - accuracy: 0.9520\n 264/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1593 - accuracy: 0.9529\n 275/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1594 - accuracy: 0.9528\n 284/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1594 - accuracy: 0.9529\n 292/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1602 - accuracy: 0.9525\n 301/1875 [===&gt;..........................] - ETA: 8s - loss: 0.1595 - accuracy: 0.9532\n 310/1875 [===&gt;..........................] - ETA: 8s - loss: 0.1573 - accuracy: 0.9536\n 321/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1578 - accuracy: 0.9535\n 332/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1571 - accuracy: 0.9540\n 341/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1561 - accuracy: 0.9542\n 351/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1553 - accuracy: 0.9546\n 361/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1554 - accuracy: 0.9547\n 371/1875 [====&gt;.........................] - ETA: 8s - loss: 0.1561 - accuracy: 0.9543\n 381/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1570 - accuracy: 0.9539\n 390/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1569 - accuracy: 0.9538\n 400/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1566 - accuracy: 0.9537\n 410/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1554 - accuracy: 0.9540\n 420/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1544 - accuracy: 0.9542\n 429/1875 [=====&gt;........................] - ETA: 8s - loss: 0.1531 - accuracy: 0.9545\n 440/1875 [======&gt;.......................] - ETA: 8s - loss: 0.1546 - accuracy: 0.9541\n 449/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1549 - accuracy: 0.9541\n 456/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1553 - accuracy: 0.9540\n 463/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1544 - accuracy: 0.9544\n 471/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1539 - accuracy: 0.9546\n 481/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1540 - accuracy: 0.9543\n 492/1875 [======&gt;.......................] - ETA: 7s - loss: 0.1541 - accuracy: 0.9544\n 501/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1536 - accuracy: 0.9546\n 512/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1526 - accuracy: 0.9551\n 522/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1535 - accuracy: 0.9549\n 530/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1534 - accuracy: 0.9549\n 538/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1527 - accuracy: 0.9553\n 547/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1535 - accuracy: 0.9550\n 557/1875 [=======&gt;......................] - ETA: 7s - loss: 0.1537 - accuracy: 0.9548\n 568/1875 [========&gt;.....................] - ETA: 7s - loss: 0.1533 - accuracy: 0.9548\n 578/1875 [========&gt;.....................] - ETA: 7s - loss: 0.1531 - accuracy: 0.9548\n 589/1875 [========&gt;.....................] - ETA: 7s - loss: 0.1531 - accuracy: 0.9547\n 600/1875 [========&gt;.....................] - ETA: 7s - loss: 0.1541 - accuracy: 0.9542\n 610/1875 [========&gt;.....................] - ETA: 7s - loss: 0.1533 - accuracy: 0.9545\n 621/1875 [========&gt;.....................] - ETA: 6s - loss: 0.1537 - accuracy: 0.9543\n 631/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1532 - accuracy: 0.9545\n 642/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1528 - accuracy: 0.9546\n 653/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1525 - accuracy: 0.9547\n 664/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1534 - accuracy: 0.9543\n 674/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1534 - accuracy: 0.9542\n 684/1875 [=========&gt;....................] - ETA: 6s - loss: 0.1537 - accuracy: 0.9541\n 692/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1537 - accuracy: 0.9540\n 702/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1542 - accuracy: 0.9539\n 712/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1542 - accuracy: 0.9540\n 722/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1543 - accuracy: 0.9540\n 733/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1542 - accuracy: 0.9540\n 744/1875 [==========&gt;...................] - ETA: 6s - loss: 0.1542 - accuracy: 0.9539\n 754/1875 [===========&gt;..................] - ETA: 6s - loss: 0.1535 - accuracy: 0.9542\n 765/1875 [===========&gt;..................] - ETA: 6s - loss: 0.1534 - accuracy: 0.9542\n 774/1875 [===========&gt;..................] - ETA: 6s - loss: 0.1532 - accuracy: 0.9543\n 782/1875 [===========&gt;..................] - ETA: 6s - loss: 0.1529 - accuracy: 0.9543\n 792/1875 [===========&gt;..................] - ETA: 5s - loss: 0.1526 - accuracy: 0.9543\n 801/1875 [===========&gt;..................] - ETA: 5s - loss: 0.1528 - accuracy: 0.9541\n 809/1875 [===========&gt;..................] - ETA: 5s - loss: 0.1521 - accuracy: 0.9544\n 817/1875 [============&gt;.................] - ETA: 5s - loss: 0.1516 - accuracy: 0.9546\n 825/1875 [============&gt;.................] - ETA: 5s - loss: 0.1509 - accuracy: 0.9548\n 833/1875 [============&gt;.................] - ETA: 5s - loss: 0.1506 - accuracy: 0.9548\n 842/1875 [============&gt;.................] - ETA: 5s - loss: 0.1506 - accuracy: 0.9547\n 850/1875 [============&gt;.................] - ETA: 5s - loss: 0.1503 - accuracy: 0.9549\n 858/1875 [============&gt;.................] - ETA: 5s - loss: 0.1503 - accuracy: 0.9548\n 866/1875 [============&gt;.................] - ETA: 5s - loss: 0.1502 - accuracy: 0.9549\n 874/1875 [============&gt;.................] - ETA: 5s - loss: 0.1504 - accuracy: 0.9548\n 882/1875 [=============&gt;................] - ETA: 5s - loss: 0.1501 - accuracy: 0.9548\n 889/1875 [=============&gt;................] - ETA: 5s - loss: 0.1505 - accuracy: 0.9545\n 896/1875 [=============&gt;................] - ETA: 5s - loss: 0.1508 - accuracy: 0.9545\n 903/1875 [=============&gt;................] - ETA: 5s - loss: 0.1515 - accuracy: 0.9543\n 910/1875 [=============&gt;................] - ETA: 5s - loss: 0.1513 - accuracy: 0.9545\n 917/1875 [=============&gt;................] - ETA: 5s - loss: 0.1510 - accuracy: 0.9545\n 924/1875 [=============&gt;................] - ETA: 5s - loss: 0.1506 - accuracy: 0.9545\n 931/1875 [=============&gt;................] - ETA: 5s - loss: 0.1506 - accuracy: 0.9546\n 939/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1503 - accuracy: 0.9545\n 947/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1502 - accuracy: 0.9545\n 957/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1503 - accuracy: 0.9545\n 967/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1499 - accuracy: 0.9547\n 976/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1502 - accuracy: 0.9544\n 986/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1498 - accuracy: 0.9545\n 997/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1498 - accuracy: 0.9545\n1008/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1492 - accuracy: 0.9549\n1019/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1493 - accuracy: 0.9549\n1029/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1489 - accuracy: 0.9550\n1039/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1487 - accuracy: 0.9551\n1048/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1483 - accuracy: 0.9552\n1058/1875 [===============&gt;..............] - ETA: 4s - loss: 0.1478 - accuracy: 0.9554\n1069/1875 [================&gt;.............] - ETA: 4s - loss: 0.1473 - accuracy: 0.9555\n1079/1875 [================&gt;.............] - ETA: 4s - loss: 0.1471 - accuracy: 0.9556\n1087/1875 [================&gt;.............] - ETA: 4s - loss: 0.1467 - accuracy: 0.9557\n1095/1875 [================&gt;.............] - ETA: 4s - loss: 0.1462 - accuracy: 0.9559\n1103/1875 [================&gt;.............] - ETA: 4s - loss: 0.1465 - accuracy: 0.9557\n1111/1875 [================&gt;.............] - ETA: 4s - loss: 0.1460 - accuracy: 0.9559\n1122/1875 [================&gt;.............] - ETA: 4s - loss: 0.1460 - accuracy: 0.9559\n1132/1875 [=================&gt;............] - ETA: 4s - loss: 0.1459 - accuracy: 0.9560\n1140/1875 [=================&gt;............] - ETA: 4s - loss: 0.1461 - accuracy: 0.9559\n1147/1875 [=================&gt;............] - ETA: 4s - loss: 0.1463 - accuracy: 0.9559\n1154/1875 [=================&gt;............] - ETA: 4s - loss: 0.1462 - accuracy: 0.9558\n1161/1875 [=================&gt;............] - ETA: 4s - loss: 0.1464 - accuracy: 0.9558\n1168/1875 [=================&gt;............] - ETA: 4s - loss: 0.1463 - accuracy: 0.9559\n1176/1875 [=================&gt;............] - ETA: 4s - loss: 0.1464 - accuracy: 0.9558\n1184/1875 [=================&gt;............] - ETA: 3s - loss: 0.1461 - accuracy: 0.9559\n1191/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1458 - accuracy: 0.9560\n1199/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1458 - accuracy: 0.9559\n1206/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1457 - accuracy: 0.9560\n1214/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1457 - accuracy: 0.9561\n1221/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1457 - accuracy: 0.9561\n1229/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1455 - accuracy: 0.9562\n1237/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1455 - accuracy: 0.9561\n1245/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1456 - accuracy: 0.9560\n1253/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1451 - accuracy: 0.9561\n1260/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1450 - accuracy: 0.9561\n1268/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1454 - accuracy: 0.9560\n1277/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1455 - accuracy: 0.9559\n1288/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1452 - accuracy: 0.9560\n1299/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1451 - accuracy: 0.9560\n1309/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1446 - accuracy: 0.9561\n1319/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1445 - accuracy: 0.9561\n1329/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1444 - accuracy: 0.9562\n1339/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1442 - accuracy: 0.9562\n1350/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1445 - accuracy: 0.9562\n1361/1875 [====================&gt;.........] - ETA: 2s - loss: 0.1443 - accuracy: 0.9563\n1370/1875 [====================&gt;.........] - ETA: 2s - loss: 0.1443 - accuracy: 0.9563\n1380/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1444 - accuracy: 0.9562\n1391/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1445 - accuracy: 0.9562\n1400/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1446 - accuracy: 0.9561\n1408/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1445 - accuracy: 0.9560\n1417/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1442 - accuracy: 0.9561\n1428/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1436 - accuracy: 0.9563\n1438/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1440 - accuracy: 0.9562\n1449/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1437 - accuracy: 0.9563\n1460/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1432 - accuracy: 0.9564\n1470/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1429 - accuracy: 0.9565\n1478/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1429 - accuracy: 0.9565\n1486/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1430 - accuracy: 0.9565\n1494/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1429 - accuracy: 0.9566\n1502/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1429 - accuracy: 0.9566\n1510/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1428 - accuracy: 0.9567\n1518/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1429 - accuracy: 0.9567\n1526/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1425 - accuracy: 0.9568\n1533/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1425 - accuracy: 0.9568\n1541/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1424 - accuracy: 0.9569\n1548/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1424 - accuracy: 0.9569\n1555/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1423 - accuracy: 0.9569\n1562/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1421 - accuracy: 0.9569\n1569/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1420 - accuracy: 0.9570\n1576/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1422 - accuracy: 0.9570\n1583/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1423 - accuracy: 0.9569\n1590/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1420 - accuracy: 0.9570\n1598/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1421 - accuracy: 0.9570\n1605/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1420 - accuracy: 0.9571\n1613/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1419 - accuracy: 0.9571\n1624/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1422 - accuracy: 0.9570\n1635/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1420 - accuracy: 0.9571\n1645/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1421 - accuracy: 0.9570\n1656/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1423 - accuracy: 0.9570\n1667/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1422 - accuracy: 0.9570\n1678/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1420 - accuracy: 0.9571\n1689/1875 [==========================&gt;...] - ETA: 1s - loss: 0.1418 - accuracy: 0.9572\n1700/1875 [==========================&gt;...] - ETA: 1s - loss: 0.1418 - accuracy: 0.9572\n1709/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1417 - accuracy: 0.9572\n1718/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1416 - accuracy: 0.9572\n1729/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1419 - accuracy: 0.9572\n1739/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1417 - accuracy: 0.9573\n1750/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1414 - accuracy: 0.9573\n1761/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1415 - accuracy: 0.9573\n1771/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1413 - accuracy: 0.9574\n1781/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1414 - accuracy: 0.9574\n1791/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1413 - accuracy: 0.9575\n1802/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1412 - accuracy: 0.9575\n1812/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1411 - accuracy: 0.9575\n1820/1875 [============================&gt;.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9576\n1828/1875 [============================&gt;.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9576\n1835/1875 [============================&gt;.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9576\n1843/1875 [============================&gt;.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9577\n1851/1875 [============================&gt;.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9577\n1859/1875 [============================&gt;.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9577\n1866/1875 [============================&gt;.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9577\n1873/1875 [============================&gt;.] - ETA: 0s - loss: 0.1409 - accuracy: 0.9577\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.1409 - accuracy: 0.9577\nEpoch 3/5\n\n   1/1875 [..............................] - ETA: 14s - loss: 0.1379 - accuracy: 0.9375\n   8/1875 [..............................] - ETA: 13s - loss: 0.1125 - accuracy: 0.9648\n  16/1875 [..............................] - ETA: 13s - loss: 0.1006 - accuracy: 0.9727\n  24/1875 [..............................] - ETA: 13s - loss: 0.1249 - accuracy: 0.9688\n  32/1875 [..............................] - ETA: 12s - loss: 0.1097 - accuracy: 0.9717\n  40/1875 [..............................] - ETA: 12s - loss: 0.1074 - accuracy: 0.9703\n  48/1875 [..............................] - ETA: 12s - loss: 0.1207 - accuracy: 0.9668\n  56/1875 [..............................] - ETA: 12s - loss: 0.1182 - accuracy: 0.9660\n  63/1875 [&gt;.............................] - ETA: 12s - loss: 0.1149 - accuracy: 0.9673\n  71/1875 [&gt;.............................] - ETA: 12s - loss: 0.1115 - accuracy: 0.9679\n  79/1875 [&gt;.............................] - ETA: 12s - loss: 0.1111 - accuracy: 0.9668\n  87/1875 [&gt;.............................] - ETA: 12s - loss: 0.1143 - accuracy: 0.9644\n  97/1875 [&gt;.............................] - ETA: 12s - loss: 0.1111 - accuracy: 0.9659\n 108/1875 [&gt;.............................] - ETA: 11s - loss: 0.1131 - accuracy: 0.9661\n 118/1875 [&gt;.............................] - ETA: 11s - loss: 0.1117 - accuracy: 0.9674\n 128/1875 [=&gt;............................] - ETA: 11s - loss: 0.1131 - accuracy: 0.9666\n 137/1875 [=&gt;............................] - ETA: 11s - loss: 0.1136 - accuracy: 0.9665\n 146/1875 [=&gt;............................] - ETA: 10s - loss: 0.1146 - accuracy: 0.9664\n 157/1875 [=&gt;............................] - ETA: 10s - loss: 0.1133 - accuracy: 0.9670\n 167/1875 [=&gt;............................] - ETA: 10s - loss: 0.1137 - accuracy: 0.9669\n 178/1875 [=&gt;............................] - ETA: 10s - loss: 0.1136 - accuracy: 0.9666\n 189/1875 [==&gt;...........................] - ETA: 10s - loss: 0.1135 - accuracy: 0.9664\n 199/1875 [==&gt;...........................] - ETA: 10s - loss: 0.1123 - accuracy: 0.9666\n 210/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1145 - accuracy: 0.9655 \n 221/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1148 - accuracy: 0.9652\n 231/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1140 - accuracy: 0.9655\n 240/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1148 - accuracy: 0.9652\n 248/1875 [==&gt;...........................] - ETA: 9s - loss: 0.1148 - accuracy: 0.9655\n 256/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1147 - accuracy: 0.9653\n 266/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1129 - accuracy: 0.9663\n 275/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1111 - accuracy: 0.9666\n 283/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1101 - accuracy: 0.9669\n 291/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1122 - accuracy: 0.9664\n 298/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1121 - accuracy: 0.9664\n 305/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1117 - accuracy: 0.9666\n 312/1875 [===&gt;..........................] - ETA: 9s - loss: 0.1122 - accuracy: 0.9662\n 318/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1118 - accuracy: 0.9664\n 325/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1133 - accuracy: 0.9659\n 333/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1143 - accuracy: 0.9657\n 340/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1128 - accuracy: 0.9662\n 347/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1129 - accuracy: 0.9661\n 355/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1132 - accuracy: 0.9659\n 362/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1129 - accuracy: 0.9662\n 369/1875 [====&gt;.........................] - ETA: 9s - loss: 0.1140 - accuracy: 0.9658\n 376/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1137 - accuracy: 0.9659\n 384/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1138 - accuracy: 0.9658\n 391/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1142 - accuracy: 0.9657\n 399/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1139 - accuracy: 0.9658\n 406/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1131 - accuracy: 0.9661\n 413/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1147 - accuracy: 0.9660\n 420/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1141 - accuracy: 0.9662\n 428/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1143 - accuracy: 0.9663\n 436/1875 [=====&gt;........................] - ETA: 9s - loss: 0.1147 - accuracy: 0.9662\n 444/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1148 - accuracy: 0.9661\n 452/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1148 - accuracy: 0.9662\n 460/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1139 - accuracy: 0.9666\n 467/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1143 - accuracy: 0.9663\n 474/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1138 - accuracy: 0.9664\n 481/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1144 - accuracy: 0.9665\n 488/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1142 - accuracy: 0.9666\n 496/1875 [======&gt;.......................] - ETA: 9s - loss: 0.1137 - accuracy: 0.9667\n 504/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1146 - accuracy: 0.9665\n 512/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1158 - accuracy: 0.9661\n 520/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1153 - accuracy: 0.9663\n 528/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1144 - accuracy: 0.9665\n 536/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1141 - accuracy: 0.9666\n 544/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1146 - accuracy: 0.9665\n 552/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1150 - accuracy: 0.9663\n 560/1875 [=======&gt;......................] - ETA: 8s - loss: 0.1147 - accuracy: 0.9664\n 568/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1145 - accuracy: 0.9664\n 576/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1148 - accuracy: 0.9663\n 584/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1143 - accuracy: 0.9664\n 592/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1141 - accuracy: 0.9663\n 600/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1135 - accuracy: 0.9665\n 608/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1132 - accuracy: 0.9665\n 616/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1130 - accuracy: 0.9666\n 624/1875 [========&gt;.....................] - ETA: 8s - loss: 0.1123 - accuracy: 0.9667\n 632/1875 [=========&gt;....................] - ETA: 8s - loss: 0.1121 - accuracy: 0.9668\n 640/1875 [=========&gt;....................] - ETA: 8s - loss: 0.1124 - accuracy: 0.9665\n 648/1875 [=========&gt;....................] - ETA: 8s - loss: 0.1125 - accuracy: 0.9665\n 656/1875 [=========&gt;....................] - ETA: 7s - loss: 0.1126 - accuracy: 0.9665\n 664/1875 [=========&gt;....................] - ETA: 7s - loss: 0.1124 - accuracy: 0.9666\n 672/1875 [=========&gt;....................] - ETA: 7s - loss: 0.1120 - accuracy: 0.9667\n 680/1875 [=========&gt;....................] - ETA: 7s - loss: 0.1114 - accuracy: 0.9668\n 688/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1116 - accuracy: 0.9665\n 696/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1119 - accuracy: 0.9665\n 704/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1115 - accuracy: 0.9666\n 712/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1117 - accuracy: 0.9666\n 719/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1114 - accuracy: 0.9666\n 727/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1113 - accuracy: 0.9666\n 735/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1113 - accuracy: 0.9667\n 743/1875 [==========&gt;...................] - ETA: 7s - loss: 0.1112 - accuracy: 0.9667\n 751/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1113 - accuracy: 0.9666\n 758/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1110 - accuracy: 0.9667\n 765/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1112 - accuracy: 0.9667\n 773/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1115 - accuracy: 0.9667\n 781/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1114 - accuracy: 0.9667\n 788/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1115 - accuracy: 0.9666\n 796/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1118 - accuracy: 0.9667\n 804/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1117 - accuracy: 0.9667\n 811/1875 [===========&gt;..................] - ETA: 7s - loss: 0.1115 - accuracy: 0.9667\n 819/1875 [============&gt;.................] - ETA: 7s - loss: 0.1112 - accuracy: 0.9668\n 827/1875 [============&gt;.................] - ETA: 6s - loss: 0.1110 - accuracy: 0.9669\n 835/1875 [============&gt;.................] - ETA: 6s - loss: 0.1105 - accuracy: 0.9670\n 842/1875 [============&gt;.................] - ETA: 6s - loss: 0.1106 - accuracy: 0.9669\n 849/1875 [============&gt;.................] - ETA: 6s - loss: 0.1106 - accuracy: 0.9669\n 856/1875 [============&gt;.................] - ETA: 6s - loss: 0.1102 - accuracy: 0.9671\n 866/1875 [============&gt;.................] - ETA: 6s - loss: 0.1101 - accuracy: 0.9671\n 877/1875 [=============&gt;................] - ETA: 6s - loss: 0.1104 - accuracy: 0.9670\n 888/1875 [=============&gt;................] - ETA: 6s - loss: 0.1101 - accuracy: 0.9672\n 898/1875 [=============&gt;................] - ETA: 6s - loss: 0.1097 - accuracy: 0.9674\n 907/1875 [=============&gt;................] - ETA: 6s - loss: 0.1094 - accuracy: 0.9675\n 918/1875 [=============&gt;................] - ETA: 6s - loss: 0.1096 - accuracy: 0.9675\n 928/1875 [=============&gt;................] - ETA: 6s - loss: 0.1105 - accuracy: 0.9672\n 938/1875 [==============&gt;...............] - ETA: 6s - loss: 0.1105 - accuracy: 0.9672\n 949/1875 [==============&gt;...............] - ETA: 6s - loss: 0.1104 - accuracy: 0.9672\n 960/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1103 - accuracy: 0.9672\n 971/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1103 - accuracy: 0.9672\n 982/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1101 - accuracy: 0.9673\n 992/1875 [==============&gt;...............] - ETA: 5s - loss: 0.1102 - accuracy: 0.9672\n1000/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1100 - accuracy: 0.9673\n1009/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1097 - accuracy: 0.9674\n1019/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1094 - accuracy: 0.9674\n1027/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1094 - accuracy: 0.9674\n1036/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1091 - accuracy: 0.9674\n1046/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1090 - accuracy: 0.9674\n1057/1875 [===============&gt;..............] - ETA: 5s - loss: 0.1086 - accuracy: 0.9676\n1066/1875 [================&gt;.............] - ETA: 5s - loss: 0.1090 - accuracy: 0.9675\n1074/1875 [================&gt;.............] - ETA: 5s - loss: 0.1086 - accuracy: 0.9677\n1084/1875 [================&gt;.............] - ETA: 5s - loss: 0.1082 - accuracy: 0.9678\n1092/1875 [================&gt;.............] - ETA: 4s - loss: 0.1084 - accuracy: 0.9677\n1101/1875 [================&gt;.............] - ETA: 4s - loss: 0.1084 - accuracy: 0.9677\n1112/1875 [================&gt;.............] - ETA: 4s - loss: 0.1082 - accuracy: 0.9678\n1123/1875 [================&gt;.............] - ETA: 4s - loss: 0.1080 - accuracy: 0.9679\n1132/1875 [=================&gt;............] - ETA: 4s - loss: 0.1080 - accuracy: 0.9679\n1141/1875 [=================&gt;............] - ETA: 4s - loss: 0.1080 - accuracy: 0.9678\n1151/1875 [=================&gt;............] - ETA: 4s - loss: 0.1082 - accuracy: 0.9678\n1162/1875 [=================&gt;............] - ETA: 4s - loss: 0.1086 - accuracy: 0.9677\n1171/1875 [=================&gt;............] - ETA: 4s - loss: 0.1086 - accuracy: 0.9677\n1181/1875 [=================&gt;............] - ETA: 4s - loss: 0.1084 - accuracy: 0.9678\n1191/1875 [==================&gt;...........] - ETA: 4s - loss: 0.1085 - accuracy: 0.9677\n1202/1875 [==================&gt;...........] - ETA: 4s - loss: 0.1081 - accuracy: 0.9678\n1212/1875 [==================&gt;...........] - ETA: 4s - loss: 0.1082 - accuracy: 0.9677\n1221/1875 [==================&gt;...........] - ETA: 4s - loss: 0.1080 - accuracy: 0.9679\n1230/1875 [==================&gt;...........] - ETA: 4s - loss: 0.1079 - accuracy: 0.9679\n1241/1875 [==================&gt;...........] - ETA: 3s - loss: 0.1078 - accuracy: 0.9679\n1252/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1079 - accuracy: 0.9679\n1263/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1075 - accuracy: 0.9681\n1274/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1075 - accuracy: 0.9680\n1283/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1074 - accuracy: 0.9681\n1293/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1076 - accuracy: 0.9680\n1302/1875 [===================&gt;..........] - ETA: 3s - loss: 0.1072 - accuracy: 0.9681\n1313/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1070 - accuracy: 0.9681\n1324/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1068 - accuracy: 0.9681\n1335/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1068 - accuracy: 0.9680\n1346/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1069 - accuracy: 0.9680\n1357/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1070 - accuracy: 0.9680\n1367/1875 [====================&gt;.........] - ETA: 3s - loss: 0.1072 - accuracy: 0.9679\n1375/1875 [=====================&gt;........] - ETA: 3s - loss: 0.1071 - accuracy: 0.9679\n1383/1875 [=====================&gt;........] - ETA: 3s - loss: 0.1069 - accuracy: 0.9679\n1391/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1068 - accuracy: 0.9679\n1400/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1074 - accuracy: 0.9679\n1409/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1074 - accuracy: 0.9679\n1418/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1075 - accuracy: 0.9679\n1428/1875 [=====================&gt;........] - ETA: 2s - loss: 0.1073 - accuracy: 0.9679\n1438/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1074 - accuracy: 0.9678\n1449/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1074 - accuracy: 0.9679\n1459/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1073 - accuracy: 0.9680\n1470/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1072 - accuracy: 0.9680\n1478/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1073 - accuracy: 0.9680\n1486/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1073 - accuracy: 0.9680\n1494/1875 [======================&gt;.......] - ETA: 2s - loss: 0.1073 - accuracy: 0.9680\n1503/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1074 - accuracy: 0.9679\n1512/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1072 - accuracy: 0.9679\n1520/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1070 - accuracy: 0.9680\n1529/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1070 - accuracy: 0.9680\n1537/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1070 - accuracy: 0.9680\n1545/1875 [=======================&gt;......] - ETA: 2s - loss: 0.1072 - accuracy: 0.9679\n1555/1875 [=======================&gt;......] - ETA: 1s - loss: 0.1073 - accuracy: 0.9679\n1565/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1074 - accuracy: 0.9679\n1576/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1077 - accuracy: 0.9678\n1585/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1080 - accuracy: 0.9677\n1593/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1078 - accuracy: 0.9678\n1602/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1078 - accuracy: 0.9678\n1613/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1078 - accuracy: 0.9678\n1624/1875 [========================&gt;.....] - ETA: 1s - loss: 0.1078 - accuracy: 0.9679\n1635/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1076 - accuracy: 0.9679\n1646/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1073 - accuracy: 0.9680\n1657/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1074 - accuracy: 0.9680\n1667/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1071 - accuracy: 0.9680\n1678/1875 [=========================&gt;....] - ETA: 1s - loss: 0.1070 - accuracy: 0.9680\n1688/1875 [==========================&gt;...] - ETA: 1s - loss: 0.1069 - accuracy: 0.9681\n1699/1875 [==========================&gt;...] - ETA: 1s - loss: 0.1067 - accuracy: 0.9681\n1709/1875 [==========================&gt;...] - ETA: 1s - loss: 0.1066 - accuracy: 0.9682\n1719/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1062 - accuracy: 0.9684\n1728/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1061 - accuracy: 0.9684\n1737/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1062 - accuracy: 0.9684\n1746/1875 [==========================&gt;...] - ETA: 0s - loss: 0.1063 - accuracy: 0.9684\n1757/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1060 - accuracy: 0.9684\n1768/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1058 - accuracy: 0.9685\n1779/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1055 - accuracy: 0.9686\n1790/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1057 - accuracy: 0.9685\n1801/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1058 - accuracy: 0.9685\n1812/1875 [===========================&gt;..] - ETA: 0s - loss: 0.1057 - accuracy: 0.9685\n1823/1875 [============================&gt;.] - ETA: 0s - loss: 0.1060 - accuracy: 0.9685\n1833/1875 [============================&gt;.] - ETA: 0s - loss: 0.1060 - accuracy: 0.9684\n1843/1875 [============================&gt;.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9685\n1853/1875 [============================&gt;.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9684\n1864/1875 [============================&gt;.] - ETA: 0s - loss: 0.1059 - accuracy: 0.9685\n1875/1875 [==============================] - ETA: 0s - loss: 0.1057 - accuracy: 0.9685\n1875/1875 [==============================] - 11s 6ms/step - loss: 0.1057 - accuracy: 0.9685\nEpoch 4/5\n\n   1/1875 [..............................] - ETA: 9s - loss: 0.0532 - accuracy: 1.0000\n  11/1875 [..............................] - ETA: 9s - loss: 0.0714 - accuracy: 0.9773\n  21/1875 [..............................] - ETA: 10s - loss: 0.0671 - accuracy: 0.9792\n  30/1875 [..............................] - ETA: 10s - loss: 0.0674 - accuracy: 0.9781\n  41/1875 [..............................] - ETA: 9s - loss: 0.0827 - accuracy: 0.9748 \n  51/1875 [..............................] - ETA: 9s - loss: 0.0809 - accuracy: 0.9749\n  62/1875 [..............................] - ETA: 9s - loss: 0.0859 - accuracy: 0.9723\n  72/1875 [&gt;.............................] - ETA: 9s - loss: 0.0846 - accuracy: 0.9727\n  81/1875 [&gt;.............................] - ETA: 9s - loss: 0.0824 - accuracy: 0.9730\n  91/1875 [&gt;.............................] - ETA: 9s - loss: 0.0806 - accuracy: 0.9736\n 102/1875 [&gt;.............................] - ETA: 9s - loss: 0.0818 - accuracy: 0.9721\n 112/1875 [&gt;.............................] - ETA: 9s - loss: 0.0821 - accuracy: 0.9715\n 123/1875 [&gt;.............................] - ETA: 9s - loss: 0.0825 - accuracy: 0.9721\n 133/1875 [=&gt;............................] - ETA: 9s - loss: 0.0802 - accuracy: 0.9730\n 144/1875 [=&gt;............................] - ETA: 8s - loss: 0.0831 - accuracy: 0.9729\n 154/1875 [=&gt;............................] - ETA: 8s - loss: 0.0835 - accuracy: 0.9734\n 165/1875 [=&gt;............................] - ETA: 8s - loss: 0.0820 - accuracy: 0.9741\n 176/1875 [=&gt;............................] - ETA: 8s - loss: 0.0810 - accuracy: 0.9743\n 187/1875 [=&gt;............................] - ETA: 8s - loss: 0.0833 - accuracy: 0.9731\n 197/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0821 - accuracy: 0.9735\n 207/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0830 - accuracy: 0.9733\n 218/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0817 - accuracy: 0.9738\n 229/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0806 - accuracy: 0.9741\n 239/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0802 - accuracy: 0.9742\n 247/1875 [==&gt;...........................] - ETA: 8s - loss: 0.0808 - accuracy: 0.9736\n 257/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0822 - accuracy: 0.9731\n 268/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0825 - accuracy: 0.9731\n 279/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0816 - accuracy: 0.9731\n 289/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0819 - accuracy: 0.9734\n 300/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0816 - accuracy: 0.9735\n 310/1875 [===&gt;..........................] - ETA: 8s - loss: 0.0816 - accuracy: 0.9737\n 319/1875 [====&gt;.........................] - ETA: 8s - loss: 0.0822 - accuracy: 0.9737\n 329/1875 [====&gt;.........................] - ETA: 7s - loss: 0.0809 - accuracy: 0.9743\n 340/1875 [====&gt;.........................] - ETA: 7s - loss: 0.0819 - accuracy: 0.9740\n 350/1875 [====&gt;.........................] - ETA: 7s - loss: 0.0812 - accuracy: 0.9743\n 361/1875 [====&gt;.........................] - ETA: 7s - loss: 0.0807 - accuracy: 0.9745\n 371/1875 [====&gt;.........................] - ETA: 7s - loss: 0.0805 - accuracy: 0.9746\n 382/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0811 - accuracy: 0.9741\n 393/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0814 - accuracy: 0.9740\n 402/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0811 - accuracy: 0.9742\n 413/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0819 - accuracy: 0.9741\n 424/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0821 - accuracy: 0.9740\n 435/1875 [=====&gt;........................] - ETA: 7s - loss: 0.0822 - accuracy: 0.9743\n 445/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0821 - accuracy: 0.9742\n 456/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0817 - accuracy: 0.9741\n 467/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0833 - accuracy: 0.9739\n 478/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0840 - accuracy: 0.9737\n 488/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0846 - accuracy: 0.9736\n 496/1875 [======&gt;.......................] - ETA: 7s - loss: 0.0862 - accuracy: 0.9730\n 503/1875 [=======&gt;......................] - ETA: 7s - loss: 0.0857 - accuracy: 0.9732\n 511/1875 [=======&gt;......................] - ETA: 7s - loss: 0.0859 - accuracy: 0.9733\n 519/1875 [=======&gt;......................] - ETA: 7s - loss: 0.0857 - accuracy: 0.9734\n 529/1875 [=======&gt;......................] - ETA: 7s - loss: 0.0849 - accuracy: 0.9737\n 539/1875 [=======&gt;......................] - ETA: 6s - loss: 0.0848 - accuracy: 0.9737\n 547/1875 [=======&gt;......................] - ETA: 6s - loss: 0.0845 - accuracy: 0.9739\n 558/1875 [=======&gt;......................] - ETA: 6s - loss: 0.0843 - accuracy: 0.9738\n 568/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0843 - accuracy: 0.9738\n 579/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0836 - accuracy: 0.9740\n 589/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0845 - accuracy: 0.9738\n 600/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0846 - accuracy: 0.9737\n 611/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0844 - accuracy: 0.9739\n 620/1875 [========&gt;.....................] - ETA: 6s - loss: 0.0843 - accuracy: 0.9738\n 631/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0843 - accuracy: 0.9737\n 641/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0839 - accuracy: 0.9738\n 652/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0835 - accuracy: 0.9740\n 663/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0837 - accuracy: 0.9740\n 673/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0830 - accuracy: 0.9743\n 683/1875 [=========&gt;....................] - ETA: 6s - loss: 0.0827 - accuracy: 0.9743\n 694/1875 [==========&gt;...................] - ETA: 6s - loss: 0.0827 - accuracy: 0.9743\n 704/1875 [==========&gt;...................] - ETA: 6s - loss: 0.0825 - accuracy: 0.9744\n 714/1875 [==========&gt;...................] - ETA: 6s - loss: 0.0821 - accuracy: 0.9745\n 725/1875 [==========&gt;...................] - ETA: 5s - loss: 0.0821 - accuracy: 0.9745\n 735/1875 [==========&gt;...................] - ETA: 5s - loss: 0.0831 - accuracy: 0.9741\n 745/1875 [==========&gt;...................] - ETA: 5s - loss: 0.0838 - accuracy: 0.9740\n 755/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0842 - accuracy: 0.9738\n 765/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0844 - accuracy: 0.9737\n 776/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0839 - accuracy: 0.9739\n 787/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0842 - accuracy: 0.9738\n 797/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0842 - accuracy: 0.9738\n 808/1875 [===========&gt;..................] - ETA: 5s - loss: 0.0839 - accuracy: 0.9740\n 818/1875 [============&gt;.................] - ETA: 5s - loss: 0.0840 - accuracy: 0.9739\n 827/1875 [============&gt;.................] - ETA: 5s - loss: 0.0839 - accuracy: 0.9739\n 837/1875 [============&gt;.................] - ETA: 5s - loss: 0.0845 - accuracy: 0.9737\n 847/1875 [============&gt;.................] - ETA: 5s - loss: 0.0845 - accuracy: 0.9736\n 857/1875 [============&gt;.................] - ETA: 5s - loss: 0.0844 - accuracy: 0.9737\n 867/1875 [============&gt;.................] - ETA: 5s - loss: 0.0841 - accuracy: 0.9738\n 878/1875 [=============&gt;................] - ETA: 5s - loss: 0.0842 - accuracy: 0.9737\n 889/1875 [=============&gt;................] - ETA: 5s - loss: 0.0843 - accuracy: 0.9738\n 900/1875 [=============&gt;................] - ETA: 5s - loss: 0.0845 - accuracy: 0.9739\n 910/1875 [=============&gt;................] - ETA: 5s - loss: 0.0846 - accuracy: 0.9738\n 920/1875 [=============&gt;................] - ETA: 4s - loss: 0.0849 - accuracy: 0.9738\n 931/1875 [=============&gt;................] - ETA: 4s - loss: 0.0849 - accuracy: 0.9738\n 942/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0847 - accuracy: 0.9739\n 952/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0845 - accuracy: 0.9740\n 963/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0845 - accuracy: 0.9739\n 973/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0842 - accuracy: 0.9740\n 981/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0843 - accuracy: 0.9740\n 989/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0844 - accuracy: 0.9741\n 997/1875 [==============&gt;...............] - ETA: 4s - loss: 0.0842 - accuracy: 0.9741\n1005/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0845 - accuracy: 0.9741\n1015/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0847 - accuracy: 0.9740\n1023/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0850 - accuracy: 0.9740\n1034/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0850 - accuracy: 0.9739\n1045/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0851 - accuracy: 0.9738\n1055/1875 [===============&gt;..............] - ETA: 4s - loss: 0.0852 - accuracy: 0.9739\n1065/1875 [================&gt;.............] - ETA: 4s - loss: 0.0851 - accuracy: 0.9739\n1076/1875 [================&gt;.............] - ETA: 4s - loss: 0.0848 - accuracy: 0.9740\n1085/1875 [================&gt;.............] - ETA: 4s - loss: 0.0848 - accuracy: 0.9741\n1093/1875 [================&gt;.............] - ETA: 4s - loss: 0.0849 - accuracy: 0.9741\n1101/1875 [================&gt;.............] - ETA: 4s - loss: 0.0847 - accuracy: 0.9742\n1109/1875 [================&gt;.............] - ETA: 4s - loss: 0.0849 - accuracy: 0.9741\n1117/1875 [================&gt;.............] - ETA: 3s - loss: 0.0850 - accuracy: 0.9740\n1125/1875 [=================&gt;............] - ETA: 3s - loss: 0.0850 - accuracy: 0.9740\n1136/1875 [=================&gt;............] - ETA: 3s - loss: 0.0853 - accuracy: 0.9738\n1146/1875 [=================&gt;............] - ETA: 3s - loss: 0.0854 - accuracy: 0.9738\n1154/1875 [=================&gt;............] - ETA: 3s - loss: 0.0852 - accuracy: 0.9739\n1164/1875 [=================&gt;............] - ETA: 3s - loss: 0.0852 - accuracy: 0.9738\n1175/1875 [=================&gt;............] - ETA: 3s - loss: 0.0849 - accuracy: 0.9739\n1186/1875 [=================&gt;............] - ETA: 3s - loss: 0.0850 - accuracy: 0.9738\n1197/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0854 - accuracy: 0.9737\n1207/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0852 - accuracy: 0.9737\n1217/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0852 - accuracy: 0.9738\n1226/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0854 - accuracy: 0.9737\n1234/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0854 - accuracy: 0.9737\n1243/1875 [==================&gt;...........] - ETA: 3s - loss: 0.0851 - accuracy: 0.9738\n1253/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0852 - accuracy: 0.9737\n1264/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0850 - accuracy: 0.9737\n1274/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0849 - accuracy: 0.9738\n1284/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0852 - accuracy: 0.9736\n1294/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0855 - accuracy: 0.9735\n1305/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0855 - accuracy: 0.9735\n1316/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0855 - accuracy: 0.9734\n1327/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0854 - accuracy: 0.9735\n1335/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0856 - accuracy: 0.9733\n1343/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0854 - accuracy: 0.9735\n1351/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0853 - accuracy: 0.9734\n1359/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1367/1875 [====================&gt;.........] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1376/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0851 - accuracy: 0.9735\n1386/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1397/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1408/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0848 - accuracy: 0.9736\n1418/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0849 - accuracy: 0.9736\n1429/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0854 - accuracy: 0.9734\n1440/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0854 - accuracy: 0.9734\n1451/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0854 - accuracy: 0.9734\n1461/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1472/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0852 - accuracy: 0.9735\n1482/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0855 - accuracy: 0.9734\n1492/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0855 - accuracy: 0.9734\n1502/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0853 - accuracy: 0.9734\n1512/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0854 - accuracy: 0.9734\n1522/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0857 - accuracy: 0.9733\n1531/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0858 - accuracy: 0.9733\n1541/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0860 - accuracy: 0.9733\n1552/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0858 - accuracy: 0.9734\n1563/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0857 - accuracy: 0.9734\n1574/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0859 - accuracy: 0.9733\n1584/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0856 - accuracy: 0.9734\n1595/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0859 - accuracy: 0.9734\n1606/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0858 - accuracy: 0.9733\n1617/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0859 - accuracy: 0.9733\n1628/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0859 - accuracy: 0.9734\n1639/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0860 - accuracy: 0.9732\n1649/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0860 - accuracy: 0.9733\n1660/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0860 - accuracy: 0.9732\n1670/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0860 - accuracy: 0.9732\n1681/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0861 - accuracy: 0.9732\n1691/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9732\n1701/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0861 - accuracy: 0.9732\n1710/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9732\n1719/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9732\n1727/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9731\n1735/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0860 - accuracy: 0.9732\n1743/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0859 - accuracy: 0.9732\n1753/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0861 - accuracy: 0.9732\n1763/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0862 - accuracy: 0.9731\n1770/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0864 - accuracy: 0.9731\n1780/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0867 - accuracy: 0.9730\n1790/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0866 - accuracy: 0.9730\n1798/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0867 - accuracy: 0.9730\n1806/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0866 - accuracy: 0.9731\n1814/1875 [============================&gt;.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9731\n1824/1875 [============================&gt;.] - ETA: 0s - loss: 0.0863 - accuracy: 0.9731\n1834/1875 [============================&gt;.] - ETA: 0s - loss: 0.0862 - accuracy: 0.9731\n1843/1875 [============================&gt;.] - ETA: 0s - loss: 0.0864 - accuracy: 0.9731\n1851/1875 [============================&gt;.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9730\n1860/1875 [============================&gt;.] - ETA: 0s - loss: 0.0865 - accuracy: 0.9730\n1871/1875 [============================&gt;.] - ETA: 0s - loss: 0.0863 - accuracy: 0.9731\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0864 - accuracy: 0.9730\nEpoch 5/5\n\n   1/1875 [..............................] - ETA: 9s - loss: 0.0046 - accuracy: 1.0000\n  11/1875 [..............................] - ETA: 10s - loss: 0.0449 - accuracy: 0.9801\n  19/1875 [..............................] - ETA: 10s - loss: 0.0526 - accuracy: 0.9819\n  29/1875 [..............................] - ETA: 10s - loss: 0.0576 - accuracy: 0.9817\n  39/1875 [..............................] - ETA: 10s - loss: 0.0668 - accuracy: 0.9808\n  49/1875 [..............................] - ETA: 9s - loss: 0.0603 - accuracy: 0.9821 \n  59/1875 [..............................] - ETA: 9s - loss: 0.0597 - accuracy: 0.9815\n  70/1875 [&gt;.............................] - ETA: 9s - loss: 0.0666 - accuracy: 0.9799\n  81/1875 [&gt;.............................] - ETA: 9s - loss: 0.0703 - accuracy: 0.9788\n  92/1875 [&gt;.............................] - ETA: 9s - loss: 0.0730 - accuracy: 0.9789\n 103/1875 [&gt;.............................] - ETA: 9s - loss: 0.0706 - accuracy: 0.9803\n 113/1875 [&gt;.............................] - ETA: 9s - loss: 0.0713 - accuracy: 0.9804\n 121/1875 [&gt;.............................] - ETA: 9s - loss: 0.0724 - accuracy: 0.9796\n 130/1875 [=&gt;............................] - ETA: 9s - loss: 0.0721 - accuracy: 0.9796\n 141/1875 [=&gt;............................] - ETA: 9s - loss: 0.0714 - accuracy: 0.9798\n 150/1875 [=&gt;............................] - ETA: 9s - loss: 0.0737 - accuracy: 0.9794\n 158/1875 [=&gt;............................] - ETA: 9s - loss: 0.0746 - accuracy: 0.9782\n 166/1875 [=&gt;............................] - ETA: 9s - loss: 0.0731 - accuracy: 0.9787\n 173/1875 [=&gt;............................] - ETA: 9s - loss: 0.0747 - accuracy: 0.9787\n 181/1875 [=&gt;............................] - ETA: 9s - loss: 0.0744 - accuracy: 0.9789\n 189/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0737 - accuracy: 0.9790\n 197/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0746 - accuracy: 0.9786\n 204/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0741 - accuracy: 0.9786\n 212/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0740 - accuracy: 0.9783\n 219/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0737 - accuracy: 0.9785\n 227/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0728 - accuracy: 0.9785\n 235/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0718 - accuracy: 0.9789\n 242/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0711 - accuracy: 0.9791\n 249/1875 [==&gt;...........................] - ETA: 9s - loss: 0.0711 - accuracy: 0.9788\n 256/1875 [===&gt;..........................] - ETA: 9s - loss: 0.0729 - accuracy: 0.9785\n 263/1875 [===&gt;..........................] - ETA: 9s - loss: 0.0725 - accuracy: 0.9785\n 270/1875 [===&gt;..........................] - ETA: 10s - loss: 0.0723 - accuracy: 0.9786\n 277/1875 [===&gt;..........................] - ETA: 10s - loss: 0.0726 - accuracy: 0.9787\n 284/1875 [===&gt;..........................] - ETA: 10s - loss: 0.0725 - accuracy: 0.9785\n 292/1875 [===&gt;..........................] - ETA: 10s - loss: 0.0765 - accuracy: 0.9775\n 300/1875 [===&gt;..........................] - ETA: 9s - loss: 0.0780 - accuracy: 0.9770 \n 307/1875 [===&gt;..........................] - ETA: 9s - loss: 0.0785 - accuracy: 0.9765\n 315/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0788 - accuracy: 0.9764\n 322/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0781 - accuracy: 0.9764\n 329/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0778 - accuracy: 0.9765\n 336/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0786 - accuracy: 0.9762\n 343/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0785 - accuracy: 0.9762\n 350/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0782 - accuracy: 0.9763\n 357/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0786 - accuracy: 0.9765\n 364/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0785 - accuracy: 0.9764\n 372/1875 [====&gt;.........................] - ETA: 9s - loss: 0.0780 - accuracy: 0.9766\n 379/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0774 - accuracy: 0.9769\n 386/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0771 - accuracy: 0.9768\n 394/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0764 - accuracy: 0.9770\n 402/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0756 - accuracy: 0.9771\n 409/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0756 - accuracy: 0.9772\n 417/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0762 - accuracy: 0.9771\n 424/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0762 - accuracy: 0.9770\n 432/1875 [=====&gt;........................] - ETA: 9s - loss: 0.0764 - accuracy: 0.9769\n 440/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0759 - accuracy: 0.9771\n 448/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0754 - accuracy: 0.9772\n 455/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0747 - accuracy: 0.9774\n 463/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0745 - accuracy: 0.9773\n 471/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0744 - accuracy: 0.9771\n 479/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0742 - accuracy: 0.9772\n 487/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0746 - accuracy: 0.9770\n 494/1875 [======&gt;.......................] - ETA: 9s - loss: 0.0746 - accuracy: 0.9768\n 502/1875 [=======&gt;......................] - ETA: 9s - loss: 0.0745 - accuracy: 0.9769\n 510/1875 [=======&gt;......................] - ETA: 9s - loss: 0.0752 - accuracy: 0.9768\n 518/1875 [=======&gt;......................] - ETA: 9s - loss: 0.0748 - accuracy: 0.9770\n 526/1875 [=======&gt;......................] - ETA: 9s - loss: 0.0752 - accuracy: 0.9769\n 535/1875 [=======&gt;......................] - ETA: 8s - loss: 0.0750 - accuracy: 0.9770\n 543/1875 [=======&gt;......................] - ETA: 8s - loss: 0.0749 - accuracy: 0.9770\n 551/1875 [=======&gt;......................] - ETA: 8s - loss: 0.0751 - accuracy: 0.9769\n 559/1875 [=======&gt;......................] - ETA: 8s - loss: 0.0751 - accuracy: 0.9769\n 567/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0756 - accuracy: 0.9766\n 575/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0757 - accuracy: 0.9766\n 583/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0760 - accuracy: 0.9765\n 591/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0757 - accuracy: 0.9766\n 599/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0756 - accuracy: 0.9765\n 607/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0758 - accuracy: 0.9765\n 615/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0766 - accuracy: 0.9762\n 623/1875 [========&gt;.....................] - ETA: 8s - loss: 0.0765 - accuracy: 0.9762\n 631/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0764 - accuracy: 0.9763\n 639/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0764 - accuracy: 0.9762\n 647/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0764 - accuracy: 0.9762\n 655/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0769 - accuracy: 0.9761\n 663/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0767 - accuracy: 0.9762\n 671/1875 [=========&gt;....................] - ETA: 8s - loss: 0.0765 - accuracy: 0.9763\n 679/1875 [=========&gt;....................] - ETA: 7s - loss: 0.0762 - accuracy: 0.9764\n 687/1875 [=========&gt;....................] - ETA: 7s - loss: 0.0760 - accuracy: 0.9765\n 695/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0756 - accuracy: 0.9766\n 703/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0752 - accuracy: 0.9768\n 711/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0748 - accuracy: 0.9769\n 719/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0745 - accuracy: 0.9770\n 727/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0742 - accuracy: 0.9770\n 735/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0737 - accuracy: 0.9772\n 743/1875 [==========&gt;...................] - ETA: 7s - loss: 0.0740 - accuracy: 0.9771\n 751/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0736 - accuracy: 0.9771\n 759/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0739 - accuracy: 0.9770\n 768/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0744 - accuracy: 0.9769\n 776/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0742 - accuracy: 0.9770\n 784/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0744 - accuracy: 0.9770\n 792/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0744 - accuracy: 0.9769\n 800/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0745 - accuracy: 0.9770\n 808/1875 [===========&gt;..................] - ETA: 7s - loss: 0.0742 - accuracy: 0.9771\n 816/1875 [============&gt;.................] - ETA: 7s - loss: 0.0745 - accuracy: 0.9769\n 824/1875 [============&gt;.................] - ETA: 7s - loss: 0.0741 - accuracy: 0.9771\n 832/1875 [============&gt;.................] - ETA: 6s - loss: 0.0744 - accuracy: 0.9770\n 840/1875 [============&gt;.................] - ETA: 6s - loss: 0.0742 - accuracy: 0.9770\n 848/1875 [============&gt;.................] - ETA: 6s - loss: 0.0738 - accuracy: 0.9770\n 856/1875 [============&gt;.................] - ETA: 6s - loss: 0.0735 - accuracy: 0.9771\n 865/1875 [============&gt;.................] - ETA: 6s - loss: 0.0733 - accuracy: 0.9771\n 874/1875 [============&gt;.................] - ETA: 6s - loss: 0.0734 - accuracy: 0.9771\n 882/1875 [=============&gt;................] - ETA: 6s - loss: 0.0735 - accuracy: 0.9770\n 890/1875 [=============&gt;................] - ETA: 6s - loss: 0.0739 - accuracy: 0.9770\n 898/1875 [=============&gt;................] - ETA: 6s - loss: 0.0742 - accuracy: 0.9770\n 907/1875 [=============&gt;................] - ETA: 6s - loss: 0.0742 - accuracy: 0.9770\n 915/1875 [=============&gt;................] - ETA: 6s - loss: 0.0740 - accuracy: 0.9770\n 923/1875 [=============&gt;................] - ETA: 6s - loss: 0.0740 - accuracy: 0.9769\n 931/1875 [=============&gt;................] - ETA: 6s - loss: 0.0738 - accuracy: 0.9769\n 939/1875 [==============&gt;...............] - ETA: 6s - loss: 0.0735 - accuracy: 0.9770\n 948/1875 [==============&gt;...............] - ETA: 6s - loss: 0.0735 - accuracy: 0.9771\n 957/1875 [==============&gt;...............] - ETA: 6s - loss: 0.0735 - accuracy: 0.9770\n 966/1875 [==============&gt;...............] - ETA: 6s - loss: 0.0735 - accuracy: 0.9770\n 974/1875 [==============&gt;...............] - ETA: 5s - loss: 0.0734 - accuracy: 0.9769\n 982/1875 [==============&gt;...............] - ETA: 5s - loss: 0.0733 - accuracy: 0.9768\n 990/1875 [==============&gt;...............] - ETA: 5s - loss: 0.0731 - accuracy: 0.9769\n 998/1875 [==============&gt;...............] - ETA: 5s - loss: 0.0730 - accuracy: 0.9769\n1007/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0732 - accuracy: 0.9768\n1015/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0733 - accuracy: 0.9768\n1023/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0734 - accuracy: 0.9767\n1031/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0734 - accuracy: 0.9767\n1039/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0738 - accuracy: 0.9767\n1047/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0738 - accuracy: 0.9767\n1055/1875 [===============&gt;..............] - ETA: 5s - loss: 0.0741 - accuracy: 0.9766\n1063/1875 [================&gt;.............] - ETA: 5s - loss: 0.0739 - accuracy: 0.9766\n1071/1875 [================&gt;.............] - ETA: 5s - loss: 0.0742 - accuracy: 0.9766\n1079/1875 [================&gt;.............] - ETA: 5s - loss: 0.0741 - accuracy: 0.9766\n1087/1875 [================&gt;.............] - ETA: 5s - loss: 0.0739 - accuracy: 0.9767\n1095/1875 [================&gt;.............] - ETA: 5s - loss: 0.0737 - accuracy: 0.9768\n1103/1875 [================&gt;.............] - ETA: 5s - loss: 0.0740 - accuracy: 0.9768\n1111/1875 [================&gt;.............] - ETA: 5s - loss: 0.0742 - accuracy: 0.9767\n1119/1875 [================&gt;.............] - ETA: 5s - loss: 0.0740 - accuracy: 0.9767\n1127/1875 [=================&gt;............] - ETA: 4s - loss: 0.0738 - accuracy: 0.9768\n1135/1875 [=================&gt;............] - ETA: 4s - loss: 0.0737 - accuracy: 0.9768\n1143/1875 [=================&gt;............] - ETA: 4s - loss: 0.0735 - accuracy: 0.9768\n1151/1875 [=================&gt;............] - ETA: 4s - loss: 0.0735 - accuracy: 0.9768\n1159/1875 [=================&gt;............] - ETA: 4s - loss: 0.0732 - accuracy: 0.9769\n1168/1875 [=================&gt;............] - ETA: 4s - loss: 0.0734 - accuracy: 0.9769\n1176/1875 [=================&gt;............] - ETA: 4s - loss: 0.0737 - accuracy: 0.9768\n1184/1875 [=================&gt;............] - ETA: 4s - loss: 0.0737 - accuracy: 0.9768\n1193/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0735 - accuracy: 0.9769\n1202/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0737 - accuracy: 0.9768\n1213/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0738 - accuracy: 0.9768\n1223/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0736 - accuracy: 0.9769\n1231/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0735 - accuracy: 0.9770\n1241/1875 [==================&gt;...........] - ETA: 4s - loss: 0.0734 - accuracy: 0.9770\n1251/1875 [===================&gt;..........] - ETA: 4s - loss: 0.0733 - accuracy: 0.9771\n1259/1875 [===================&gt;..........] - ETA: 4s - loss: 0.0735 - accuracy: 0.9770\n1267/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0734 - accuracy: 0.9770\n1277/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0735 - accuracy: 0.9769\n1288/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0739 - accuracy: 0.9769\n1299/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0741 - accuracy: 0.9769\n1310/1875 [===================&gt;..........] - ETA: 3s - loss: 0.0743 - accuracy: 0.9768\n1320/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0743 - accuracy: 0.9769\n1327/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0741 - accuracy: 0.9769\n1337/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0741 - accuracy: 0.9769\n1348/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0740 - accuracy: 0.9770\n1358/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0739 - accuracy: 0.9770\n1369/1875 [====================&gt;.........] - ETA: 3s - loss: 0.0737 - accuracy: 0.9771\n1379/1875 [=====================&gt;........] - ETA: 3s - loss: 0.0735 - accuracy: 0.9771\n1389/1875 [=====================&gt;........] - ETA: 3s - loss: 0.0734 - accuracy: 0.9772\n1397/1875 [=====================&gt;........] - ETA: 3s - loss: 0.0732 - accuracy: 0.9772\n1405/1875 [=====================&gt;........] - ETA: 3s - loss: 0.0731 - accuracy: 0.9773\n1413/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0729 - accuracy: 0.9773\n1421/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0730 - accuracy: 0.9773\n1432/1875 [=====================&gt;........] - ETA: 2s - loss: 0.0731 - accuracy: 0.9773\n1443/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0731 - accuracy: 0.9772\n1451/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0733 - accuracy: 0.9771\n1458/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0734 - accuracy: 0.9771\n1466/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0733 - accuracy: 0.9771\n1477/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0733 - accuracy: 0.9770\n1487/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0735 - accuracy: 0.9770\n1497/1875 [======================&gt;.......] - ETA: 2s - loss: 0.0736 - accuracy: 0.9769\n1508/1875 [=======================&gt;......] - ETA: 2s - loss: 0.0736 - accuracy: 0.9770\n1518/1875 [=======================&gt;......] - ETA: 2s - loss: 0.0734 - accuracy: 0.9771\n1529/1875 [=======================&gt;......] - ETA: 2s - loss: 0.0737 - accuracy: 0.9770\n1540/1875 [=======================&gt;......] - ETA: 2s - loss: 0.0737 - accuracy: 0.9770\n1551/1875 [=======================&gt;......] - ETA: 2s - loss: 0.0735 - accuracy: 0.9770\n1561/1875 [=======================&gt;......] - ETA: 1s - loss: 0.0737 - accuracy: 0.9769\n1571/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0738 - accuracy: 0.9769\n1579/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0738 - accuracy: 0.9769\n1587/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9769\n1595/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9769\n1603/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0739 - accuracy: 0.9769\n1611/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0739 - accuracy: 0.9770\n1619/1875 [========================&gt;.....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9770\n1627/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9770\n1635/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0741 - accuracy: 0.9770\n1643/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9770\n1651/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0742 - accuracy: 0.9770\n1659/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0741 - accuracy: 0.9770\n1667/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0741 - accuracy: 0.9770\n1675/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0740 - accuracy: 0.9771\n1682/1875 [=========================&gt;....] - ETA: 1s - loss: 0.0741 - accuracy: 0.9771\n1690/1875 [==========================&gt;...] - ETA: 1s - loss: 0.0741 - accuracy: 0.9770\n1698/1875 [==========================&gt;...] - ETA: 1s - loss: 0.0742 - accuracy: 0.9770\n1706/1875 [==========================&gt;...] - ETA: 1s - loss: 0.0742 - accuracy: 0.9770\n1714/1875 [==========================&gt;...] - ETA: 1s - loss: 0.0740 - accuracy: 0.9770\n1722/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0739 - accuracy: 0.9771\n1730/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0738 - accuracy: 0.9771\n1738/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0738 - accuracy: 0.9771\n1747/1875 [==========================&gt;...] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1755/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1763/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1771/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1779/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0738 - accuracy: 0.9771\n1787/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1795/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1803/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0736 - accuracy: 0.9771\n1811/1875 [===========================&gt;..] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1819/1875 [============================&gt;.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9770\n1827/1875 [============================&gt;.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9771\n1834/1875 [============================&gt;.] - ETA: 0s - loss: 0.0738 - accuracy: 0.9771\n1843/1875 [============================&gt;.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9771\n1851/1875 [============================&gt;.] - ETA: 0s - loss: 0.0736 - accuracy: 0.9772\n1859/1875 [============================&gt;.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9772\n1867/1875 [============================&gt;.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9772\n1875/1875 [==============================] - 12s 6ms/step - loss: 0.0733 - accuracy: 0.9772\n&lt;keras.callbacks.History object at 0x000002D3DAF4BBB0&gt;\n\n\n\nmodel.evaluate(x_test,  y_test, verbose=2)\n\n313/313 - 1s - loss: 0.0761 - accuracy: 0.9764 - 1s/epoch - 4ms/step\n[0.07606449723243713, 0.9764000177383423]\n\nprobability_model = tf.keras.Sequential([\n  model,\n  tf.keras.layers.Softmax()\n])\n\nprobability_model(x_test[:5])\n\n&lt;tf.Tensor: shape=(5, 10), dtype=float32, numpy=\narray([[1.3065116e-07, 1.1906588e-08, 8.6254568e-06, 1.9500349e-04,\n        3.9280154e-13, 2.7274782e-08, 8.9514090e-14, 9.9979573e-01,\n        1.6844812e-07, 2.5713874e-07],\n       [3.4573787e-08, 3.8334861e-06, 9.9999619e-01, 4.0571624e-08,\n        1.1119099e-15, 2.6929008e-09, 4.4391992e-09, 6.3775271e-13,\n        8.0284677e-09, 4.7792949e-15],\n       [3.5627131e-08, 9.9984312e-01, 3.6839916e-05, 1.0188468e-06,\n        8.2455826e-06, 2.3786862e-07, 2.3271245e-06, 1.0226570e-04,\n        5.7826060e-06, 2.2025354e-08],\n       [9.9988711e-01, 8.9482723e-09, 1.3988179e-05, 2.6024620e-07,\n        1.2380229e-06, 4.9973755e-06, 5.2229767e-05, 2.6953589e-05,\n        3.2521751e-07, 1.2755126e-05],\n       [2.7323288e-06, 2.6271124e-07, 2.4348870e-05, 2.1980078e-08,\n        9.9810565e-01, 4.4357641e-08, 2.3158240e-07, 4.0300849e-05,\n        6.7471746e-08, 1.8263297e-03]], dtype=float32)&gt;"
  },
  {
    "objectID": "baking/julia_labelled_array/index.html",
    "href": "baking/julia_labelled_array/index.html",
    "title": "LabelledArrays and NamedTupleTools make it easy to use the ODE model in Julia",
    "section": "",
    "text": "SEIR model using LabelledArrays\nThe LabelledArrays package makes it easy to use the ODE model in Julia. It offers a method to manage variables via keys instead of indices. Variables can be constructed using the @LArray macro or LVector.\nHowever, using arrays to store a mix of information types is not optimal for performance. I frequently need to store different variable types within a single parameter, which prompts a warning, highlighting that combining variable types in an array could reduce performance and suggesting tuples as a more efficient alternative.\n\n┌ Warning: Utilizing arrays or dictionaries to store parameters of diverse types can compromise performance. │ Using tuples is advised for better efficiency. └ @ SciMLBase C:.kim.julia_warnings.jl:32\n\nTuples, being essentially immutable, presents a challenge since some of model parameters that must be adjustible or estimated. The NamedTupleTools package offers a convenient solution with its merge function, enabling easy updates to the values within a Tuple.\nIn conclusion, the best approach seems to be leveraging LabelledArrays for state variables and NamedTupleTools for parameters. Let’s delve into this approach using the SEIR model as a case study.\n\nusing LabelledArrays, OrdinaryDiffEq, Plots, NamedTupleTools, BenchmarkTools\n\nfunction seir(du, u, p, t)\n    N = sum(u)\n    du.S = - p.β * u.S * u.I / N\n    du.E = + p.β * u.S * u.I / N - p.ϵ * u.E\n    du.I = + p.ϵ * u.E - p.γ* u.I\n    du.R = + p.γ* u.I \nend\n\nseir (generic function with 1 method)\n\n\nu0 = @LArray [0.99, 0.0, 0.01, 0.0] (:S, :E, :I, :R);\np_la = @LArray [0.5, 1/2, 1/4] (:β, :ϵ, :γ);\np_la = LVector(p_la, β=0.8, str=\"string\", int=9, la=LVector(a=1,b=2,c=3), tp=(a=1,b=2,c=3));\ntspan = (0.0, 100.0);\nprob = ODEProblem(seir, u0, tspan, p_la);\nsol = solve(prob, Tsit5(), saveat=1);\n\ns = [sol[i].S for i in 1:101];\ne = [sol[i].E for i in 1:101];\ni = [sol[i].I for i in 1:101];\nr = [sol[i].R for i in 1:101];\n\nplot(sol.t, [s e i r])\n\n\n\n\n# NamedTupleTools approach\np_nt = (β=0.5, ϵ=1/2, γ=1/4);\np_nt = merge(p_nt, (β=0.8, str=\"string\", int=9, la=LVector(a=1,b=2,c=3), tp=(a=1,b=2,c=3)));\n\nusing BenchmarkTools\ntime_la = @benchmark solve(ODEProblem(seir, u0, tspan, p_la), Tsit5(), saveat=1);\ntime_nt = @benchmark solve(ODEProblem(seir, u0, tspan, p_nt), Tsit5(), saveat=1);\ntime_la\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  115.900 μs …  1.000 ms  ┊ GC (min … max): 0.00% … 82.69%\n Time  (median):     122.100 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   127.771 μs ± 41.269 μs  ┊ GC (mean ± σ):  1.43% ±  4.10%\n\n  ▁▃▄█▇▄▃▂▁▁                                                   ▁\n  ███████████▇▇▆▇▇▇██▇▆▅▅▆▆▆▆▇▅▄▅▄▅▅▅▆▅▅▅▆▆▆▅▆▅▅▄▅▅▅▅▄▄▄▆▅▅▅▃▄ █\n  116 μs        Histogram: log(frequency) by time       211 μs &lt;\n\n Memory estimate: 106.75 KiB, allocs estimate: 5727.\n\ntime_nt\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  36.800 μs …  1.249 ms  ┊ GC (min … max): 0.00% … 95.62%\n Time  (median):     38.200 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   39.219 μs ± 26.706 μs  ┊ GC (mean ± σ):  1.49% ±  2.13%\n\n      ▁▄█▁                                                     \n  ▂▃▄▆████▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▂ ▃\n  36.8 μs         Histogram: frequency by time        49.5 μs &lt;\n\n Memory estimate: 28.64 KiB, allocs estimate: 685."
  },
  {
    "objectID": "baking/python_how_GPT_works/index.html",
    "href": "baking/python_how_GPT_works/index.html",
    "title": "How GPT works",
    "section": "",
    "text": "챗GPT 정의\n한 단어씩 더할 뿐이데 그럴듯한 문장이 된다 문장을 중간에 멈추었을 때와 정확한 질문을 했을때 대답이 다르다. 어떻게 물음으로 끝났을때 정확하게 명사로 시작하는 문장을 만드나?\n확률에 따라 다음 단어를 고르면 되는데 확률은 어디서 오나?\n제일 높은 걸 고른다고 다 되는 것도 아니고?\ntemperature 모수 = 0.8이 최고?\n1750억개 (175 billiion)의 모수를 추정하다. 뉴럴넷은 이미 1940년데에 개발\n인간의 언어 모델\n모델링, 일부의 데이타를 최대한 이용하여 처음 보는 자료에까지 적용 가능한 자료를 만들자\n뉴럴넷\n기계학습\n트레이닝, 모수의 추정\n파이썬 코드\n뉴럴넷이면 못풀 문제가 없다.\n기본적인 트레이닝을 넘어서\n\n가능성. 프롬프트 엔지니어링 MNIST 자료"
  },
  {
    "objectID": "posts/julia_labelledarrays_namedtupletools/index.html",
    "href": "posts/julia_labelledarrays_namedtupletools/index.html",
    "title": "LabelledArrays and NamedTupleTools make it easy to use the ODE model in Julia",
    "section": "",
    "text": "SEIR model using LabelledArrays\nThe LabelledArrays package makes it easy to use the ODE model in Julia. It offers a method to manage variables via keys instead of indices. Variables can be constructed using the @LArray macro or LVector.\nHowever, using arrays to store a mix of information types is not optimal for performance. I frequently need to store different variable types within a single parameter, which prompts a warning, highlighting that combining variable types in an array could reduce performance and suggesting tuples as a more efficient alternative.\n\n┌ Warning: Utilizing arrays or dictionaries to store parameters of diverse types can compromise performance. │ Using tuples is advised for better efficiency. └ @ SciMLBase C:.kim.julia_warnings.jl:32\n\nTuples, being essentially immutable, presents a challenge since some of model parameters that must be adjustible or estimated. The NamedTupleTools package offers a convenient solution with its merge function, enabling easy updates to the values within a Tuple.\nIn conclusion, the best approach seems to be leveraging LabelledArrays for state variables and NamedTupleTools for parameters. Let’s delve into this approach using the SEIR model as a case study.\n\nusing LabelledArrays, OrdinaryDiffEq, Plots, NamedTupleTools, BenchmarkTools\n\nfunction seir(du, u, p, t)\n    N = sum(u)\n    du.S = - p.β * u.S * u.I / N\n    du.E = + p.β * u.S * u.I / N - p.ϵ * u.E\n    du.I = + p.ϵ * u.E - p.γ* u.I\n    du.R = + p.γ* u.I \nend\n\nseir (generic function with 1 method)\n\n\nu0 = @LArray [0.99, 0.0, 0.01, 0.0] (:S, :E, :I, :R);\np_la = @LArray [0.5, 1/2, 1/4] (:β, :ϵ, :γ);\np_la = LVector(p_la, β=0.8, str=\"string\", int=9, la=LVector(a=1,b=2,c=3), tp=(a=1,b=2,c=3));\ntspan = (0.0, 100.0);\nprob = ODEProblem(seir, u0, tspan, p_la);\nsol = solve(prob, Tsit5(), saveat=1);\n\ns = [sol[i].S for i in 1:101];\ne = [sol[i].E for i in 1:101];\ni = [sol[i].I for i in 1:101];\nr = [sol[i].R for i in 1:101];\n\nplot(sol.t, [s e i r])\n\n\n\n\n# NamedTupleTools approach\np_nt = (β=0.5, ϵ=1/2, γ=1/4);\np_nt = merge(p_nt, (β=0.8, str=\"string\", int=9, la=LVector(a=1,b=2,c=3), tp=(a=1,b=2,c=3)));\n\nusing BenchmarkTools\ntime_la = @benchmark solve(ODEProblem(seir, u0, tspan, p_la), Tsit5(), saveat=1);\ntime_nt = @benchmark solve(ODEProblem(seir, u0, tspan, p_nt), Tsit5(), saveat=1);\ntime_la\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  118.300 μs …  1.065 ms  ┊ GC (min … max): 0.00% … 85.42%\n Time  (median):     123.800 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   127.737 μs ± 39.266 μs  ┊ GC (mean ± σ):  1.42% ±  4.10%\n\n  ▁▃▂▄██▇▄▃▂▂▁            ▁▂▁                                  ▂\n  █████████████▇█▇▆▆▇▇▆▅▆█████▇▇▆▄▅▅▅▄▄▅▅▆▅▄▅▆▅▅▄▄▅▄▁▅▄▆▅▄▅▆▅▆ █\n  118 μs        Histogram: log(frequency) by time       180 μs &lt;\n\n Memory estimate: 106.75 KiB, allocs estimate: 5727.\n\ntime_nt\n\nBenchmarkTools.Trial: 10000 samples with 1 evaluation.\n Range (min … max):  37.400 μs …  1.306 ms  ┊ GC (min … max): 0.00% … 94.58%\n Time  (median):     39.700 μs              ┊ GC (median):    0.00%\n Time  (mean ± σ):   41.141 μs ± 28.101 μs  ┊ GC (mean ± σ):  1.49% ±  2.13%\n\n   ▁▄▆██▇▆▄▃▂▁▁▁▁▁▁▁▁▁                                        ▂\n  ▅██████████████████████▇▇▆▆▆▆▆▆▅▆▇▆▆▆▆▅▅▅▁▄▃▃▅▄▁▅▃▄▃▁▁▄▄▄▃▅ █\n  37.4 μs      Histogram: log(frequency) by time      62.3 μs &lt;\n\n Memory estimate: 28.64 KiB, allocs estimate: 685."
  },
  {
    "objectID": "posts/density_frequency/index.html",
    "href": "posts/density_frequency/index.html",
    "title": "Mass-action assumption: density- vs. frequency-dependent transmission",
    "section": "",
    "text": "Density-dependent vs. frequency-dependent transmission\nIn models of transmission of directly transmitted pathogens, e.g., COVID-19, the transmission is assumed to occur via so-called mass action principle. It means the rate of newly infected people per unit area, per unit of time is proportional to the product between the numbers (or densities) of susceptible and infectious individuals. The term appears to be first coined by Hamer in 1906 in his paper. However, as indicated by McCallum, there have been some confusion over using the term, mass action. The confusion is around that mass action principle may be implemented in the manner of frequency-dependent or density-dependent transmission.\nIn the density-dependent transmission scenario, the rate of change in the number or density of infected individual can be implemented as follows:\n\\[\\mathrm{d}I = \\beta^* SI - \\gamma I\\].\nIn the frequency-dependent scenario, arguably more widely adopted one, the rate of change in the number or density of infected individual can be implemented as follows:\n\\[\\mathrm{d}I = \\beta SI/N - \\gamma I\\] Two formulations have different implications. The most notable difference is that there is a threshold density, \\(N_T\\) in the density-dependent formulation whereas the second model leads to the basic reproduction ratio, \\(R_0\\), is derived from frequency-dependent. Of course, it is straightforward to interchange between the two types of models.\nBoth \\(\\beta^*\\) and \\(\\beta\\) are transmission coefficients but their unit are different. In the frequency-dependent formulation, \\(\\beta\\) can mean the number of transmissions per unit of time for an infected individual when it contacts with susceptible individuals. Therefore, the whole term, \\(\\beta I S/N\\), could mean that the number of newly infected individuals per unit of time. For density-dependent formulation, \\(\\beta^*\\) can mean the number of transmissions per unit of density per unit of time for an infected individual. The whole term \\(\\beta^* I S\\) can then mean the density of newly infected individuals per unit of time.\nNow let’s implement a simple SIR model in two different formulations.\n\nDensity-dependent transmission\n\nsir_den &lt;- function(t, u, p){\n  new_Istar = p[[\"beta_star\"]]*u[1]*u[2]\n  du1 &lt;- - new_Istar\n  du2 &lt;- + new_Istar - p[[\"gamma\"]]*u[2]\n  du3 &lt;- + p[[\"gamma\"]]*u[2]\n    \n  return(list(c(du1,du2,du3))) \n}\n\nlibrary(deSolve)\npop = 1000\nI0 = 10\nu0 &lt;- c(pop - I0, I0, 0)\ntspan &lt;- seq(from=0, to=50)\np &lt;- c(beta_star=0.0006, gamma=0.2)\n\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=sir_den, parms=p))\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(outdf,aes(x=time))+\n  geom_line(aes(y=`1`, color=\"S\")) +\n  geom_line(aes(y=`2`, color=\"I\")) +\n  geom_line(aes(y=`3`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n# threshold host population size\nNt = p[\"gamma\"] / p[\"beta_star\"]\n# R0 can be computed through N/Nt\nR0 = pop/Nt\n\nfinal_epidemic_size &lt;- function(R0 = 2) {\n  y = function(x) x - 1 + exp(-R0*x)\n  final_size &lt;- uniroot(y, interval=c(1e-6,1-1e-6))$root\n\n  return(final_size)\n\n}\nround(pop*final_epidemic_size(R0=R0), digits=2)\n\n[1] 940.48\n\nround(tail(outdf, 1)$`3`, digits=2)\n\n[1] 939.32\n\n\n\n\nFrequency-dependent formulation\n\nsir_freq &lt;- function(t, u, p){\n  new_I = p[[\"beta\"]]*u[1]*u[2]/sum(u)\n  du1 &lt;- - new_I\n  du2 &lt;- + new_I - p[[\"gamma\"]]*u[2]\n  du3 &lt;- + p[[\"gamma\"]]*u[2]\n    \n  return(list(c(du1,du2,du3))) \n}\n\nlibrary(deSolve)\npop = 1000\nI0 = 10\nu0 &lt;- c(pop - I0, I0, 0)\ntspan &lt;- seq(from=0, to=50)\np &lt;- c(beta=0.6, gamma=0.2)\n\noutdf &lt;- as.data.frame(ode(y=u0, times=tspan, func=sir_freq, parms=p))\n\nggplot(outdf,aes(x=time))+\n  geom_line(aes(y=`1`, color=\"S\")) +\n  geom_line(aes(y=`2`, color=\"I\")) +\n  geom_line(aes(y=`3`, color=\"R\")) +\n  scale_color_manual(\"\",values=c(\"S\"=\"steelblue\",\"I\"=\"firebrick\",\n                              \"R\"=\"darkgreen\"))+\n  labs(y=\"Number of individuals\", x=\"Time\", color=\"\")\n\n\n\n# R0 threshold\n(R0 = p[[\"beta\"]] / p[[\"gamma\"]])\n\n[1] 3\n\nround(pop*final_epidemic_size(R0=R0), digits=2)\n\n[1] 940.48\n\nround(tail(outdf, 1)$`3`, digits=2)\n\n[1] 939.32"
  },
  {
    "objectID": "baking/how_GPT_works_in_python_1/index.html",
    "href": "baking/how_GPT_works_in_python_1/index.html",
    "title": "How GPT works 1: Add one word at a time",
    "section": "",
    "text": "스티븐 울프램 (Stephen Wolfram)이 쓴 챗GPT에 관한 블로그 What Is ChatGPT Doing … and Why Does It Work?를 읽고 난 후 ChatGPT에 더 흥미가 생겼다. 블로그 내용을 잘 이해하기 위해 한글로 번역하고 내 생각과 파이썬 코드를 더하여 블로그로 만들고자 하였다. 원본에 충실한 번역은 아니고 내 생각을 더해가면서 작성하였다. ChatGPT에 대한 기초 이해가 없어 아래 블로그와 논문을 참조하였다.\n울프램이 블로그에 썼듯이 ChatGPT가 하는 것은 한 마다로 말해 기존 문장의 끝에 한 번에 한 단어씩 더할 뿐이다. 단어 (word) 보다는 토큰 (token)이라는 용어가 정확한 말이다. 토큰은 단어, 문장 부호, 혹 단어의 일부가 될 수도 있다. 영어에 약 50,000개의 토큰이 있다고 한다. 편의상 이 글에서는 토큰 대신 단어를 사용하겠다.\n한 번에 하나씩 단어를 더할 뿐인데 사람이 작성한 것 같은 긴 글을 쓸 수 있는 것이 놀랍다. 주어진 문장의 다음에 50,000개의 단어 중에 어떤 것을 쓸지 ChatGPT는 어떻게 결정할까? 인터넷이나, 전자책 등을 이용해서 기존의 문장을 학습한 후 ChatGPT는 각 문장 (들)의 다음에 올 수 있는 단어의 빈도수를 기반으로 50000개의 단어 각각에 확률을 부어하고 그 확률에 기반하여 다음 단어를 선택한다. 다만 확률이 제일 높은 단어만을 고르는 것도 아니고 전적으로 확률에 기반하여 고르는 것도 아니다. 여러가지 방법이 있고 확률이 낮은 단어들도 사용되는 빈도를 결정하는 모수가 소위 temperature라고 불리는 모수이다. 이 모수는 0과 1사이의 값을 갖는데 temperature가 0이면 확률이 제일 높은 단어만을 고르게 되고 0.8정도의 값일때 사람이 작성한 문장과 가정 비슷한 문장을 만들어 낸다고 한다.\n아래 파이썬 코드로 특정한 문장 이후에 올 단어의 확률을 알아보자. 울프램의 블로그에서처럼 GPT2를 사용하였다. GPT2는 개인용 컴퓨터에서 구동하는 데도 문제가 없다.\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Initialize the model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\n\nDownloading:   0%|          | 0.00/26.0 [00:00&lt;?, ?B/s]\nDownloading: 100%|##########| 26.0/26.0 [00:00&lt;00:00, 12.4kB/s]\n\nDownloading:   0%|          | 0.00/665 [00:00&lt;?, ?B/s]\nDownloading: 100%|##########| 665/665 [00:00&lt;00:00, 344kB/s]\n\nDownloading:   0%|          | 0.00/1.04M [00:00&lt;?, ?B/s]\nDownloading:   3%|3         | 31.7k/1.04M [00:00&lt;00:05, 176kB/s]\nDownloading:  10%|9         | 100k/1.04M [00:00&lt;00:03, 292kB/s] \nDownloading:  22%|##2       | 232k/1.04M [00:00&lt;00:01, 485kB/s]\nDownloading:  50%|#####     | 526k/1.04M [00:00&lt;00:00, 920kB/s]\nDownloading: 100%|##########| 1.04M/1.04M [00:00&lt;00:00, 1.38MB/s]\n\nDownloading:   0%|          | 0.00/456k [00:00&lt;?, ?B/s]\nDownloading:   7%|7         | 33.8k/456k [00:00&lt;00:02, 192kB/s]\nDownloading:  22%|##1       | 99.3k/456k [00:00&lt;00:01, 290kB/s]\nDownloading:  54%|#####4    | 247k/456k [00:00&lt;00:00, 525kB/s] \nDownloading: 100%|##########| 456k/456k [00:00&lt;00:00, 818kB/s]\n\nDownloading:   0%|          | 0.00/1.36M [00:00&lt;?, ?B/s]\nDownloading:   2%|2         | 33.8k/1.36M [00:00&lt;00:06, 195kB/s]\nDownloading:   7%|7         | 99.3k/1.36M [00:00&lt;00:04, 290kB/s]\nDownloading:  18%|#8        | 247k/1.36M [00:00&lt;00:02, 523kB/s] \nDownloading:  39%|###8      | 525k/1.36M [00:00&lt;00:00, 913kB/s]\nDownloading:  81%|########1 | 1.10M/1.36M [00:00&lt;00:00, 1.71MB/s]\nDownloading: 100%|##########| 1.36M/1.36M [00:00&lt;00:00, 1.45MB/s]\n\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n\n\nDownloading:   0%|          | 0.00/548M [00:00&lt;?, ?B/s]\nDownloading:   0%|          | 2.23M/548M [00:00&lt;00:24, 22.2MB/s]\nDownloading:   1%|          | 4.73M/548M [00:00&lt;00:22, 23.8MB/s]\nDownloading:   1%|1         | 7.11M/548M [00:00&lt;00:24, 22.2MB/s]\nDownloading:   2%|1         | 9.72M/548M [00:00&lt;00:22, 23.6MB/s]\nDownloading:   2%|2         | 12.2M/548M [00:00&lt;00:22, 24.0MB/s]\nDownloading:   3%|2         | 14.6M/548M [00:00&lt;00:22, 23.5MB/s]\nDownloading:   3%|3         | 17.0M/548M [00:00&lt;00:23, 23.1MB/s]\nDownloading:   4%|3         | 19.3M/548M [00:00&lt;00:23, 22.8MB/s]\nDownloading:   4%|3         | 21.7M/548M [00:00&lt;00:22, 23.1MB/s]\nDownloading:   4%|4         | 24.2M/548M [00:01&lt;00:22, 23.7MB/s]\nDownloading:   5%|4         | 26.6M/548M [00:01&lt;00:22, 23.3MB/s]\nDownloading:   5%|5         | 28.9M/548M [00:01&lt;00:22, 23.4MB/s]\nDownloading:   6%|5         | 31.4M/548M [00:01&lt;00:21, 23.8MB/s]\nDownloading:   6%|6         | 33.9M/548M [00:01&lt;00:21, 23.9MB/s]\nDownloading:   7%|6         | 36.3M/548M [00:01&lt;00:22, 22.5MB/s]\nDownloading:   7%|7         | 38.5M/548M [00:01&lt;00:23, 21.8MB/s]\nDownloading:   7%|7         | 40.7M/548M [00:01&lt;00:23, 21.2MB/s]\nDownloading:   8%|7         | 42.9M/548M [00:01&lt;00:25, 20.1MB/s]\nDownloading:   8%|8         | 44.9M/548M [00:02&lt;00:25, 20.0MB/s]\nDownloading:   9%|8         | 46.9M/548M [00:02&lt;00:24, 20.1MB/s]\nDownloading:   9%|9         | 49.4M/548M [00:02&lt;00:23, 21.2MB/s]\nDownloading:   9%|9         | 51.6M/548M [00:02&lt;00:23, 21.5MB/s]\nDownloading:  10%|9         | 53.9M/548M [00:02&lt;00:22, 22.1MB/s]\nDownloading:  10%|#         | 56.9M/548M [00:02&lt;00:20, 24.3MB/s]\nDownloading:  11%|#         | 59.7M/548M [00:02&lt;00:19, 25.3MB/s]\nDownloading:  11%|#1        | 62.2M/548M [00:02&lt;00:19, 25.1MB/s]\nDownloading:  12%|#1        | 64.8M/548M [00:02&lt;00:19, 25.3MB/s]\nDownloading:  12%|#2        | 67.4M/548M [00:02&lt;00:19, 25.2MB/s]\nDownloading:  13%|#2        | 69.9M/548M [00:03&lt;00:19, 25.1MB/s]\nDownloading:  13%|#3        | 72.5M/548M [00:03&lt;00:18, 25.3MB/s]\nDownloading:  14%|#3        | 75.0M/548M [00:03&lt;00:18, 25.2MB/s]\nDownloading:  14%|#4        | 77.8M/548M [00:03&lt;00:18, 25.9MB/s]\nDownloading:  15%|#4        | 80.4M/548M [00:03&lt;00:18, 25.3MB/s]\nDownloading:  15%|#5        | 82.9M/548M [00:03&lt;00:18, 25.4MB/s]\nDownloading:  16%|#5        | 85.5M/548M [00:03&lt;00:18, 24.4MB/s]\nDownloading:  16%|#6        | 87.9M/548M [00:03&lt;00:19, 23.4MB/s]\nDownloading:  16%|#6        | 90.3M/548M [00:03&lt;00:21, 21.6MB/s]\nDownloading:  17%|#6        | 92.5M/548M [00:03&lt;00:21, 21.4MB/s]\nDownloading:  17%|#7        | 94.7M/548M [00:04&lt;00:20, 21.7MB/s]\nDownloading:  18%|#7        | 97.0M/548M [00:04&lt;00:20, 21.9MB/s]\nDownloading:  18%|#8        | 99.6M/548M [00:04&lt;00:19, 23.1MB/s]\nDownloading:  19%|#8        | 102M/548M [00:04&lt;00:18, 23.9MB/s] \nDownloading:  19%|#9        | 105M/548M [00:04&lt;00:18, 24.1MB/s]\nDownloading:  20%|#9        | 107M/548M [00:04&lt;00:17, 24.6MB/s]\nDownloading:  20%|##        | 110M/548M [00:04&lt;00:17, 25.5MB/s]\nDownloading:  21%|##        | 113M/548M [00:04&lt;00:17, 25.4MB/s]\nDownloading:  21%|##1       | 115M/548M [00:04&lt;00:17, 25.4MB/s]\nDownloading:  21%|##1       | 118M/548M [00:05&lt;00:16, 25.6MB/s]\nDownloading:  22%|##1       | 120M/548M [00:05&lt;00:17, 24.2MB/s]\nDownloading:  22%|##2       | 123M/548M [00:05&lt;00:17, 25.0MB/s]\nDownloading:  23%|##2       | 126M/548M [00:05&lt;00:17, 24.5MB/s]\nDownloading:  23%|##3       | 128M/548M [00:05&lt;00:17, 23.9MB/s]\nDownloading:  24%|##3       | 131M/548M [00:05&lt;00:16, 24.6MB/s]\nDownloading:  24%|##4       | 133M/548M [00:05&lt;00:16, 24.5MB/s]\nDownloading:  25%|##4       | 136M/548M [00:05&lt;00:16, 24.8MB/s]\nDownloading:  25%|##5       | 138M/548M [00:05&lt;00:16, 24.7MB/s]\nDownloading:  26%|##5       | 141M/548M [00:05&lt;00:16, 24.9MB/s]\nDownloading:  26%|##6       | 143M/548M [00:06&lt;00:16, 24.8MB/s]\nDownloading:  27%|##6       | 146M/548M [00:06&lt;00:16, 24.8MB/s]\nDownloading:  27%|##7       | 148M/548M [00:06&lt;00:15, 25.0MB/s]\nDownloading:  28%|##7       | 151M/548M [00:06&lt;00:15, 25.6MB/s]\nDownloading:  28%|##8       | 154M/548M [00:06&lt;00:16, 23.4MB/s]\nDownloading:  28%|##8       | 156M/548M [00:06&lt;00:17, 22.7MB/s]\nDownloading:  29%|##8       | 159M/548M [00:06&lt;00:16, 23.5MB/s]\nDownloading:  29%|##9       | 161M/548M [00:06&lt;00:16, 24.1MB/s]\nDownloading:  30%|##9       | 164M/548M [00:06&lt;00:16, 23.9MB/s]\nDownloading:  30%|###       | 166M/548M [00:06&lt;00:15, 24.9MB/s]\nDownloading:  31%|###       | 170M/548M [00:07&lt;00:13, 27.3MB/s]\nDownloading:  31%|###1      | 173M/548M [00:07&lt;00:13, 28.1MB/s]\nDownloading:  32%|###2      | 175M/548M [00:07&lt;00:13, 28.0MB/s]\nDownloading:  33%|###2      | 178M/548M [00:07&lt;00:13, 26.9MB/s]\nDownloading:  33%|###3      | 181M/548M [00:07&lt;00:13, 26.6MB/s]\nDownloading:  34%|###3      | 184M/548M [00:07&lt;00:14, 24.8MB/s]\nDownloading:  34%|###3      | 186M/548M [00:07&lt;00:15, 23.4MB/s]\nDownloading:  34%|###4      | 189M/548M [00:07&lt;00:14, 24.5MB/s]\nDownloading:  35%|###4      | 191M/548M [00:07&lt;00:14, 24.5MB/s]\nDownloading:  35%|###5      | 194M/548M [00:08&lt;00:14, 24.7MB/s]\nDownloading:  36%|###5      | 197M/548M [00:08&lt;00:13, 26.2MB/s]\nDownloading:  36%|###6      | 200M/548M [00:08&lt;00:13, 26.2MB/s]\nDownloading:  37%|###6      | 202M/548M [00:08&lt;00:12, 27.0MB/s]\nDownloading:  37%|###7      | 205M/548M [00:08&lt;00:13, 25.8MB/s]\nDownloading:  38%|###8      | 209M/548M [00:08&lt;00:12, 28.2MB/s]\nDownloading:  39%|###8      | 211M/548M [00:08&lt;00:12, 27.6MB/s]\nDownloading:  39%|###9      | 215M/548M [00:08&lt;00:11, 29.6MB/s]\nDownloading:  40%|###9      | 218M/548M [00:08&lt;00:10, 30.4MB/s]\nDownloading:  40%|####      | 221M/548M [00:08&lt;00:10, 30.5MB/s]\nDownloading:  41%|####      | 224M/548M [00:09&lt;00:10, 29.5MB/s]\nDownloading:  42%|####1     | 228M/548M [00:09&lt;00:10, 30.9MB/s]\nDownloading:  42%|####2     | 231M/548M [00:09&lt;00:10, 30.1MB/s]\nDownloading:  43%|####2     | 234M/548M [00:09&lt;00:11, 28.2MB/s]\nDownloading:  43%|####3     | 237M/548M [00:09&lt;00:11, 27.2MB/s]\nDownloading:  44%|####3     | 240M/548M [00:09&lt;00:11, 26.6MB/s]\nDownloading:  44%|####4     | 242M/548M [00:09&lt;00:12, 25.3MB/s]\nDownloading:  45%|####4     | 245M/548M [00:09&lt;00:12, 24.4MB/s]\nDownloading:  45%|####5     | 247M/548M [00:10&lt;00:12, 23.2MB/s]\nDownloading:  46%|####5     | 250M/548M [00:10&lt;00:18, 16.3MB/s]\nDownloading:  46%|####5     | 252M/548M [00:10&lt;00:17, 17.4MB/s]\nDownloading:  46%|####6     | 254M/548M [00:10&lt;00:15, 18.6MB/s]\nDownloading:  47%|####6     | 256M/548M [00:10&lt;00:15, 18.4MB/s]\nDownloading:  47%|####7     | 258M/548M [00:10&lt;00:15, 19.3MB/s]\nDownloading:  48%|####7     | 261M/548M [00:10&lt;00:13, 20.6MB/s]\nDownloading:  48%|####8     | 263M/548M [00:10&lt;00:12, 22.1MB/s]\nDownloading:  48%|####8     | 266M/548M [00:11&lt;00:13, 21.6MB/s]\nDownloading:  49%|####8     | 268M/548M [00:11&lt;00:12, 21.6MB/s]\nDownloading:  49%|####9     | 270M/548M [00:11&lt;00:12, 21.5MB/s]\nDownloading:  50%|####9     | 272M/548M [00:11&lt;00:13, 21.1MB/s]\nDownloading:  50%|#####     | 274M/548M [00:11&lt;00:13, 20.5MB/s]\nDownloading:  50%|#####     | 277M/548M [00:11&lt;00:12, 21.5MB/s]\nDownloading:  51%|#####     | 279M/548M [00:11&lt;00:12, 22.3MB/s]\nDownloading:  51%|#####1    | 282M/548M [00:11&lt;00:11, 23.1MB/s]\nDownloading:  52%|#####1    | 284M/548M [00:11&lt;00:11, 23.2MB/s]\nDownloading:  52%|#####2    | 286M/548M [00:11&lt;00:11, 22.2MB/s]\nDownloading:  53%|#####2    | 289M/548M [00:12&lt;00:12, 20.5MB/s]\nDownloading:  53%|#####3    | 291M/548M [00:12&lt;00:12, 20.1MB/s]\nDownloading:  53%|#####3    | 293M/548M [00:12&lt;00:12, 19.8MB/s]\nDownloading:  54%|#####3    | 295M/548M [00:12&lt;00:13, 19.1MB/s]\nDownloading:  54%|#####4    | 297M/548M [00:12&lt;00:13, 18.4MB/s]\nDownloading:  55%|#####4    | 299M/548M [00:12&lt;00:12, 19.9MB/s]\nDownloading:  55%|#####4    | 301M/548M [00:12&lt;00:12, 20.1MB/s]\nDownloading:  55%|#####5    | 303M/548M [00:12&lt;00:11, 21.3MB/s]\nDownloading:  56%|#####5    | 306M/548M [00:12&lt;00:11, 21.0MB/s]\nDownloading:  56%|#####6    | 308M/548M [00:13&lt;00:10, 22.0MB/s]\nDownloading:  57%|#####6    | 310M/548M [00:13&lt;00:11, 21.1MB/s]\nDownloading:  57%|#####7    | 313M/548M [00:13&lt;00:10, 22.3MB/s]\nDownloading:  58%|#####7    | 315M/548M [00:13&lt;00:10, 22.8MB/s]\nDownloading:  58%|#####7    | 318M/548M [00:13&lt;00:10, 22.7MB/s]\nDownloading:  58%|#####8    | 320M/548M [00:13&lt;00:10, 21.1MB/s]\nDownloading:  59%|#####8    | 322M/548M [00:13&lt;00:10, 22.1MB/s]\nDownloading:  59%|#####9    | 325M/548M [00:13&lt;00:10, 21.7MB/s]\nDownloading:  60%|#####9    | 327M/548M [00:13&lt;00:10, 22.0MB/s]\nDownloading:  60%|######    | 329M/548M [00:13&lt;00:10, 21.1MB/s]\nDownloading:  60%|######    | 331M/548M [00:14&lt;00:10, 19.8MB/s]\nDownloading:  61%|######    | 333M/548M [00:14&lt;00:11, 19.1MB/s]\nDownloading:  61%|######1   | 335M/548M [00:14&lt;00:10, 19.5MB/s]\nDownloading:  62%|######1   | 338M/548M [00:14&lt;00:10, 20.4MB/s]\nDownloading:  62%|######1   | 340M/548M [00:14&lt;00:10, 20.0MB/s]\nDownloading:  62%|######2   | 342M/548M [00:14&lt;00:10, 20.0MB/s]\nDownloading:  63%|######2   | 344M/548M [00:14&lt;00:10, 20.0MB/s]\nDownloading:  63%|######3   | 346M/548M [00:14&lt;00:09, 20.3MB/s]\nDownloading:  64%|######3   | 348M/548M [00:14&lt;00:09, 22.0MB/s]\nDownloading:  64%|######3   | 351M/548M [00:15&lt;00:08, 22.0MB/s]\nDownloading:  64%|######4   | 353M/548M [00:15&lt;00:08, 22.8MB/s]\nDownloading:  65%|######4   | 355M/548M [00:15&lt;00:08, 22.2MB/s]\nDownloading:  65%|######5   | 358M/548M [00:15&lt;00:08, 22.5MB/s]\nDownloading:  66%|######5   | 360M/548M [00:15&lt;00:08, 21.8MB/s]\nDownloading:  66%|######6   | 363M/548M [00:15&lt;00:07, 23.5MB/s]\nDownloading:  67%|######6   | 365M/548M [00:15&lt;00:08, 21.9MB/s]\nDownloading:  67%|######7   | 367M/548M [00:15&lt;00:08, 22.2MB/s]\nDownloading:  67%|######7   | 370M/548M [00:15&lt;00:08, 22.1MB/s]\nDownloading:  68%|######7   | 372M/548M [00:16&lt;00:08, 21.4MB/s]\nDownloading:  68%|######8   | 374M/548M [00:16&lt;00:08, 21.5MB/s]\nDownloading:  69%|######8   | 377M/548M [00:16&lt;00:07, 23.2MB/s]\nDownloading:  69%|######9   | 380M/548M [00:16&lt;00:06, 25.0MB/s]\nDownloading:  70%|######9   | 382M/548M [00:16&lt;00:07, 22.3MB/s]\nDownloading:  70%|#######   | 384M/548M [00:16&lt;00:07, 21.1MB/s]\nDownloading:  71%|#######   | 387M/548M [00:16&lt;00:07, 21.4MB/s]\nDownloading:  71%|#######1  | 389M/548M [00:16&lt;00:07, 22.6MB/s]\nDownloading:  72%|#######1  | 392M/548M [00:16&lt;00:06, 24.0MB/s]\nDownloading:  72%|#######2  | 395M/548M [00:16&lt;00:05, 27.0MB/s]\nDownloading:  73%|#######2  | 398M/548M [00:17&lt;00:05, 26.4MB/s]\nDownloading:  73%|#######3  | 402M/548M [00:17&lt;00:05, 28.6MB/s]\nDownloading:  74%|#######3  | 405M/548M [00:17&lt;00:04, 29.9MB/s]\nDownloading:  74%|#######4  | 408M/548M [00:17&lt;00:04, 29.6MB/s]\nDownloading:  75%|#######4  | 411M/548M [00:17&lt;00:04, 30.0MB/s]\nDownloading:  76%|#######5  | 415M/548M [00:17&lt;00:04, 32.1MB/s]\nDownloading:  76%|#######6  | 419M/548M [00:17&lt;00:03, 35.6MB/s]\nDownloading:  77%|#######7  | 424M/548M [00:17&lt;00:03, 38.4MB/s]\nDownloading:  78%|#######8  | 428M/548M [00:17&lt;00:03, 39.5MB/s]\nDownloading:  79%|#######8  | 432M/548M [00:17&lt;00:02, 40.3MB/s]\nDownloading:  80%|#######9  | 437M/548M [00:18&lt;00:02, 41.5MB/s]\nDownloading:  81%|########  | 441M/548M [00:18&lt;00:02, 42.9MB/s]\nDownloading:  81%|########1 | 446M/548M [00:18&lt;00:02, 41.0MB/s]\nDownloading:  82%|########2 | 450M/548M [00:18&lt;00:02, 40.9MB/s]\nDownloading:  83%|########2 | 454M/548M [00:18&lt;00:02, 40.2MB/s]\nDownloading:  84%|########3 | 458M/548M [00:18&lt;00:02, 35.3MB/s]\nDownloading:  84%|########4 | 462M/548M [00:18&lt;00:02, 29.9MB/s]\nDownloading:  85%|########4 | 465M/548M [00:18&lt;00:02, 28.5MB/s]\nDownloading:  85%|########5 | 468M/548M [00:19&lt;00:03, 25.5MB/s]\nDownloading:  86%|########5 | 470M/548M [00:19&lt;00:03, 23.5MB/s]\nDownloading:  86%|########6 | 473M/548M [00:19&lt;00:03, 22.7MB/s]\nDownloading:  87%|########6 | 475M/548M [00:19&lt;00:03, 22.3MB/s]\nDownloading:  87%|########7 | 477M/548M [00:19&lt;00:03, 22.5MB/s]\nDownloading:  88%|########7 | 480M/548M [00:19&lt;00:03, 21.8MB/s]\nDownloading:  88%|########7 | 482M/548M [00:19&lt;00:03, 21.7MB/s]\nDownloading:  88%|########8 | 484M/548M [00:19&lt;00:02, 22.5MB/s]\nDownloading:  89%|########8 | 487M/548M [00:20&lt;00:02, 23.4MB/s]\nDownloading:  89%|########9 | 490M/548M [00:20&lt;00:02, 24.0MB/s]\nDownloading:  90%|########9 | 492M/548M [00:20&lt;00:02, 24.2MB/s]\nDownloading:  90%|######### | 494M/548M [00:20&lt;00:03, 15.5MB/s]\nDownloading:  91%|######### | 497M/548M [00:20&lt;00:03, 16.6MB/s]\nDownloading:  91%|######### | 499M/548M [00:20&lt;00:02, 17.4MB/s]\nDownloading:  91%|#########1| 501M/548M [00:20&lt;00:02, 18.1MB/s]\nDownloading:  92%|#########1| 503M/548M [00:20&lt;00:02, 18.5MB/s]\nDownloading:  92%|#########2| 505M/548M [00:21&lt;00:02, 19.5MB/s]\nDownloading:  92%|#########2| 507M/548M [00:21&lt;00:02, 19.3MB/s]\nDownloading:  93%|#########2| 509M/548M [00:21&lt;00:01, 20.2MB/s]\nDownloading:  93%|#########3| 512M/548M [00:21&lt;00:01, 21.9MB/s]\nDownloading:  94%|#########3| 515M/548M [00:21&lt;00:01, 23.6MB/s]\nDownloading:  94%|#########4| 518M/548M [00:21&lt;00:01, 25.2MB/s]\nDownloading:  95%|#########4| 520M/548M [00:21&lt;00:01, 26.2MB/s]\nDownloading:  95%|#########5| 523M/548M [00:21&lt;00:00, 25.6MB/s]\nDownloading:  96%|#########5| 526M/548M [00:21&lt;00:00, 25.6MB/s]\nDownloading:  96%|#########6| 528M/548M [00:21&lt;00:00, 25.0MB/s]\nDownloading:  97%|#########6| 531M/548M [00:22&lt;00:00, 25.3MB/s]\nDownloading:  97%|#########7| 534M/548M [00:22&lt;00:00, 25.9MB/s]\nDownloading:  98%|#########7| 537M/548M [00:22&lt;00:00, 27.3MB/s]\nDownloading:  98%|#########8| 540M/548M [00:22&lt;00:00, 28.5MB/s]\nDownloading:  99%|#########9| 543M/548M [00:22&lt;00:00, 30.4MB/s]\nDownloading: 100%|##########| 548M/548M [00:22&lt;00:00, 24.3MB/s]\n\n# Move model to the appropriate device\ntorch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel.to(torch_device)\n\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)\n\n# Encode the input text\ninputs = tokenizer(\"I enjoy walking with my cute dog\", return_tensors=\"pt\")\ninputs = inputs.to(torch_device)\n\n# Get model output (logits)\nwith torch.no_grad():\n    outputs = model(**inputs)\n    logits = outputs.logits\n\n# Select logits of the last token and get the top 10 tokens\nlast_token_logits = logits[0, -1, :]\n# the number of tokens\nlen(last_token_logits)\n\n50257\n\n# Convert logits to probabilities\nprobs = torch.softmax(last_token_logits, dim=0)\n\n# plot the  probability distribution\nimport matplotlib.pyplot as plt\n\nplt.hist(probs, bins=500)\n# plt.xscale('log', base=10)\n# plt.yscale('log', base=10)\nplt.show()"
  },
  {
    "objectID": "posts/exp_erlang_lognormal/index.html",
    "href": "posts/exp_erlang_lognormal/index.html",
    "title": "Implementing incubation period of cholera in the ODE model",
    "section": "",
    "text": "Modeling incubation period in the ODE model\nIn the realm of infectious disease modeling, accurately simulating the incubation period–the time between exposure to a pathogen and the onset of symptoms–is crucial for understanding and predicting the spread of diseases. Classic ordinary differential equation (ODE) models often employ the exponential distribution to represent this incubation period mostly for the sake of convenience. However, the exponential distribution is not an optimal distribution to model the waiting time for the incubation period, which is often modeled as a log-normal distribution as shown in the study by Lessler et al.. In this post, I show that waiting time may be modeled more realistically by adding an extra compartment (e.g., \\(SE_1E_2IR\\) rather than \\(SEIR\\)) and thus making the waiting time Erlang-distributed.\nIn particular, in the following R codes, I explored the incubation period of cholera from the study by Azman et al.. Azman writes: We estimate the median incubation period of toxigenic cholera to be 1.4 days (95% Credible Interval (CI), 1.3-1.6) with a dispersion of 1.98 (95% CI 1.87-2.11). Five percent of cholera cases will develop symptoms by 0.5 days (95% CI 0.4-0.5), and 95% by 4.4 days (95% CI 3.9-5.0) after infection.\nBased on the description, I set the parameters for the log-normal distribution as follows.\n\na = 1.40 # median\nb = 1.98 # disperson\nround(qlnorm(c(0.05,0.95), meanlog = log(a), sdlog = log(b)), digits=1)\n\n[1] 0.5 4.3\n\n\nThere is a slight mismatch for the 95th percentile (4.3 vs 4.4 [reported]). This is, however, possible considering that the estimates are from Bayesian posterior samples and I can’t account for the correlation between the parameter estimates.\nUsing this log-normal distribution, I generate samples (\\(n=1000\\)), to which exponential and Erlang distributions are fit.\n\ndat = rlnorm(1e3, meanlog = log(a), sdlog = log(b))\n\nfit_exp = fitdistrplus::fitdist(dat, distr=\"exp\")\nfit_gam = fitdistrplus::fitdist(dat, distr=\"gamma\")\n\nround(qexp(c(0.05, 0.95), rate=fit_exp$estimate[[1]]), digits=1)\n\n[1] 0.1 5.1\n\nround(qgamma(c(0.05, 0.95), shape=fit_gam$estimate[[1]], rate=fit_gam$estimate[[2]]), digits=1)\n\n[1] 0.4 3.9\n\n\nBy examining \\(5^{th}\\) and \\(95^{th}\\) percentiles, we realize that the best-fit exponential distribution is too wide compared with the log-normal distribution whereas the Gamma distribution looks better.\nHowever, in the context of ODE models, Gamma distribution is not easy to implement except for a subset in which the shape parameter is an integer (i.e., Erlang distribution). Therefore, I fit the rate parameter of the Gamma distribution while the shape parameter is set to an integer. Based on the previous fitting results (shape parameter = 2.348677), I only test the cases where the shape parameter is 2 or 3.\n\nerlang_nll = function(p, shp=3){\n  - sum(dgamma(dat, shape=shp, rate=p, log=TRUE))\n}\ner_shp2 = optimize(erlang_nll, c(1e-6,10), shp=2)\ner_shp3 = optimize(erlang_nll, c(1e-6,10), shp=3)\n\nConsidering the negative log likelihood values, we realize that the Gamma distribution with shape of 2 provides a better fit than the one with shape of 3.\nWe plot the results as a visual summary.\n\nn = 1000 # the number of\nx = seq(0, 12, length.out=n)\ndf = data.frame(\n  val = rep(x, 4),\n  dist = rep(c(\"Log-normal\", \"Exponential\", \"Gamma\", \"Erlang (shape=2)\"), \n             each=n), \n  density = \n    c(dlnorm(x, meanlog=log(1.4), sdlog=log(1.98)), \n      dexp(x, rate=fit_exp$estimate[[1]]), \n      dgamma(x, shape=fit_gam$estimate[[1]], \n                 rate=fit_gam$estimate[[2]]), \n      dgamma(x, shape=2, rate=er_shp2$minimum)))\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(df)+\n  geom_line(aes(x=val, y=density, color=dist))+\n  labs(y=\"Density\", x=\"\", color=\"\")+\n  theme(legend.position = \"bottom\")\n\n\n\n\nThe above figure clearly shows that the Erlang distribution has a better representation of reality (in this case, the log-normal distributed incubation period) than the commonly used exponential distribution."
  },
  {
    "objectID": "posts/ICER/index.html",
    "href": "posts/ICER/index.html",
    "title": "Incremental Cost-Effectiveness Ratio (ICER)",
    "section": "",
    "text": "Understanding the Incremental Cost-Effectiveness Ratio (ICER) Through Cholera Vaccination\nThe Incremental Cost-Effectiveness Ratio (ICER) is a crucial metric in health economics, offering insights into the value of medical interventions by comparing their costs and effectiveness. Essentially, ICER is used to evaluate the cost-effectiveness of a new healthcare intervention compared to an existing standard of care. It is calculated as the difference in costs between two options divided by the difference in their effectiveness, typically measured in quality-adjusted life years (QALYs). This ratio helps policymakers and healthcare providers make informed decisions on allocating limited resources to maximize health benefits.\nConsider the example of cholera, a severe diarrhoeal disease caused by the _/Vibrio cholerae bacterium, and the use of the oral cholera vaccine (OCV). In regions where cholera is endemic, introducing or expanding the use of OCV can significantly reduce the incidence of the disease. To assess the cost-effectiveness of OCV, we can compare the ICER of vaccinating a population against cholera to the standard of care, which might include treatments like rehydration solutions or antibiotics for those already infected.\nLet’s explore how to calculate the ICER for introducing OCV in a hypothetical scenario using R. Assume we have data on the cost of vaccinating individuals and the effectiveness of the vaccine in preventing cholera, as well as the cost and effectiveness of the current standard cholera treatment.\n\n# Define the costs and effectiveness of the oral cholera vaccine (OCV) and standard treatment\ncost_OCV &lt;- 10000 # Total cost of vaccinating a population\neffectiveness_OCV &lt;- 0.5 # Reduction in cholera incidence due to vaccination\ncost_standard &lt;- 0 # Total cost of treating cholera without vaccination\neffectiveness_standard &lt;- 0 # Reduction in cholera incidence with standard treatment\n\n# Calculate the incremental cost and effectiveness\nincremental_cost &lt;- cost_OCV - cost_standard\nincremental_effectiveness &lt;- effectiveness_OCV - effectiveness_standard\n\n# Calculate the ICER\nICER &lt;- incremental_cost / incremental_effectiveness\n\nThis R code snippet outlines the basic calculation for ICER, where cost_OCV and effectiveness_OCV represent the cost and effectiveness of the OCV intervention, respectively, and cost_standard and effectiveness_standard represent these metrics for the standard treatment. By comparing these values, the ICER provides a dollar amount per percentage point increase in effectiveness, offering a straightforward metric for evaluating the cost-effectiveness of implementing the oral cholera vaccine in a specific population.\nIn summary, the ICER is a valuable tool in health economics, helping to guide decisions on the allocation of resources for interventions like the oral cholera vaccine. By quantifying the trade-offs between cost and effectiveness, ICER facilitates more informed, evidence-based choices in healthcare policy and practice."
  },
  {
    "objectID": "posts/vacc_waning_nls_erlang/index.html",
    "href": "posts/vacc_waning_nls_erlang/index.html",
    "title": "Modeling the waning of the vaccine-derived immunity in the ODE model",
    "section": "",
    "text": "The use of ordinary differential equation (ODE) models to simulate disease spread and evaluate vaccine impact is growing. Waning of vaccine-derived immunity often follows an exponential distribution in these models. However, as with the case with incubation period incubation period that we examined in the previous post, exponential distribution may not accurately depict the waning of vaccine-derived immunity.\nIn this post, I explore how to model the waning of vaccine efficacy over time using an ODE framework, utilizing the data from a study by Lee et al. Specifically, I examine how various cumulative distributions can be applied to acurately represent the diminishing percentage of of the vaccine protection over time, through the use of R code examples.\n\n# data from Table S4, Lee et al. (2020) Lancet Glob Health\ndat = data.frame(month = rep(seq(0,60,by=6),4),\n                 age = c(rep(\"Adult\", 11*2), rep(\"Kid\", 11*2)),\n                 type = rep(\"data\", 11*4),\n                 regimen = rep(c(rep(\"two doses\", 11), rep(\"one dose\", 11)),2),\n                 val = c(c(76,72,68,63,58,52,46,39,32,24,15)/100,\n                 c(76,72,68,0,0,0,0,0,0,0,0)/100,\n                 c(36,34,32,30,27,24,22,18,15,11,7)/100,\n                 c(36,34,32,0,0,0,0,0,0,0,0)/100))\n\nlibrary(dplyr)\n\nds = vector(\"list\", 4)\nds[[1]] = filter(dat, age==\"Adult\", type==\"data\", regimen==\"two doses\")\nds[[2]] = filter(dat, age==\"Kid\", type==\"data\", regimen==\"two doses\")\nds[[3]] = filter(dat, age==\"Adult\", type==\"data\", regimen==\"one dose\")\nds[[4]] = filter(dat, age==\"Kid\", type==\"data\", regimen==\"one dose\")\n\nfits_gamma_shape_2 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_2[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=2, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_3 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_3[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=3, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_4 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_4[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=4, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_5 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_5[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=5, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_8 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_8[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=8, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_exp &lt;- list()\n\nfor (i in 1:4) {\n fits_exp[[i]] &lt;- nls(val ~ \n        val[1]*pexp(month, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_gamma_shape_2[[1]], newdata = newd),\n                          predict(fits_gamma_shape_2[[2]], newdata = newd),\n                          predict(fits_gamma_shape_2[[3]], newdata = newd),\n                          predict(fits_gamma_shape_2[[4]], newdata = newd)))\n                 \nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age) +\n  ggtitle(\"Gamma decay with shape=2\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_gamma_shape_3[[1]], newdata = newd),\n                          predict(fits_gamma_shape_3[[2]], newdata = newd),\n                          predict(fits_gamma_shape_3[[3]], newdata = newd),\n                          predict(fits_gamma_shape_3[[4]], newdata = newd)))\n                 \nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age) +\n  ggtitle(\"Gamma decay with shape=3\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_exp[[1]], newdata = newd),\n                          predict(fits_exp[[2]], newdata = newd),\n                          predict(fits_exp[[3]], newdata = newd),\n                          predict(fits_exp[[4]], newdata = newd)))\n                 \nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age)+\n  ggtitle(\"Exponential decay\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\nGiven that the data for adults following a two-dose regimen seem to be the most reliable, we evaluated the model’s residual errors in comparison to this data. The analysis of residual errors indicates that a gamma distribution with a shape parameter of 3 presents the optimal fit.\n\nsummary(fits_exp[[1]])\n\n\nFormula: val ~ val[1] * pexp(month, rate = exp(lograte), lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -4.09710    0.09047  -45.29 6.63e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0643 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 6.629e-06\n\nsummary(fits_gamma_shape_2[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 2, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -3.18619    0.02946  -108.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03005 on 10 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 6.246e-06\n\nsummary(fits_gamma_shape_3[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 3, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.72304    0.02375  -114.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02893 on 10 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 1.595e-06\n\nsummary(fits_gamma_shape_4[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 4, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.41214    0.02884  -83.64 1.46e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0394 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 7.874e-07\n\nsummary(fits_gamma_shape_5[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 5, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.17801    0.03374  -64.56 1.94e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05009 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 9.615e-06\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(\"Adult\", 101*4),\n                 type = c(rep(\"Exponential\", 101),\n                          rep(\"Gamma w/ shape=2\", 101),\n                          rep(\"Gamma w/ shape=3\", 101),\n                          rep(\"Gamma w/ shape=4\", 101)),\n                 regimen = rep(\"two doses\", 101*4),\n                 val = c(predict(fits_exp[[1]], newdata = newd),\n                          predict(fits_gamma_shape_2[[1]], newdata = newd),\n                          predict(fits_gamma_shape_3[[1]], newdata = newd),\n                          predict(fits_gamma_shape_4[[1]], newdata = newd)))\n\nggplot() +\n  geom_point(data=filter(dat, age==\"Adult\", regimen==\"two doses\"), \n             aes(month, val, color=\"data\"), shape=19, size=3, alpha=0.3)+\n  geom_line(data=pred, aes(month, val, color=type)) +\n  ggtitle(\"Two-dose vaccine efficacy for adults\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")+\n  scale_colour_manual(\n    values=c(\"data\" = \"black\",  \"Exponential\" = \"firebrick\",\n             \"Gamma w/ shape=2\" = \"green4\",\"Gamma w/ shape=3\" = \"#6A3D9A\",\n              \"Gamma w/ shape=4\" = \"#FF7F00\"),\n    guide=guide_legend(override.aes = list(\n      linetype=c(\"data\"=\"blank\",\"Exponential\" = \"solid\",\n                          \"Gamma w/ shape=2\" = \"solid\",\n                          \"Gamma w/ shape=3\" = \"solid\",\n                          \"Gamma w/ shape=4\" = \"solid\"),\n      shape=c('data'=19,\"Exponential\" = NA,\n                          \"Gamma w/ shape=2\" = NA,\n                          \"Gamma w/ shape=3\" = NA,\n                          \"Gamma w/ shape=4\" = NA))))+ \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/vacc_waning_nls_erlang/index.html#modeling-the-waning-of-the-vaccine-derived-immunity-in-the-ode-model",
    "href": "posts/vacc_waning_nls_erlang/index.html#modeling-the-waning-of-the-vaccine-derived-immunity-in-the-ode-model",
    "title": "Modeling the waning of the vaccine-derived immunity in the ODE model",
    "section": "",
    "text": "The use of ordinary differential equation (ODE) models to simulate disease spread and evaluate vaccine impact is growing. Waning of vaccine-derived immunity often follows an exponential distribution in these models. However, as with the case with incubation period incubation period that we examined in the previous post, exponential distribution may not accurately depict the waning of vaccine-derived immunity.\nIn this post, I explore how to model the waning of vaccine efficacy over time using an ODE framework, utilizing the data from a study by Lee et al. Specifically, I examine how various cumulative distributions can be applied to acurately represent the diminishing percentage of of the vaccine protection over time, through the use of R code examples.\n\n# data from Table S4, Lee et al. (2020) Lancet Glob Health\ndat = data.frame(month = rep(seq(0,60,by=6),4),\n                 age = c(rep(\"Adult\", 11*2), rep(\"Kid\", 11*2)),\n                 type = rep(\"data\", 11*4),\n                 regimen = rep(c(rep(\"two doses\", 11), rep(\"one dose\", 11)),2),\n                 val = c(c(76,72,68,63,58,52,46,39,32,24,15)/100,\n                 c(76,72,68,0,0,0,0,0,0,0,0)/100,\n                 c(36,34,32,30,27,24,22,18,15,11,7)/100,\n                 c(36,34,32,0,0,0,0,0,0,0,0)/100))\n\nlibrary(dplyr)\n\nds = vector(\"list\", 4)\nds[[1]] = filter(dat, age==\"Adult\", type==\"data\", regimen==\"two doses\")\nds[[2]] = filter(dat, age==\"Kid\", type==\"data\", regimen==\"two doses\")\nds[[3]] = filter(dat, age==\"Adult\", type==\"data\", regimen==\"one dose\")\nds[[4]] = filter(dat, age==\"Kid\", type==\"data\", regimen==\"one dose\")\n\nfits_gamma_shape_2 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_2[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=2, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_3 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_3[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=3, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_4 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_4[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=4, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_5 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_5[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=5, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_gamma_shape_8 &lt;- list()\n\nfor (i in 1:4) {\n fits_gamma_shape_8[[i]] &lt;- nls(val ~ \n        val[1]*pgamma(month, shape=8, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\nfits_exp &lt;- list()\n\nfor (i in 1:4) {\n fits_exp[[i]] &lt;- nls(val ~ \n        val[1]*pexp(month, rate=exp(lograte),\n          lower.tail=FALSE), start=list(lograte=log(0.3)), data=ds[[i]])\n}\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_gamma_shape_2[[1]], newdata = newd),\n                          predict(fits_gamma_shape_2[[2]], newdata = newd),\n                          predict(fits_gamma_shape_2[[3]], newdata = newd),\n                          predict(fits_gamma_shape_2[[4]], newdata = newd)))\n                 \nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, axis_title_size=12))\n\nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age) +\n  ggtitle(\"Gamma decay with shape=2\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_gamma_shape_3[[1]], newdata = newd),\n                          predict(fits_gamma_shape_3[[2]], newdata = newd),\n                          predict(fits_gamma_shape_3[[3]], newdata = newd),\n                          predict(fits_gamma_shape_3[[4]], newdata = newd)))\n                 \nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age) +\n  ggtitle(\"Gamma decay with shape=3\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\n\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(c(rep(\"Adult\", 101), rep(\"Kid\", 101)), 2),\n                 type = rep(\"model\", 101*4),\n                 regimen = c(rep(\"two doses\", 101*2), rep(\"one dose\", 101*2)),\n                 val = c(predict(fits_exp[[1]], newdata = newd),\n                          predict(fits_exp[[2]], newdata = newd),\n                          predict(fits_exp[[3]], newdata = newd),\n                          predict(fits_exp[[4]], newdata = newd)))\n                 \nggplot(dat) +\n  geom_point(aes(month, val, color=regimen))+\n  geom_line(data=pred, aes(month, val, color=regimen)) +\n  facet_wrap(~age)+\n  ggtitle(\"Exponential decay\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")\n\n\n\n\nGiven that the data for adults following a two-dose regimen seem to be the most reliable, we evaluated the model’s residual errors in comparison to this data. The analysis of residual errors indicates that a gamma distribution with a shape parameter of 3 presents the optimal fit.\n\nsummary(fits_exp[[1]])\n\n\nFormula: val ~ val[1] * pexp(month, rate = exp(lograte), lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -4.09710    0.09047  -45.29 6.63e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0643 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 6.629e-06\n\nsummary(fits_gamma_shape_2[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 2, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -3.18619    0.02946  -108.2   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.03005 on 10 degrees of freedom\n\nNumber of iterations to convergence: 5 \nAchieved convergence tolerance: 6.246e-06\n\nsummary(fits_gamma_shape_3[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 3, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.72304    0.02375  -114.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02893 on 10 degrees of freedom\n\nNumber of iterations to convergence: 4 \nAchieved convergence tolerance: 1.595e-06\n\nsummary(fits_gamma_shape_4[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 4, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.41214    0.02884  -83.64 1.46e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.0394 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 7.874e-07\n\nsummary(fits_gamma_shape_5[[1]])\n\n\nFormula: val ~ val[1] * pgamma(month, shape = 5, rate = exp(lograte), \n    lower.tail = FALSE)\n\nParameters:\n        Estimate Std. Error t value Pr(&gt;|t|)    \nlograte -2.17801    0.03374  -64.56 1.94e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.05009 on 10 degrees of freedom\n\nNumber of iterations to convergence: 6 \nAchieved convergence tolerance: 9.615e-06\n\n\n\n# prediction\nmonths = 0:100\nnewd = data.frame(month = months)\npred = data.frame(month = rep(months, 4),\n                 age = rep(\"Adult\", 101*4),\n                 type = c(rep(\"Exponential\", 101),\n                          rep(\"Gamma w/ shape=2\", 101),\n                          rep(\"Gamma w/ shape=3\", 101),\n                          rep(\"Gamma w/ shape=4\", 101)),\n                 regimen = rep(\"two doses\", 101*4),\n                 val = c(predict(fits_exp[[1]], newdata = newd),\n                          predict(fits_gamma_shape_2[[1]], newdata = newd),\n                          predict(fits_gamma_shape_3[[1]], newdata = newd),\n                          predict(fits_gamma_shape_4[[1]], newdata = newd)))\n\nggplot() +\n  geom_point(data=filter(dat, age==\"Adult\", regimen==\"two doses\"), \n             aes(month, val, color=\"data\"), shape=19, size=3, alpha=0.3)+\n  geom_line(data=pred, aes(month, val, color=type)) +\n  ggtitle(\"Two-dose vaccine efficacy for adults\") +\n  labs(y=\"Vaccine efficacy\", x=\"Months\", color=\"\")+\n  scale_colour_manual(\n    values=c(\"data\" = \"black\",  \"Exponential\" = \"firebrick\",\n             \"Gamma w/ shape=2\" = \"green4\",\"Gamma w/ shape=3\" = \"#6A3D9A\",\n              \"Gamma w/ shape=4\" = \"#FF7F00\"),\n    guide=guide_legend(override.aes = list(\n      linetype=c(\"data\"=\"blank\",\"Exponential\" = \"solid\",\n                          \"Gamma w/ shape=2\" = \"solid\",\n                          \"Gamma w/ shape=3\" = \"solid\",\n                          \"Gamma w/ shape=4\" = \"solid\"),\n      shape=c('data'=19,\"Exponential\" = NA,\n                          \"Gamma w/ shape=2\" = NA,\n                          \"Gamma w/ shape=3\" = NA,\n                          \"Gamma w/ shape=4\" = NA))))+ \n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/Probabily_outbreak/index.html",
    "href": "posts/Probabily_outbreak/index.html",
    "title": "\\(R_0\\) and probability of a large outbreak",
    "section": "",
    "text": "감염병 인류 라는 책을 재미있게 읽는 중이다. 136페이지에는 기초감염재생산지수와 대유행의 가능성에 대한 간단한 수식이 나온다. \\[\\text{대유행의 가능성} = 1 - \\frac{1}{R_0}\\]\n\\(R_0\\)는 기초감염재생산지수, 즉 한 사람의 감염자가 다른 모든 사람이 감수성자 (susceptible) 일때 감염시키는 평균 감염자수를 나타낸다. 내가 아는 한 위 수식은 매우 제한적인 경우에만 적용된다. 감염병 유입 시 대유행의 가능성에 대한 내용은 Niels G. Becker가 저술한 Modeling to Inform Infectious Disease Control의 제 2장에 자세하게 기술되어 있다.\n동일한 사람들로 이루어진 인구 집단에서 감염병이 퍼져나가는 현상을 가지치기 과정 (branching process)에 빗대어 설명할 수 있다. 한 사람이 평균적으로 \\(R_0\\) 명을 감염시킨다고 해보자. 하지만 \\(R_0\\)은 평균일뿐 실제 한 감염자의 후속 감염자수는 확률 분포를 가진다고 생각해볼 수 있다. 한 명의 감염자가 총 \\(j\\) 명의 후속 감염자를 만들어 내는 확률을 \\(P(X=j) = p_j\\)라고 하자. 그렇다면 위에서 언급했던 기초감염재생산지수는 아래와 같이 표현할 수 있다.\n\\[R_0 = \\sum_{k=0}^\\infty j p_j.\\]\n위의 확률분포와 가지치기 과정을 이용하여 대유행의 확률에 대해 계산해보자. 대유행의 가능성은 역으로 생각하는 것이 유리하다. 즉 대유행이 아닌 소규모의 감염으로 막을 내리는 확률, \\(\\theta\\), 을 구한 후 대유행의 확률은 \\(1-\\theta\\)로 구하는 것이다. 소규모의 감염이 일어나기 위해서는 유입된 초기 환자가 아무도 감염시키지 않거나 혹은 몇 명을 감염시켰다고 할지라도 후속 감염자들이 추가적으로 일으키는 감염이 소규모일때만 가능할 것이다. 즉 소규모 감염의 확률, \\(\\theta\\), 는 아래의 식을 만족한다.\n\\[\\theta =  p_0 + p_1 \\theta + p_2 \\theta^2 + ... =  \\sum_{j=0}^\\infty p_j \\theta^{j}\n\\text{ for } j=0,1,2,...\\] 후속 감염자수의 분포가 푸아송 분포 (Poisson distribution)를 따른다고 가정해 보자.\n\\[\\text{Prob}(X=j) = \\frac{R^j e^R}{j!}\\text{ for } j=0,1,2,...\\]\n이 경우 우변은 아래와 같이 나타내어질 수 있다.\n\\[\\sum_{j=0}^\\infty p_j \\theta^{j} = e^{-R_0 + R_0 \\theta}.\\]\n따라서 \\(\\theta\\)는 아래의 식을 계산하면 된다. 다만 해를 직접 구할 수는 없고 수치해석방법을 이용 해서 답을 구해야 한다.\n\\[\\theta = e^{-R_0 + R_0 \\theta}.\\]\n위에서 언급한 \\(\\text{대유행의 가능성} = 1 - \\frac{1}{R_0}\\)는 후속 감염자수의 분포가 기하분포 (geometric distribution)를 따를때 성립한다. 기하분포는 아래와 같이 표현되고 \\[\\text{Prob}(X=j) = (1-p)^j p\\]\n평균과 성공확률의 관게, \\(R = \\frac{1-p}{p}\\), 이용하여 다시 표현하면 아래와 같다.\n\\[\\text{Prob}(X=j) = (\\frac{R_0}{1+R_0})^j \\frac{1}{1+R_0}\\] 위에서와 동일하게 계산하면 아래와 같다. \\[\\sum_{j=0}^\\infty p_j \\theta^{j} = \\frac{1}{1 + R_0 - R_0 \\theta}\\] 따라서 아래의 식을 풀면 \\(\\theta\\)를 구할 수 있다.\n\\[\\theta = \\frac{1}{1 + R_0 - R_0 \\theta}\\]\n이 경우에는 직접 해를 구할 수 있다. \\[\\theta = \\frac{1}{R_0}\\].\n따라서 감염병 인류 책에서 언급된 것처럼 \\[\\text{대유행의 가능성} = 1 - \\frac{1}{R_0}.\\] 마지막으로 후속 감염자수가 이항분포 (negative binomial distribution)을 따른다고 가정해보자. 최근 연구들에서 빈번하게 언급되고 가장 현실에 가까운 가정인 듯 하다. 이항분포를 다양한 모수를 사용하여 표현할 수 있고 평균 \\(R_0\\)과 확산(dispersion; \\(k\\))를 이용하여 나타내면 아래와 같다.\n\\[\\text{Prob}(X=j) = \\frac{\\Gamma(k+j)}{j!\\Gamma(k)}\\left(\\frac{k}{k+R_0}\\right)^k\\left(\\frac{R_0}{k+R_0}\\right)^j \\text{ for } k=0,1,2,...\\] The probability of a minor outbreak, 위와 동일한 방법으로 계산하면 우변은 아래와 같다.\n\\[\\sum_{j=0}^\\infty p_j \\theta^{j} = \\left(\\frac{k}{k + R_0 - R_0 \\theta}\\right)^k\\] 따라서 \\[\\theta = \\left(\\frac{k}{k + R_0 - R_0 \\theta}\\right)^k\\]를 계산하여 소규모감염의 확률을 계산하고 \\(1-\\theta\\)를 계산하여 대규모유행 확률을 계산하면 된다.\nR 시물레이션을 통해서 세방법이 어떻게 다른 결과를 나타내는지 살펴보자\n\nprob_outbreak_pois = function(theta, R0){\n  theta - exp(-R0 + R0*theta) \n}\nprob_outbreak_geo = function(theta, R0){\n  1/R0 \n}\nprob_outbreak_nb = function(theta, R0, k){\n  theta - (k / (k + R0 - R0*theta))^k \n}\n\nsol1 &lt;- min(rootSolve::multiroot(prob_outbreak_pois, c(0, 1), R0=3)$root)\nsol2 &lt;- 1/3\nsol3 &lt;- min(rootSolve::multiroot(prob_outbreak_nb, c(0, 1), R0=3, k=20)$root)\n\nRs &lt;- seq(1.1, 10, length.out=100)\ntheta1 &lt;- sapply(Rs, function(x) \n  min(rootSolve::multiroot(prob_outbreak_pois, c(0, 1), R0=x)$root))\ntheta2 &lt;- sapply(Rs, function(x) 1/x)\ntheta3 &lt;- sapply(Rs, function(x) \n  min(rootSolve::multiroot(prob_outbreak_nb, c(0, 1), R0=x, k=20)$root))\ntheta4 &lt;- sapply(Rs, function(x) \n  min(rootSolve::multiroot(prob_outbreak_nb, c(0, 1), R0=x, k=0.2)$root))\n\ndf &lt;- data.frame(R0=rep(Rs,4), \n                 dist=rep(c(\"Pois\",\"Geom\",\"NB(k=20)\",\"NB(k=2)\"),each=100),\n                 prob_outbreak=c(1-theta1,1-theta2,1-theta3,1-theta4))\n\nlibrary(ggplot2)\nextrafont::loadfonts(\"win\", quiet=TRUE)\ntheme_set(hrbrthemes::theme_ipsum_rc(base_size=14, subtitle_size=16, \n                                     axis_title_size=12))\n\nggplot(df) +\n  geom_line(aes(R0, prob_outbreak, color=dist))+\n  ggtitle(expression(\"Probability of a large outbreak vs.\" ~italic(R)[0])) +\n  labs(y=\"Probability of a large outbreak\", x=expression(italic(R)[0]), color=\"\")"
  },
  {
    "objectID": "posts/infectious_disease_names/index.html",
    "href": "posts/infectious_disease_names/index.html",
    "title": "Origins of major human infectious diseases",
    "section": "",
    "text": "Major human infecious disease are believed to have arisen after agriculture revolutionOrigins of major human infectious diseases. By the way, this emergence of infectious diseases are one reason that agricultural revolution is callled one of the major mistakes of the human species by Diamond and\nby Bill Gates is a comprehensive and accessible guide on how to tackle the urgent issue of climate change. Gates begins by laying out the scope of the problem, explaining that the world needs to reduce its greenhouse gas emissions to zero to avoid a catastrophe. I’ve compiled important numbers from the book to understand the climate change issues.\n\n51 Billion Tonnes: This is the amount of greenhouse gases, measured in CO2 equivalent (\\(\\mathrm{CO_{2}e}\\)), that humanity adds to the atmosphere every year.\nZero: According to Bill Gates, we need to bring this number down to zero to avoid a climate disaster. And he thinks it is possible through our technological advances. Watch his TED talk, Innovating to zero!. He is a great speaker!\n\n\n\n\nPercentage\nItem\n\n\n\n\n27%\nHow we generate electricity\n\n\n31%\nHow we make things\n\n\n18%\nHow we grow our food\n\n\n16%\nHow we move around\n\n\n6%\nHow we keep warm or cool"
  }
]