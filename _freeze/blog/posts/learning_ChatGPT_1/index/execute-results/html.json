{
  "hash": "1787c540cffbe877ae34f0192f1a8d0b",
  "result": {
    "markdown": "---\ntitle: \"Learning ChatGPT 1: Probabilities for the next word\"\nauthor: \"Jong-Hoon Kim\"\ndate: \"2024-05-01\"\ncategories: [GPT-2, ChatGPT, Wolfram]\nimage: \"top5_prob.png\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\nInspired by the [blog post by Stephen Wolfram](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/) about the workings of the GPT-2 system, I decided to learn a bit about ChatGPT myself. Luckily, [GPT-2](https://github.com/AGPatriota/GPT-2-for-R) is now available for R. My first task is simply to learn to run the model and generate the probability table for the words that can follow the text, \"The best thing about AI is to be able to\". The following contents are mainly based on [the blog post by Stephen Wolfram](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/). Additional resources include OpenAI Github page for [gpt-2](https://github.com/openai/gpt-2) and [the paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf) describing GPT-2 paper.\n\nLoad the libraries and R implementation of GPT-2.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(tok)\nrequire(torch)\nsource('GPT.R')\n```\n:::\n\n\nCreate a tokenizer to process inputs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntok <- tok::tokenizer$from_pretrained(\"gpt2\")\n```\n:::\n\n\nLoad a GPT-2 model\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntorch::with_device(device = \"meta\",{\n  Model0 <- GPT(\n    block_size = 1024,\n    n_embd = 768,\n    N_Layers = 12,\n    nvoc = 50257,\n    Head = 12\n  )\n})\n```\n:::\n\n\nApply the model weights\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel0$load_state_dict(state_dict = torch_load(\"Model-weights.pt\"),\n                       .refer_to_state_dict = TRUE)\n```\n:::\n\n\nCreate a function to list up `top_k` words with their probabilities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntop_k_words <- function(model = NULL,\n                             device = NULL,\n                             raw_text = NULL, \n                             temperature = NULL, \n                             top_k = 10){\n  \n  idx = tok$encode(raw_text)$ids\n  paste0(\"Input text is \", raw_text)\n  idx = torch::torch_tensor(idx+1, dtype=torch::torch_int(), device=device0)\n  idx = torch::torch_unsqueeze(idx, 1)\n  idx_cond = idx\t\n  logits = model$eval()(idx_cond)\n  logits = logits[, min(idx$size(2),1024), ] / temperature\n  \n  logits = logits$topk(top_k)\n  probs = torch::nnf_softmax(logits[[1]],-1)\n  df <- data.frame(token = NA,\n                   probability=as.numeric(probs))\n  \n  for (i in 1:top_k) {\n    idx_next <- logits[[2]][,as.integer(i)]$unsqueeze(1)\n    token <- tok$decode(as.integer(idx_next$cpu()-1))\n    df$token[i] <- token\n  }\n  return(df)\n}\n```\n:::\n\n\nHave the model run on GPU\n\n\n::: {.cell}\n\n```{.r .cell-code}\nModel0 = if (torch::cuda_is_available()) Model0$cuda() else Model0$cpu()\ndevice0 = if (torch::cuda_is_available()) \"cuda\" else \"cpu\"\n```\n:::\n\n\nGenerate the results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1) # for reproducibility, GPT-2 output is random.\ndf <- top_k_words(model = Model0,\n                  device = device0,\n                  raw_text = \"The best thing about AI is its ability to\", \n                  temperature = 0.8,\n                  top_k = 5)\n\nnames(df) <- c(\"Token\",\"Probability\")\nknitr::kable(df)\n```\n\n::: {.cell-output-display}\n|Token      | Probability|\n|:----------|-----------:|\n|learn      |   0.2770396|\n|predict    |   0.2047849|\n|make       |   0.1826125|\n|understand |   0.1745481|\n|do         |   0.1610149|\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}