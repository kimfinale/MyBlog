{
  "hash": "723ef00362b7546fd258430fcf3b8cb6",
  "result": {
    "markdown": "---\ntitle: \"Learning ChatGPT 4: Universal Approximation Theorem\"\nauthor: \"Jong-Hoon Kim\"\ndate: \"2024-06-24\"\ncategories: [Universal Approximation Theorem, Neural network, Arbitrary-Width, Arbitrary-Depth] \nimage: \"UAT.png\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\nNeural networks can approximate any function according to the [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem). A more precise statement of the theorem is that neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision by the [e-book](http://neuralnetworksanddeeplearning.com/chap4.html) by Michael Nielsen. This e-book provides intuitive explanations about the proof. In what follows, I wrote what I was able to understand based on those materials and added a few R code chunks to supplement.\n\nFor a simple network with one hidden layer of two neurons, n1 and n2, what is being computed in a hidden neuron is $\\sigma(w x + b)$, where $\\sigma (z) \\equiv 1/(1+e^{-z})$ is the sigmoid function. And what happens in the output neuron is $w_{31} n1 + w_{32} n2 + b$. Here, $w_{31} \\text{and} w_{32}$ refer to weights that correspond to two neurons, n1 and n2, respectively.\n\nThe basic idea of the theorem is that weighted sum of outputs of neurons can approximate any functions. This can be visualized by studying a special case in which output from each neuron is close to a step function and weighted sum of a pair of step functions can create a bump of any height and width. With numerous those bumps, one can approximate any continuous function at any desired precision.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(igraph)\nplot(graph_from_literal(Input--+n1, Input--+n2, n1--+Output, n2--+Output))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nIn what follows, I will explain how a neural network can create a bump. Before that, let's get familiar with what is being computed in a single neuron. In the following codes, some combination of weight, $w$ , and bias, $b$, generate a typical sigmoid function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 1, 0.01)\nw <- 20\nb <- -5\nplot(x, 1/(1+exp(-(w*x+b))), type=\"l\", xlab=\"Input\", ylab=\"n1\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nWith high values for $w$, the output becomes close to a step function and the change point occurs at $-b/w$. For instance, let's look at the plot for $w=1000, b=-300$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 1, 0.01)\nw <- 1e3\nb <- -300\nplot(x, 1/(1+exp(-(w*x+b))), type=\"l\", xlab=\"Input\", ylab=\"n1\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nOne can use the following codes to have a feel for how the weight and bias change the value in the neuron.\n\n``` r\nlibrary(manipulate)\nmanipulate(\n  plot(x, 1/(1+exp(-(w*x+b))), type=\"l\", xlab=\"Input\", ylab=\"n1\"),\n  w = slider(-1e3,1e3, initial = 20), \n  b = slider(-1e3,1e3, initial = -5))\n```\n\nNow let's create another step function in which the change point occurs at 0.4\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(0, 1, 0.01)\nw1 <- 1000\nb1 <- -300\ny1 <- 1/(1+exp(-(w1*x+b1)))\nplot(x, y1, type=\"l\")\n\nw2 <- 1000\nb2 <- -400\ny2 <- 1 / (1+exp(-(w2*x+b2)))\nlines(x, y2, col=2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nWhat happens in the Output node is $\\omega_{31} n1 + \\omega_{32} n2 + b$. By setting $w1$ and $w2$ at different sign, we can create a bump.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nw31 <- 1        \nw32 <- -1       \nb3 <- 0\nplot(x, y2*w32 + y1*w31 + b3, type=\"l\", xlab=\"x\", ylab=\"Output\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nAgain, one may use the following codes to have a feel for how two weights and the bias change the output in the Output.\n\n``` r\nmanipulate(\n  plot(x, y2*w32 + y1*w31 + b3, type=\"l\", xlab=\"x\", ylab=\"Output\"),\n  w31 = slider(-1e2,1e2, initial = 1), \n  w32 = slider(-1e2,1e2, initial = -1),\n  b3 = slider(-1e2,1e2, initial = 0))\n```\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}