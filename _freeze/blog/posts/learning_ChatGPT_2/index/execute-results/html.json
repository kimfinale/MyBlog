{
  "hash": "0847cf240b3282272b8bfaf46abdf162",
  "result": {
    "markdown": "---\ntitle: \"Learning ChatGPT 2: Approximating a tower function using a neural net\"\nauthor: \"Jong-Hoon Kim\"\ndate: \"2024-05-28\"\ncategories: [Neural network, Adam, matrix]\nimage: \"stepfunc_nn.png\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\nThe [blog post by Stephen Wolfram](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/) discusses a neural network for approximating some sort of a step function. This post is my attempt to reproduce it. A [YouTube video](https://www.youtube.com/watch?v=Ijqkc7OLenI) and a [book chapter](http://neuralnetworksanddeeplearning.com/chap4.html) by Michael Nielsen beautifully explain how a neural network can approximate a tower function while explaining the [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem).\n\n### Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(torch)\nx <- seq(0, 3,by=0.01)\ny <- ifelse(x < 1, 0.3, ifelse(x < 2, 0.8, 0.4))\n# input and output are turned into a 1-column tensor  \nx <- torch_tensor(as.matrix(x, ncol=1))\ny <- torch_tensor(as.matrix(y, ncol=1))\nplot(x, y, xlab=\"X\", type=\"l\", ylab=\"Y\", lty=\"dashed\", lwd=2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n### Neural network\n\nOne simple way to create a neural network is to use `nn_sequential` function of the `torch` package. [Rectified linear unit (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) is used for an activation function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim_in <- 1\ndim_out <- 1\ndim_hidden <- 32\n\nnet <- nn_sequential(\n  nn_linear(dim_in, dim_hidden),\n  nn_relu(),\n  nn_linear(dim_hidden, dim_out)\n)\n```\n:::\n\n\n### Traing a neural network\n\nThe [Adam optimizer](https://arxiv.org/abs/1412.6980), which a popular choice, is used.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopt <- optim_adam(net$parameters)\n# opt <- optim_sgd(net$parameters, lr=0.001)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_epochs <- 1000\nfor (epoch in 1:num_epochs) {\n  y_pred <- net(x)  # forward pass\n  loss <- nnf_mse_loss(y_pred, y)  # compute loss \n  if (epoch %% 100 == 0) { \n    cat(\"Epoch: \", epoch, \", Loss: \", loss$item(), \"\\n\")\n  }\n  # back propagation \n  opt$zero_grad()\n  loss$backward()\n  opt$step()  # update weights\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:  100 , Loss:  0.05918226 \nEpoch:  200 , Loss:  0.04266065 \nEpoch:  300 , Loss:  0.02848286 \nEpoch:  400 , Loss:  0.01916445 \nEpoch:  500 , Loss:  0.01608658 \nEpoch:  600 , Loss:  0.01477717 \nEpoch:  700 , Loss:  0.01393809 \nEpoch:  800 , Loss:  0.01333173 \nEpoch:  900 , Loss:  0.01286871 \nEpoch:  1000 , Loss:  0.01249429 \n```\n:::\n:::\n\n\nCompare the data and the model predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nypred <- net(x)\nsprintf(\"L2 loss: %.4f\", sum((ypred-y)^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"L2 loss: 3.7598\"\n```\n:::\n\n```{.r .cell-code}\nplot(x, y, type=\"l\", xlab=\"X\", ylab=\"Y\", lty=\"dashed\", lwd=2)\nlines(x, ypred, lwd=2, col=\"firebrick\")\nlegend(\"topright\", \n  legend=c(\"data\",\"neural net\"), \n  col=c(\"black\",\"firebrick\"), \n  lty= c(\"dashed\",\"solid\"),\n  lwd=2, \n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n### Enlarge the neural network\n\nLet's repeat an experiment using a larger network - more nodes and layers\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndim_hidden <- 64\n\nnet <- nn_sequential(\n  nn_linear(dim_in, dim_hidden),\n  nn_relu(),\n  nn_linear(dim_hidden, dim_hidden),\n  nn_relu(),\n  nn_linear(dim_hidden, dim_hidden),\n  nn_relu(),\n  nn_linear(dim_hidden, dim_out)\n)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nopt <- optim_adam(net$parameters)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnum_epochs <- 1000\nfor (epoch in 1:num_epochs) {\n  y_pred <- net(x)  # forward pass\n  loss <- nnf_mse_loss(y_pred, y)  # compute loss \n  if (epoch %% 100 == 0) { \n    cat(\"Epoch: \", epoch, \", Loss: \", loss$item(), \"\\n\")\n  }\n  # back propagation \n  opt$zero_grad()\n  loss$backward()\n  opt$step()  # update weights\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:  100 , Loss:  0.02049372 \nEpoch:  200 , Loss:  0.005859586 \nEpoch:  300 , Loss:  0.00293437 \nEpoch:  400 , Loss:  0.001943566 \nEpoch:  500 , Loss:  0.001552111 \nEpoch:  600 , Loss:  0.001466194 \nEpoch:  700 , Loss:  0.00113698 \nEpoch:  800 , Loss:  0.001119171 \nEpoch:  900 , Loss:  0.0009291215 \nEpoch:  1000 , Loss:  0.001042918 \n```\n:::\n:::\n\n\nCompare the data and the model predictions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nypred <- net(x)\nsprintf(\"L2 loss: %.4f\", sum((ypred-y)^2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"L2 loss: 0.3239\"\n```\n:::\n\n```{.r .cell-code}\n# png(\"stepfunc_nn.png\")\nplot(x, y, type=\"l\", xlab=\"X\", ylab=\"Y\", lty=\"dashed\", lwd=2)\nlines(x, ypred, lwd=2, col=\"firebrick\")\nlegend(\"topright\", \n  legend=c(\"data\",\"neural net\"), \n  col=c(\"black\",\"firebrick\"), \n  lty= c(\"dashed\",\"solid\"),\n  lwd=2, \n  bty = \"n\", \n  cex = 1.0, \n  text.col = \"black\", \n  horiz = F , \n  inset = c(0.02,0.02))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# dev.off()\n```\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}