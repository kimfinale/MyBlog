{
  "hash": "11df4b5c6ebc4a8b8f312c0c37b83808",
  "result": {
    "markdown": "---\ntitle: \"Regression with censored data: AER::tobit and optim\"\nauthor: \"Jong-Hoon Kim\"\ndate: \"2023-10-15\"\ncategories: [R, regression, censor, tobit]\nimage: \"apt_censored.png\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\nThe following example was adapted from the Tobit model in [Model Estimation by Example](https://m-clark.github.io/models-by-example/tobit.html). The dataset contains 200 observations. The academic aptitude variable is `apt`, the reading and math test scores are `read` and `math`, respectively. The variable `prog` is the type of program the student is in, it is a categorical (nominal) variable that takes on three values, academic (prog = 1), general (prog = 2), and vocational (prog = 3). The variable `id` is an identification variable. More details of the dataset available at https://stats.oarc.ucla.edu/r/dae/tobit-models/.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\ndat = fread(\"https://stats.idre.ucla.edu/stat/data/tobit.csv\")\ndat[, prog := as.factor(prog)]\ndat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      id read math       prog apt\n  1:   1   34   40 vocational 352\n  2:   2   39   33 vocational 449\n  3:   3   63   48    general 648\n  4:   4   44   41    general 501\n  5:   5   47   43    general 762\n ---                             \n196: 196   44   49    general 539\n197: 197   50   50    general 594\n198: 198   47   51    general 616\n199: 199   52   50    general 558\n200: 200   68   75    general 800\n```\n:::\n:::\n\n\nFollowing codes were borrowed from the [UCLA Advanced Research Computing](https://stats.oarc.ucla.edu/r/dae/tobit-models/)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# function that gives the density of normal distribution\n# for given mean and sd, scaled to be on a count metric\n# for the histogram: count = density * sample size * bin width\nf <- function(x, var, bw = 15) {\n  dnorm(x, mean = mean(var), sd(var)) * length(var) * bw\n}\nlibrary(ggplot2)\n# setup base plot\np <- ggplot(dat, aes(x = apt, fill=prog))\n# histogram, coloured by proportion in different programs\n# with a normal distribution overlayed\np <- p + stat_bin(binwidth=15) + \n  stat_function(fun = f, size = 1,\n    args = list(var = dat$apt))\n\nggsave(\"apt_censored.png\", p)\n```\n:::\n\n\nLooking at the above histogram, we can see the censoring in the values of `apt`, that is, there are far more cases with scores of 750 to 800 than one would expect looking at the rest of the distribution. Below is an alternative histogram that further highlights the excess of cases where `apt=800`.\n\n**Note on the difference between truncation and censoring**: With censored variables, all of the observations are in the dataset, but we don't know the \"true\" values of some of them. With truncation some of the observations are not included in the analysis because of the value of the variable.\n\nThe Tobit model can be used for such a case. It is a class of regression models in which the observed range of the dependent variable is censored in some way, according to the \\[Wikipedia article\\] (https://en.wikipedia.org/wiki/Tobit_model). The possible maximum score 800, which\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntobit = AER::tobit(\n  apt ~ read + math + prog,\n  data = dat,\n  left = -Inf,\n  right = 800\n)\n```\n:::\n\n\nTo account for censoring, likelihood function is modified to so that it reflects the unequal sampling probability for each observation depending on whether the latent dependent variable fell above or below the determined threshold. It appears that this approach was first proposed by [James Tobin](https://en.wikipedia.org/wiki/James_Tobin). For a sample that, as in Tobin's original case, was censored from below at zero, the sampling probability for each non-limit observation is simply the height of the appropriate density function. For any limit observation, it is the cumulative distribution, i.e. the integral below zero of the appropriate density function. The likelihood function is thus a mixture of densities and cumulative distribution functions, according to the [Wikipedia article](https://en.wikipedia.org/wiki/Tobit_model).\n\n$$\n\\text{log }L = \\sum_{i=1}^n w_i\\left(\\delta_i~\\text{log} \\left(P\\left(Y=y_i|X\\right)\\right) + \\left(1-\\delta_i\\right)~\\text{log} \\left(1-\\sum_{i=1}^{l_U}P(Y=y_i|X)\\right)\n\\right) \n$$ , where $l_U$ represents the upper limit and\n\n$$\n\\begin{equation}\n  \\delta_i=\\begin{cases}\n    1, & \\text{if }y_i < l_U.\\\\\n    0, & \\text{otherwise}.\n  \\end{cases}\n\\end{equation}\n$$\n\n### Log likelihood accounting for censoring\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnegloglik <- function(par, y, X, ul=100) {\n  # parameters\n  sd = par[length(par)]\n  beta = par[-length(par)]\n  # create indicator depending on chosen limit\n  indicator = y < ul\n  # linear predictor\n  mu = X %*% beta\n  # log likelihood\n  loglik = indicator * dnorm(y, mean=mu, sd=sd, log=T) +\n             (1-indicator) * log(1-pnorm(ul, mean=mu, sd=sd))\n\n  sumloglik = sum(loglik, na.rm=T)\n  return(-sumloglik)\n}\n```\n:::\n\n\n### optim\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Setup data and initial values.\nmod = lm(apt ~ read + math + prog, data = dat)\nX = model.matrix(mod)\ninit = c(coef(mod), sigma=summary(mod)$sigma)\n\n# negloglik(par=init, y=acad_apt$apt, X=X, ul=800)\n\nfit <- optim(par = init,\n            fn = negloglik,\n            y = dat$apt,\n            X = X,\n            ul = 800)\n\ncoef(tobit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   (Intercept)           read           math    proggeneral progvocational \n    209.565971       2.697939       5.914485     -12.714763     -46.143904 \n```\n:::\n\n```{.r .cell-code}\n(fit$par)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   (Intercept)           read           math    proggeneral progvocational \n    211.054867       2.813491       5.763177     -12.354492     -45.847701 \n         sigma \n     65.652919 \n```\n:::\n:::\n\n\n### `optim` control parameters\n\nBy adjusting control parameters of the `optim` function, results can match more closely. Below is such an example.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- optim(par = init,\n            fn = negloglik,\n            y = dat$apt,\n            X = X,\n            ul = 800,\n            method = \"Nelder-Mead\",\n            control = list(maxit=2e4, reltol=1e-15))\n\ncoef(tobit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   (Intercept)           read           math    proggeneral progvocational \n    209.565971       2.697939       5.914485     -12.714763     -46.143904 \n```\n:::\n\n```{.r .cell-code}\n(fit$par)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   (Intercept)           read           math    proggeneral progvocational \n    209.565967       2.697937       5.914486     -12.714777     -46.143838 \n         sigma \n     65.676724 \n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}