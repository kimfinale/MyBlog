{
  "hash": "5187394d45f39c74730fefee4aea0698",
  "result": {
    "markdown": "---\ntitle: \"A very basic implementation of a neural network\"\nauthor: \"Jong-Hoon Kim\"\ndate: \"2024-05-27\"\ncategories: [GPT-2, ChatGPT, Wolfram]\nimage: \"obs_pred.png\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\nI am documenting my learning of a neural network. The contents are mostly based on the [e-book](https://skeydan.github.io/Deep-Learning-and-Scientific-Computing-with-R-torch/).\n\nLoad the `torch` library.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(torch)\n```\n:::\n\n\n### Data\n \n\n::: {.cell}\n\n```{.r .cell-code}\n# input dimensionality (number of input features)\ndim_in <- 3\n# number of observations in training set\nn <- 200\n\nx <- torch_randn(n, dim_in)\ncoefs <- c(0.2, -1.3, -0.5)\ny <- x$matmul(coefs)$unsqueeze(2) + torch_randn(n, 1) # column matrix\n```\n:::\n\n\nWeights and biases\n\n$$f(\\bf{X})=\\bf{XW}+b$$\n\n\n\nUsing two layers with corresponding parameters, `w1, b1, w2` and `b2`. \n\n$$f(\\bf{X})=(\\bf{XW_1}+b_1)\\bf{W_2}+b_2$$\n\n`y_pred <- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)`\n  \n\n::: {.cell}\n\n```{.r .cell-code}\n# dimensionality of hidden layer\ndim_hidden <- 32\n# output dimensionality (number of predicted features)\ndim_out <- 1\n\n# weights connecting input to hidden layer\nw1 <- torch_randn(dim_in, dim_hidden, requires_grad = TRUE)\n# weights connecting hidden to output layer\nw2 <- torch_randn(dim_hidden, dim_out, requires_grad = TRUE)\n\n# hidden layer bias\nb1 <- torch_zeros(1, dim_hidden, requires_grad = TRUE)\n# output layer bias\nb2 <- torch_zeros(1, dim_out, requires_grad = TRUE)\n```\n:::\n\n\nPredicted values from the above network is computed as follows and using Rectified Linear Unit (ReLU) as the activation function\n\n::: {.cell}\n\n```{.r .cell-code}\ny_pred <- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)\n```\n:::\n\n\nThen the loss function can be created as follows\n\n::: {.cell}\n\n```{.r .cell-code}\nloss <- (y_pred - y)$pow(2)$mean()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlearning_rate <- 1e-2\n\n### training loop ----------------------------------------\n\nfor (epoch in 1:200) {\n  ### -------- Forward pass --------\n  y_pred <- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)\n  \n  ### -------- Compute loss -------- \n  loss <- (y_pred - y)$pow(2)$mean()\n  if (epoch %% 10 == 0)\n    cat(\"Epoch: \", epoch, \"   Loss: \", loss$item(), \"\\n\")\n  \n  ### -------- Backpropagation --------\n  # compute gradient of loss w.r.t. all tensors with\n  # requires_grad = TRUE\n  loss$backward()\n  \n  ### -------- Update weights -------- \n  # Wrap in with_no_grad() because this is a part we don't \n  # want to record for automatic gradient computation\n   with_no_grad({\n     w1 <- w1$sub_(learning_rate * w1$grad)\n     w2 <- w2$sub_(learning_rate * w2$grad)\n     b1 <- b1$sub_(learning_rate * b1$grad)\n     b2 <- b2$sub_(learning_rate * b2$grad)  \n     \n     # Zero gradients after every pass, as they'd\n     # accumulate otherwise\n     w1$grad$zero_()\n     w2$grad$zero_()\n     b1$grad$zero_()\n     b2$grad$zero_()  \n   })\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:  10    Loss:  3.000276 \nEpoch:  20    Loss:  2.144468 \nEpoch:  30    Loss:  1.749418 \nEpoch:  40    Loss:  1.538223 \nEpoch:  50    Loss:  1.413543 \nEpoch:  60    Loss:  1.33866 \nEpoch:  70    Loss:  1.294799 \nEpoch:  80    Loss:  1.265488 \nEpoch:  90    Loss:  1.244047 \nEpoch:  100    Loss:  1.226817 \nEpoch:  110    Loss:  1.212944 \nEpoch:  120    Loss:  1.201177 \nEpoch:  130    Loss:  1.190159 \nEpoch:  140    Loss:  1.178311 \nEpoch:  150    Loss:  1.167546 \nEpoch:  160    Loss:  1.157191 \nEpoch:  170    Loss:  1.147406 \nEpoch:  180    Loss:  1.13854 \nEpoch:  190    Loss:  1.131134 \nEpoch:  200    Loss:  1.123894 \n```\n:::\n:::\n\n\nEvaluate the model visually\n\n::: {.cell}\n\n```{.r .cell-code}\n# png(\"obs_pred.png\")\ny_pred <- x$mm(w1)$add(b1)$relu()$mm(w2)$add(b2)\nplot(y, y_pred, xlab=\"Observed\", ylab=\"Predicted\", \n     main=\"Neural network from scratch\")\nabline(a=0, b=1, col=\"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# dev.off()\nsum((as.numeric(y) - as.numeric(y_pred))^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 224.638\n```\n:::\n:::\n\n\n\nThe same model can be created in a more compactly way using a sequential module and using the activation function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nnet <- nn_sequential(\n  nn_linear(dim_in, dim_hidden),\n  nn_relu(),\n  nn_linear(dim_hidden, dim_out)\n)\n```\n:::\n\n\nTrain using the `Adam` optimizer, a popular choice.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopt <- optim_adam(net$parameters)\n# opt <- optim_sgd(net$parameters, lr=0.001)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n### training loop --------------------------------------\n\nfor (epoch in 1:200) {\n  # forward pass\n  y_pred <- net(x)\n  # compute loss \n  loss <- nnf_mse_loss(y_pred, y)\n  if (epoch %% 10 == 0) { \n    cat(\"Epoch: \", epoch, \", Loss: \", loss$item(), \"\\n\")\n  }\n  # back propagation \n  opt$zero_grad()\n  loss$backward()\n  # update weights\n  opt$step()\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEpoch:  10 , Loss:  3.195003 \nEpoch:  20 , Loss:  2.957336 \nEpoch:  30 , Loss:  2.741568 \nEpoch:  40 , Loss:  2.544529 \nEpoch:  50 , Loss:  2.363058 \nEpoch:  60 , Loss:  2.193356 \nEpoch:  70 , Loss:  2.034059 \nEpoch:  80 , Loss:  1.885832 \nEpoch:  90 , Loss:  1.748948 \nEpoch:  100 , Loss:  1.624851 \nEpoch:  110 , Loss:  1.513974 \nEpoch:  120 , Loss:  1.417417 \nEpoch:  130 , Loss:  1.33595 \nEpoch:  140 , Loss:  1.269105 \nEpoch:  150 , Loss:  1.216185 \nEpoch:  160 , Loss:  1.176016 \nEpoch:  170 , Loss:  1.147551 \nEpoch:  180 , Loss:  1.128549 \nEpoch:  190 , Loss:  1.116211 \nEpoch:  200 , Loss:  1.108102 \n```\n:::\n:::\n\n\nCompare the prediction and observation\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny_pred_s <- net(x)\nplot(y, y_pred, xlab=\"Observed\", ylab=\"Predicted\", \n     main=\"Neural network: A sequential module\")\nabline(a=0, b=1, col=\"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Mean squared error, L2 loss\nsum((as.numeric(y) - as.numeric(y_pred))^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 221.6205\n```\n:::\n:::\n\n\nCompared with the linear model\n\n::: {.cell}\n\n```{.r .cell-code}\nxdf <- as.data.frame(as.matrix(x))\nnames(xdf) <- c(\"x1\",\"x2\", \"x3\")\nydf <- as.data.frame(as.matrix(y))\nnames(ydf) <- c(\"y\")\ndat <- cbind(xdf, ydf)\nm <- lm(y~x1+x2+x3, data=dat)\ny_pred_lm <- predict(m, xdf)\nydf2 <- cbind(ydf, y_pred_lm)\nplot(ydf2[,1], ydf2[,2], xlab=\"Observed\", ylab=\"Predicted\", \n     main=\"Linear regresssion\")\nabline(a=0, b=1, col=\"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Mean squared error, L2 loss\nsum((ydf$y - y_pred_lm)^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 218.2733\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}